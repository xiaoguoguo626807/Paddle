
#include "glog/logging.h"
#include "paddle/phi/api/all.h"
#include "paddle/phi/api/backward/backward_api.h"
#include "paddle/phi/api/backward/sparse_bw_api.h"
#include "paddle/fluid/imperative/tracer.h"
#include "paddle/fluid/framework/op_registry.h"
#include "paddle/fluid/platform/profiler/event_tracing.h"
#include "paddle/fluid/eager/utils.h"
#include "paddle/fluid/eager/api/utils/global_utils.h"
#include "paddle/fluid/eager/api/generated/eager_generated/backwards/nodes.h"
#include "paddle/fluid/eager/api/generated/eager_generated/forwards/dygraph_functions.h"
#include "paddle/fluid/eager/to_static/run_program_op_node.h"
#include "paddle/fluid/eager/nan_inf_utils.h"
#include "paddle/phi/api/include/sparse_api.h"
#include "paddle/fluid/eager/api/manual/eager_manual/nodes/nodes.h"
#include "paddle/fluid/prim/api/manual/backward/composite_backward_api.h"
DECLARE_bool(check_nan_inf);

paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AcosGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "acos_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "acos_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::acos_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acos_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op acos_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: acos_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AcoshGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "acosh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "acosh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::acosh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acosh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op acosh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: acosh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AddmmGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "addmm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  auto& beta = this->beta_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "addmm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::addmm_grad(input, x, y, out_grad, alpha, beta, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("addmm_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  

  auto& x_grad = returns[1][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[2][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op addmm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: addmm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AngleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "angle_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "angle_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::angle_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("angle_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op angle_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: angle_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ArgsortGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "argsort_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& descending = this->descending_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "argsort_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::argsort_grad(indices, x, out_grad, axis, descending, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("argsort_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op argsort_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: argsort_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AsComplexGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "as_complex_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "as_complex_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = as_real_ad_func(out_grad);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::as_real(out_grad);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("as_complex_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: as_complex_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AsRealGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "as_real_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "as_real_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = as_complex_ad_func(out_grad);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::as_complex(out_grad);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("as_real_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: as_real_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AsinGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "asin_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "asin_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::asin_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asin_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op asin_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: asin_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AsinhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "asinh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "asinh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::asinh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asinh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op asinh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: asinh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AtanGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "atan_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "atan_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::atan_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atan_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op atan_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: atan_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Atan2GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "atan2_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "atan2_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::atan2_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atan2_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op atan2_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: atan2_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AtanhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "atanh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "atanh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::atanh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atanh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op atanh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: atanh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BmmGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "bmm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "bmm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::bmm_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bmm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op bmm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: bmm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CeilGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "ceil_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "ceil_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::ceil_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("ceil_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op ceil_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: ceil_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CeluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "celu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "celu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::celu_grad(x, grad_out, alpha, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("celu_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("celu_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<CeluDoubleGradNode>(new CeluDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: celu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CeluDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "celu_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "celu_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::celu_double_grad(x, grad_out, grad_x_grad, alpha, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("celu_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op celu_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: celu_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CholeskyGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cholesky_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& upper = this->upper_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cholesky_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cholesky_grad(out, out_grad, upper, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cholesky_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cholesky_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cholesky_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CholeskySolveGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cholesky_solve_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& upper = this->upper_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cholesky_solve_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cholesky_solve_grad(x, y, out, out_grad, upper, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cholesky_solve_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cholesky_solve_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cholesky_solve_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ComplexGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "complex_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto real = egr::EagerUtils::RecoverTensorWrapper(&this->real_);
  auto imag = egr::EagerUtils::RecoverTensorWrapper(&this->imag_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "complex_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_REAL_TEMPLATE = " \n( real , [%s]), ";
    std::string input_real_str = paddle::string::Sprintf(TENSOR_REAL_TEMPLATE, egr::EagerUtils::TensorStr(real));
    input_str += input_real_str; 
    const char* TENSOR_IMAG_TEMPLATE = " \n( imag , [%s]), ";
    std::string input_imag_str = paddle::string::Sprintf(TENSOR_IMAG_TEMPLATE, egr::EagerUtils::TensorStr(imag));
    input_str += input_imag_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::complex_grad(real, imag, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("complex_grad", returns); }

  // Get GradOut autograd_meta

  auto& real_grad = returns[0][0];
  egr::AutogradMeta* real_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&real_grad) : nullptr;
  if (real_grad_autograd_meta) real_grad_autograd_meta->SetStopGradient(false);
  

  auto& imag_grad = returns[1][0];
  egr::AutogradMeta* imag_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&imag_grad) : nullptr;
  if (imag_grad_autograd_meta) imag_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op complex_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: complex_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_REAL_TEMPLATE = " \n( real , [%s]), ";
    std::string input_real_str = paddle::string::Sprintf(TENSOR_REAL_TEMPLATE, egr::EagerUtils::TensorStr(real));
    input_str += input_real_str; 
    const char* TENSOR_IMAG_TEMPLATE = " \n( imag , [%s]), ";
    std::string input_imag_str = paddle::string::Sprintf(TENSOR_IMAG_TEMPLATE, egr::EagerUtils::TensorStr(imag));
    input_str += input_imag_str; 
    const char* TENSOR_REAL_GRAD_TEMPLATE = " \n ( real_grad , [%s]), ";
    std::string output_real_grad_str = paddle::string::Sprintf(TENSOR_REAL_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(real_grad));
    output_str += output_real_grad_str; 
    const char* TENSOR_IMAG_GRAD_TEMPLATE = " \n ( imag_grad , [%s]), ";
    std::string output_imag_grad_str = paddle::string::Sprintf(TENSOR_IMAG_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(imag_grad));
    output_str += output_imag_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ConjGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conj_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conj_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = conj_ad_func(out_grad);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::conj(out_grad);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conj_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: conj_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CosGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cos_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "cos_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cos_grad(x, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cos_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("cos_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<CosDoubleGradNode>(new CosDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: cos_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CosDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cos_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);

  paddle::optional<paddle::experimental::Tensor> grad_out_forward_optional;
  if( grad_out_forward.impl() ) grad_out_forward_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_forward);

  auto& grad_x_grad_forward = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad_forward.initialized()) {
    VLOG(10) << grad_x_grad_forward.name() << "(grad_x_grad) use_count: " << grad_x_grad_forward.impl().use_count();
    if (grad_x_grad_forward.impl().use_count() == 1 || (grad_x_grad_forward.impl().use_count() == 2 && grad_x_grad_forward.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad_forward, api_output_1);
    }
  }

  VLOG(5) << "Running C++ API: " << "cos_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cos_double_grad(x, grad_out_forward_optional, grad_x_grad_forward, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cos_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("cos_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<CosTripleGradNode>(new CosTripleGradNode(2, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out_forward(grad_out_forward);
    grad_node->SetTensorWrappergrad_x_grad_forward(grad_x_grad_forward);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out_forward, 1);
    grad_node->SetGradOutMeta(grad_x_grad_forward, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_out_grad_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_out_grad_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_out_grad_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_out_grad_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_out_grad, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_out_grad);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: cos_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CosTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cos_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_forward_);

  paddle::optional<paddle::experimental::Tensor> grad_out_forward_optional;
  if( grad_out_forward.impl() ) grad_out_forward_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_forward);

  auto grad_x_grad_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_x_grad_forward_);

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_forward_optional;
  if( grad_x_grad_forward.impl() ) grad_x_grad_forward_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad_forward);

  auto& grad_x_grad = hooked_grads[0][0];
  auto& grad_out_grad_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_out_grad_grad_optional;
  if(grad_out_grad_grad.initialized()) grad_out_grad_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_grad_grad);

  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad_forward.initialized()) {
    VLOG(10) << grad_x_grad_forward.name() << "(grad_x_grad_forward) use_count: " << grad_x_grad_forward.impl().use_count();
    if (grad_x_grad_forward.impl().use_count() == 1 || (grad_x_grad_forward.impl().use_count() == 2 && grad_x_grad_forward.impl().get() == (&this->grad_x_grad_forward_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad_forward, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "cos_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE = " \n( grad_out_grad_grad , [%s]), ";
    std::string input_grad_out_grad_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad_grad));
    input_str += input_grad_out_grad_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cos_triple_grad(x, grad_out_forward_optional, grad_x_grad_forward_optional, grad_x_grad, grad_out_grad_grad_optional, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cos_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_forward_grad = returns[1][0];
  egr::AutogradMeta* grad_out_forward_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_forward_grad) : nullptr;
  if (grad_out_forward_grad_autograd_meta) grad_out_forward_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_x_grad_forward_grad = returns[2][0];
  egr::AutogradMeta* grad_x_grad_forward_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x_grad_forward_grad) : nullptr;
  if (grad_x_grad_forward_grad_autograd_meta) grad_x_grad_forward_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cos_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cos_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE = " \n( grad_out_grad_grad , [%s]), ";
    std::string input_grad_out_grad_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad_grad));
    input_str += input_grad_out_grad_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_GRAD_TEMPLATE = " \n ( grad_out_forward_grad , [%s]), ";
    std::string output_grad_out_forward_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward_grad));
    output_str += output_grad_out_forward_grad_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_GRAD_TEMPLATE = " \n ( grad_x_grad_forward_grad , [%s]), ";
    std::string output_grad_x_grad_forward_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward_grad));
    output_str += output_grad_x_grad_forward_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CoshGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cosh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "cosh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cosh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cosh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cosh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cosh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CropGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "crop_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& offsets = this->offsets_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "crop_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::crop_grad(x, out_grad, offsets, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("crop_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op crop_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: crop_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CrossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cross_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cross_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cross_grad(x, y, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cross_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cross_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cross_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DetGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "det_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "det_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::det_grad(x, out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("det_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op det_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: det_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DiagGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "diag_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& offset = this->offset_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "diag_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::diag_grad(x, out_grad, offset, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("diag_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op diag_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: diag_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DiagonalGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "diagonal_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& offset = this->offset_;
  auto& axis1 = this->axis1_;
  auto& axis2 = this->axis2_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "diagonal_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::diagonal_grad(x, out_grad, offset, axis1, axis2, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("diagonal_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op diagonal_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: diagonal_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DigammaGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "digamma_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "digamma_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::digamma_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("digamma_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op digamma_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: digamma_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DistGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "dist_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& p = this->p_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "dist_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::dist_grad(x, y, out, out_grad, p, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("dist_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op dist_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: dist_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DotGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "dot_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "dot_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::dot_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("dot_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op dot_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: dot_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EigGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "eig_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out_w = egr::EagerUtils::RecoverTensorWrapper(&this->out_w_);
  auto out_v = egr::EagerUtils::RecoverTensorWrapper(&this->out_v_);
  auto& out_w_grad = hooked_grads[0][0];
  auto& out_v_grad = hooked_grads[1][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "eig_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_W_GRAD_TEMPLATE = " \n( out_w_grad , [%s]), ";
    std::string input_out_w_grad_str = paddle::string::Sprintf(TENSOR_OUT_W_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_w_grad));
    input_str += input_out_w_grad_str; 
    const char* TENSOR_OUT_V_GRAD_TEMPLATE = " \n( out_v_grad , [%s]), ";
    std::string input_out_v_grad_str = paddle::string::Sprintf(TENSOR_OUT_V_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_v_grad));
    input_str += input_out_v_grad_str; 
    const char* TENSOR_OUT_W_TEMPLATE = " \n( out_w , [%s]), ";
    std::string input_out_w_str = paddle::string::Sprintf(TENSOR_OUT_W_TEMPLATE, egr::EagerUtils::TensorStr(out_w));
    input_str += input_out_w_str; 
    const char* TENSOR_OUT_V_TEMPLATE = " \n( out_v , [%s]), ";
    std::string input_out_v_str = paddle::string::Sprintf(TENSOR_OUT_V_TEMPLATE, egr::EagerUtils::TensorStr(out_v));
    input_str += input_out_v_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::eig_grad(out_w, out_v, out_w_grad, out_v_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("eig_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op eig_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: eig_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_W_GRAD_TEMPLATE = " \n( out_w_grad , [%s]), ";
    std::string input_out_w_grad_str = paddle::string::Sprintf(TENSOR_OUT_W_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_w_grad));
    input_str += input_out_w_grad_str; 
    const char* TENSOR_OUT_V_GRAD_TEMPLATE = " \n( out_v_grad , [%s]), ";
    std::string input_out_v_grad_str = paddle::string::Sprintf(TENSOR_OUT_V_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_v_grad));
    input_str += input_out_v_grad_str; 
    const char* TENSOR_OUT_W_TEMPLATE = " \n( out_w , [%s]), ";
    std::string input_out_w_str = paddle::string::Sprintf(TENSOR_OUT_W_TEMPLATE, egr::EagerUtils::TensorStr(out_w));
    input_str += input_out_w_str; 
    const char* TENSOR_OUT_V_TEMPLATE = " \n( out_v , [%s]), ";
    std::string input_out_v_str = paddle::string::Sprintf(TENSOR_OUT_V_TEMPLATE, egr::EagerUtils::TensorStr(out_v));
    input_str += input_out_v_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EighGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "eigh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out_w = egr::EagerUtils::RecoverTensorWrapper(&this->out_w_);
  auto out_v = egr::EagerUtils::RecoverTensorWrapper(&this->out_v_);
  auto& out_w_grad = hooked_grads[0][0];
  auto& out_v_grad = hooked_grads[1][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "eigh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_W_GRAD_TEMPLATE = " \n( out_w_grad , [%s]), ";
    std::string input_out_w_grad_str = paddle::string::Sprintf(TENSOR_OUT_W_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_w_grad));
    input_str += input_out_w_grad_str; 
    const char* TENSOR_OUT_V_GRAD_TEMPLATE = " \n( out_v_grad , [%s]), ";
    std::string input_out_v_grad_str = paddle::string::Sprintf(TENSOR_OUT_V_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_v_grad));
    input_str += input_out_v_grad_str; 
    const char* TENSOR_OUT_W_TEMPLATE = " \n( out_w , [%s]), ";
    std::string input_out_w_str = paddle::string::Sprintf(TENSOR_OUT_W_TEMPLATE, egr::EagerUtils::TensorStr(out_w));
    input_str += input_out_w_str; 
    const char* TENSOR_OUT_V_TEMPLATE = " \n( out_v , [%s]), ";
    std::string input_out_v_str = paddle::string::Sprintf(TENSOR_OUT_V_TEMPLATE, egr::EagerUtils::TensorStr(out_v));
    input_str += input_out_v_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::eigh_grad(out_w, out_v, out_w_grad, out_v_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("eigh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op eigh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: eigh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_W_GRAD_TEMPLATE = " \n( out_w_grad , [%s]), ";
    std::string input_out_w_grad_str = paddle::string::Sprintf(TENSOR_OUT_W_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_w_grad));
    input_str += input_out_w_grad_str; 
    const char* TENSOR_OUT_V_GRAD_TEMPLATE = " \n( out_v_grad , [%s]), ";
    std::string input_out_v_grad_str = paddle::string::Sprintf(TENSOR_OUT_V_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_v_grad));
    input_str += input_out_v_grad_str; 
    const char* TENSOR_OUT_W_TEMPLATE = " \n( out_w , [%s]), ";
    std::string input_out_w_str = paddle::string::Sprintf(TENSOR_OUT_W_TEMPLATE, egr::EagerUtils::TensorStr(out_w));
    input_str += input_out_w_str; 
    const char* TENSOR_OUT_V_TEMPLATE = " \n( out_v , [%s]), ";
    std::string input_out_v_str = paddle::string::Sprintf(TENSOR_OUT_V_TEMPLATE, egr::EagerUtils::TensorStr(out_v));
    input_str += input_out_v_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "elu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "elu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::elu_grad(x, out, grad_out, alpha, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("elu_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("elu_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<EluDoubleGradNode>(new EluDoubleGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributealpha(alpha);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: elu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EluDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "elu_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "elu_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::elu_double_grad(x, grad_out, grad_x_grad, alpha, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("elu_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[2][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op elu_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: elu_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ErfGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "erf_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "erf_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::erf_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("erf_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op erf_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: erf_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ErfinvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "erfinv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "erfinv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::erfinv_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("erfinv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op erfinv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: erfinv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ExpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "exp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "exp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::exp_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("exp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op exp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: exp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Expm1GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "expm1_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "expm1_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::expm1_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expm1_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op expm1_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: expm1_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FftC2cGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fft_c2c_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& axes = this->axes_;
  auto& normalization = this->normalization_;
  auto& forward = this->forward_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fft_c2c_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fft_c2c_grad(out_grad, axes, normalization, forward, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fft_c2c_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fft_c2c_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fft_c2c_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FftC2rGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fft_c2r_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& axes = this->axes_;
  auto& normalization = this->normalization_;
  auto& forward = this->forward_;
  auto& last_dim_size = this->last_dim_size_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fft_c2r_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fft_c2r_grad(out_grad, axes, normalization, forward, last_dim_size, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fft_c2r_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fft_c2r_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fft_c2r_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FftR2cGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fft_r2c_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& axes = this->axes_;
  auto& normalization = this->normalization_;
  auto& forward = this->forward_;
  auto& onesided = this->onesided_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fft_r2c_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fft_r2c_grad(x, out_grad, axes, normalization, forward, onesided, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fft_r2c_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fft_r2c_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fft_r2c_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FillDiagonalTensorGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fill_diagonal_tensor_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& offset = this->offset_;
  auto& dim1 = this->dim1_;
  auto& dim2 = this->dim2_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "fill_diagonal_tensor_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fill_diagonal_tensor_grad(out_grad, offset, dim1, dim2, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_diagonal_tensor_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fill_diagonal_tensor_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fill_diagonal_tensor_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FlipGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "flip_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "flip_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = flip_ad_func(out_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::flip(out_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("flip_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: flip_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FloorGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "floor_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "floor_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::floor_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("floor_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op floor_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: floor_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FoldGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fold_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& output_sizes = this->output_sizes_;
  auto& kernel_sizes = this->kernel_sizes_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& dilations = this->dilations_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fold_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fold_grad(x, out_grad, output_sizes, kernel_sizes, strides, paddings, dilations, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fold_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fold_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fold_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FrameGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "frame_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& frame_length = this->frame_length_;
  auto& hop_length = this->hop_length_;
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "frame_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::frame_grad(x, out_grad, frame_length, hop_length, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("frame_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op frame_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: frame_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> GatherNdGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "gather_nd_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "gather_nd_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::gather_nd_grad(x, index, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gather_nd_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op gather_nd_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: gather_nd_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> GeluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "gelu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& approximate = this->approximate_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "gelu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::gelu_grad(x, out_grad, approximate, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gelu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op gelu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: gelu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> GridSampleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "grid_sample_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grid = egr::EagerUtils::RecoverTensorWrapper(&this->grid_);
  auto& out_grad = hooked_grads[0][0];
  auto& mode = this->mode_;
  auto& padding_mode = this->padding_mode_;
  auto& align_corners = this->align_corners_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "grid_sample_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRID_TEMPLATE = " \n( grid , [%s]), ";
    std::string input_grid_str = paddle::string::Sprintf(TENSOR_GRID_TEMPLATE, egr::EagerUtils::TensorStr(grid));
    input_str += input_grid_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::grid_sample_grad(x, grid, out_grad, mode, padding_mode, align_corners, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("grid_sample_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grid_grad = returns[1][0];
  egr::AutogradMeta* grid_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grid_grad) : nullptr;
  if (grid_grad_autograd_meta) grid_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op grid_sample_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: grid_sample_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRID_TEMPLATE = " \n( grid , [%s]), ";
    std::string input_grid_str = paddle::string::Sprintf(TENSOR_GRID_TEMPLATE, egr::EagerUtils::TensorStr(grid));
    input_str += input_grid_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRID_GRAD_TEMPLATE = " \n ( grid_grad , [%s]), ";
    std::string output_grid_grad_str = paddle::string::Sprintf(TENSOR_GRID_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grid_grad));
    output_str += output_grid_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> GumbelSoftmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "gumbel_softmax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "gumbel_softmax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::gumbel_softmax_grad(out, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gumbel_softmax_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op gumbel_softmax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: gumbel_softmax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> HardshrinkGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "hardshrink_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "hardshrink_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::hardshrink_grad(x, out_grad, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardshrink_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op hardshrink_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: hardshrink_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> HardsigmoidGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "hardsigmoid_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& slope = this->slope_;
  auto& offset = this->offset_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "hardsigmoid_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::hardsigmoid_grad(out, out_grad, slope, offset, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardsigmoid_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op hardsigmoid_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: hardsigmoid_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> IndexSampleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "index_sample_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "index_sample_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::index_sample_grad(x, index, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_sample_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op index_sample_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: index_sample_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> IndexSelectGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "index_select_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "index_select_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::index_select_grad(x, index, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_select_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op index_select_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: index_select_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> InverseGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "inverse_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "inverse_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::inverse_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("inverse_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op inverse_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: inverse_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> KthvalueGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "kthvalue_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto& out_grad = hooked_grads[0][0];
  auto& k = this->k_;
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "kthvalue_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::kthvalue_grad(x, indices, out_grad, k, axis, keepdim, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("kthvalue_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op kthvalue_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: kthvalue_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LabelSmoothGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "label_smooth_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& epsilon = this->epsilon_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "label_smooth_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::label_smooth_grad(out_grad, epsilon, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("label_smooth_grad", returns); }

  // Get GradOut autograd_meta

  auto& label_grad = returns[0][0];
  egr::AutogradMeta* label_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&label_grad) : nullptr;
  if (label_grad_autograd_meta) label_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op label_smooth_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: label_smooth_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_LABEL_GRAD_TEMPLATE = " \n ( label_grad , [%s]), ";
    std::string output_label_grad_str = paddle::string::Sprintf(TENSOR_LABEL_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(label_grad));
    output_str += output_label_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LeakyReluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "leaky_relu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& negative_slope = this->negative_slope_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "leaky_relu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::leaky_relu_grad(x, grad_out, negative_slope, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("leaky_relu_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("leaky_relu_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<LeakyReluDoubleGradNode>(new LeakyReluDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributenegative_slope(negative_slope);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: leaky_relu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LeakyReluDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "leaky_relu_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& negative_slope = this->negative_slope_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "leaky_relu_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::leaky_relu_double_grad(x, grad_x_grad, negative_slope, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("leaky_relu_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op leaky_relu_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: leaky_relu_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LerpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "lerp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto weight = egr::EagerUtils::RecoverTensorWrapper(&this->weight_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "lerp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::lerp_grad(x, y, weight, out, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lerp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op lerp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: lerp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LgammaGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "lgamma_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "lgamma_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::lgamma_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lgamma_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op lgamma_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: lgamma_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "log_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log_grad(x, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("log_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<LogDoubleGradNode>(new LogDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: log_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "log_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log_double_grad(x, grad_out, grad_x_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Log10GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log10_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "log10_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log10_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log10_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log10_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log10_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Log1pGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log1p_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "log1p_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log1p_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log1p_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log1p_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log1p_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Log2GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log2_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "log2_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log2_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log2_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log2_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log2_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto& out_grad = hooked_grads[0][0];
  auto& epsilon = this->epsilon_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "log_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log_loss_grad(input, label, out_grad, epsilon, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogitGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "logit_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& eps = this->eps_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "logit_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::logit_grad(x, out_grad, eps, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logit_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op logit_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: logit_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogsigmoidGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "logsigmoid_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "logsigmoid_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::logsigmoid_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logsigmoid_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op logsigmoid_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: logsigmoid_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LuUnpackGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "lu_unpack_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto l = egr::EagerUtils::RecoverTensorWrapper(&this->l_);
  auto u = egr::EagerUtils::RecoverTensorWrapper(&this->u_);
  auto pmat = egr::EagerUtils::RecoverTensorWrapper(&this->pmat_);
  auto& l_grad = hooked_grads[1][0];
  auto& u_grad = hooked_grads[2][0];
  auto& unpack_ludata = this->unpack_ludata_;
  auto& unpack_pivots = this->unpack_pivots_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "lu_unpack_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_L_GRAD_TEMPLATE = " \n( l_grad , [%s]), ";
    std::string input_l_grad_str = paddle::string::Sprintf(TENSOR_L_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(l_grad));
    input_str += input_l_grad_str; 
    const char* TENSOR_U_GRAD_TEMPLATE = " \n( u_grad , [%s]), ";
    std::string input_u_grad_str = paddle::string::Sprintf(TENSOR_U_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(u_grad));
    input_str += input_u_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_L_TEMPLATE = " \n( l , [%s]), ";
    std::string input_l_str = paddle::string::Sprintf(TENSOR_L_TEMPLATE, egr::EagerUtils::TensorStr(l));
    input_str += input_l_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_PMAT_TEMPLATE = " \n( pmat , [%s]), ";
    std::string input_pmat_str = paddle::string::Sprintf(TENSOR_PMAT_TEMPLATE, egr::EagerUtils::TensorStr(pmat));
    input_str += input_pmat_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::lu_unpack_grad(x, y, l, u, pmat, l_grad, u_grad, unpack_ludata, unpack_pivots, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lu_unpack_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op lu_unpack_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: lu_unpack_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_L_GRAD_TEMPLATE = " \n( l_grad , [%s]), ";
    std::string input_l_grad_str = paddle::string::Sprintf(TENSOR_L_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(l_grad));
    input_str += input_l_grad_str; 
    const char* TENSOR_U_GRAD_TEMPLATE = " \n( u_grad , [%s]), ";
    std::string input_u_grad_str = paddle::string::Sprintf(TENSOR_U_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(u_grad));
    input_str += input_u_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_L_TEMPLATE = " \n( l , [%s]), ";
    std::string input_l_str = paddle::string::Sprintf(TENSOR_L_TEMPLATE, egr::EagerUtils::TensorStr(l));
    input_str += input_l_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_PMAT_TEMPLATE = " \n( pmat , [%s]), ";
    std::string input_pmat_str = paddle::string::Sprintf(TENSOR_PMAT_TEMPLATE, egr::EagerUtils::TensorStr(pmat));
    input_str += input_pmat_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaskedSelectGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "masked_select_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto mask = egr::EagerUtils::RecoverTensorWrapper(&this->mask_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "masked_select_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::masked_select_grad(x, mask, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("masked_select_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op masked_select_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: masked_select_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MatrixPowerGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "matrix_power_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& n = this->n_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "matrix_power_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::matrix_power_grad(x, out, out_grad, n, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matrix_power_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op matrix_power_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: matrix_power_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaxoutGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "maxout_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& groups = this->groups_;
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "maxout_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::maxout_grad(x, out, out_grad, groups, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("maxout_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op maxout_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: maxout_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ModeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mode_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "mode_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::mode_grad(x, indices, out_grad, axis, keepdim, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mode_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op mode_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: mode_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto vec = egr::EagerUtils::RecoverTensorWrapper(&this->vec_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "mv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::mv_grad(x, vec, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& vec_grad = returns[1][0];
  egr::AutogradMeta* vec_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&vec_grad) : nullptr;
  if (vec_grad_autograd_meta) vec_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op mv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: mv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_VEC_GRAD_TEMPLATE = " \n ( vec_grad , [%s]), ";
    std::string output_vec_grad_str = paddle::string::Sprintf(TENSOR_VEC_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(vec_grad));
    output_str += output_vec_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> NllLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "nll_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto weight = egr::EagerUtils::RecoverTensorWrapper(&this->weight_);

  paddle::optional<paddle::experimental::Tensor> weight_optional;
  if( weight.impl() ) weight_optional = paddle::make_optional<paddle::experimental::Tensor>(weight);

  auto total_weight = egr::EagerUtils::RecoverTensorWrapper(&this->total_weight_);
  auto& out_grad = hooked_grads[0][0];
  auto& ignore_index = this->ignore_index_;
  auto& reduction = this->reduction_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "nll_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_TOTAL_WEIGHT_TEMPLATE = " \n( total_weight , [%s]), ";
    std::string input_total_weight_str = paddle::string::Sprintf(TENSOR_TOTAL_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(total_weight));
    input_str += input_total_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::nll_loss_grad(input, label, weight_optional, total_weight, out_grad, ignore_index, reduction, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("nll_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op nll_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: nll_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_TOTAL_WEIGHT_TEMPLATE = " \n( total_weight , [%s]), ";
    std::string input_total_weight_str = paddle::string::Sprintf(TENSOR_TOTAL_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(total_weight));
    input_str += input_total_weight_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> OverlapAddGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "overlap_add_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& hop_length = this->hop_length_;
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "overlap_add_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::overlap_add_grad(x, out_grad, hop_length, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("overlap_add_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op overlap_add_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: overlap_add_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PixelShuffleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pixel_shuffle_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& upscale_factor = this->upscale_factor_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pixel_shuffle_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pixel_shuffle_grad(out_grad, upscale_factor, data_format, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pixel_shuffle_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pixel_shuffle_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pixel_shuffle_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PoissonGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "poisson_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "poisson_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::poisson_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("poisson_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op poisson_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: poisson_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PutAlongAxisGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "put_along_axis_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto arr = egr::EagerUtils::RecoverTensorWrapper(&this->arr_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& reduce = this->reduce_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "put_along_axis_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::put_along_axis_grad(arr, indices, out_grad, axis, reduce, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("put_along_axis_grad", returns); }

  // Get GradOut autograd_meta

  auto& arr_grad = returns[0][0];
  egr::AutogradMeta* arr_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&arr_grad) : nullptr;
  if (arr_grad_autograd_meta) arr_grad_autograd_meta->SetStopGradient(false);
  

  auto& value_grad = returns[2][0];
  egr::AutogradMeta* value_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&value_grad) : nullptr;
  if (value_grad_autograd_meta) value_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op put_along_axis_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: put_along_axis_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_ARR_GRAD_TEMPLATE = " \n ( arr_grad , [%s]), ";
    std::string output_arr_grad_str = paddle::string::Sprintf(TENSOR_ARR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(arr_grad));
    output_str += output_arr_grad_str; 
    const char* TENSOR_VALUE_GRAD_TEMPLATE = " \n ( value_grad , [%s]), ";
    std::string output_value_grad_str = paddle::string::Sprintf(TENSOR_VALUE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(value_grad));
    output_str += output_value_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> QrGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "qr_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto q = egr::EagerUtils::RecoverTensorWrapper(&this->q_);
  auto r = egr::EagerUtils::RecoverTensorWrapper(&this->r_);
  auto& q_grad = hooked_grads[0][0];
  auto& r_grad = hooked_grads[1][0];
  auto& mode = this->mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "qr_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_Q_GRAD_TEMPLATE = " \n( q_grad , [%s]), ";
    std::string input_q_grad_str = paddle::string::Sprintf(TENSOR_Q_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(q_grad));
    input_str += input_q_grad_str; 
    const char* TENSOR_R_GRAD_TEMPLATE = " \n( r_grad , [%s]), ";
    std::string input_r_grad_str = paddle::string::Sprintf(TENSOR_R_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(r_grad));
    input_str += input_r_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Q_TEMPLATE = " \n( q , [%s]), ";
    std::string input_q_str = paddle::string::Sprintf(TENSOR_Q_TEMPLATE, egr::EagerUtils::TensorStr(q));
    input_str += input_q_str; 
    const char* TENSOR_R_TEMPLATE = " \n( r , [%s]), ";
    std::string input_r_str = paddle::string::Sprintf(TENSOR_R_TEMPLATE, egr::EagerUtils::TensorStr(r));
    input_str += input_r_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::qr_grad(x, q, r, q_grad, r_grad, mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("qr_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op qr_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: qr_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_Q_GRAD_TEMPLATE = " \n( q_grad , [%s]), ";
    std::string input_q_grad_str = paddle::string::Sprintf(TENSOR_Q_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(q_grad));
    input_str += input_q_grad_str; 
    const char* TENSOR_R_GRAD_TEMPLATE = " \n( r_grad , [%s]), ";
    std::string input_r_grad_str = paddle::string::Sprintf(TENSOR_R_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(r_grad));
    input_str += input_r_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Q_TEMPLATE = " \n( q , [%s]), ";
    std::string input_q_str = paddle::string::Sprintf(TENSOR_Q_TEMPLATE, egr::EagerUtils::TensorStr(q));
    input_str += input_q_str; 
    const char* TENSOR_R_TEMPLATE = " \n( r , [%s]), ";
    std::string input_r_str = paddle::string::Sprintf(TENSOR_R_TEMPLATE, egr::EagerUtils::TensorStr(r));
    input_str += input_r_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReciprocalGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "reciprocal_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "reciprocal_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::reciprocal_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reciprocal_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op reciprocal_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: reciprocal_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "relu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "relu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::relu_grad(out, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("relu_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<ReluDoubleGradNode>(new ReluDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: relu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReluDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "relu_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "relu_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::relu_double_grad(out, grad_x_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op relu_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: relu_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RenormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "renorm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& p = this->p_;
  auto& axis = this->axis_;
  auto& max_norm = this->max_norm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "renorm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::renorm_grad(x, out_grad, p, axis, max_norm, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("renorm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op renorm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: renorm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RollGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "roll_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& shifts = this->shifts_;
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "roll_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::roll_grad(x, out_grad, shifts, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("roll_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op roll_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: roll_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RoundGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "round_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "round_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::round_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("round_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op round_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: round_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RsqrtGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "rsqrt_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "rsqrt_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::rsqrt_grad(out, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("rsqrt_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("rsqrt_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<RsqrtDoubleGradNode>(new RsqrtDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(out, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappergrad_x(grad_x);
  }

  VLOG(4) << "Finish AD API GRAD: rsqrt_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RsqrtDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "rsqrt_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->grad_x_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "rsqrt_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n( grad_x , [%s]), ";
    std::string input_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    input_str += input_grad_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::rsqrt_double_grad(out, grad_x, grad_x_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("rsqrt_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& out_grad = returns[0][0];
  egr::AutogradMeta* out_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&out_grad) : nullptr;
  if (out_grad_autograd_meta) out_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op rsqrt_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: rsqrt_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n( grad_x , [%s]), ";
    std::string input_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    input_str += input_grad_x_str; 
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n ( out_grad , [%s]), ";
    std::string output_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    output_str += output_out_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ScatterGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "scatter_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto updates = egr::EagerUtils::RecoverTensorWrapper(&this->updates_);
  auto& out_grad = hooked_grads[0][0];
  auto& overwrite = this->overwrite_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "scatter_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::scatter_grad(index, updates, out_grad, overwrite, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scatter_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& updates_grad = returns[2][0];
  egr::AutogradMeta* updates_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&updates_grad) : nullptr;
  if (updates_grad_autograd_meta) updates_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op scatter_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: scatter_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_UPDATES_GRAD_TEMPLATE = " \n ( updates_grad , [%s]), ";
    std::string output_updates_grad_str = paddle::string::Sprintf(TENSOR_UPDATES_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(updates_grad));
    output_str += output_updates_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ScatterNdAddGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "scatter_nd_add_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto updates = egr::EagerUtils::RecoverTensorWrapper(&this->updates_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "scatter_nd_add_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::scatter_nd_add_grad(index, updates, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scatter_nd_add_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& updates_grad = returns[2][0];
  egr::AutogradMeta* updates_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&updates_grad) : nullptr;
  if (updates_grad_autograd_meta) updates_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op scatter_nd_add_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: scatter_nd_add_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_UPDATES_TEMPLATE = " \n( updates , [%s]), ";
    std::string input_updates_str = paddle::string::Sprintf(TENSOR_UPDATES_TEMPLATE, egr::EagerUtils::TensorStr(updates));
    input_str += input_updates_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_UPDATES_GRAD_TEMPLATE = " \n ( updates_grad , [%s]), ";
    std::string output_updates_grad_str = paddle::string::Sprintf(TENSOR_UPDATES_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(updates_grad));
    output_str += output_updates_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SeluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "selu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& scale = this->scale_;
  auto& alpha = this->alpha_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "selu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::selu_grad(out, out_grad, scale, alpha, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("selu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op selu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: selu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SendUvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "send_uv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto src_index = egr::EagerUtils::RecoverTensorWrapper(&this->src_index_);
  auto dst_index = egr::EagerUtils::RecoverTensorWrapper(&this->dst_index_);
  auto& out_grad = hooked_grads[0][0];
  auto& message_op = this->message_op_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "send_uv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::send_uv_grad(x, y, src_index, dst_index, out_grad, message_op, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("send_uv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op send_uv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: send_uv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SigmoidGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sigmoid_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& fwd_grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (fwd_grad_out.initialized()) {
    VLOG(10) << fwd_grad_out.name() << "(out_grad) use_count: " << fwd_grad_out.impl().use_count();
    if (fwd_grad_out.impl().use_count() == 1 || (fwd_grad_out.impl().use_count() == 2 && fwd_grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(fwd_grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "sigmoid_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sigmoid_grad(out, fwd_grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sigmoid_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("sigmoid_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SigmoidDoubleGradNode>(new SigmoidDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperfwd_grad_out(fwd_grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(out, 0);
    grad_node->SetGradOutMeta(fwd_grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: sigmoid_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SigmoidDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sigmoid_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto fwd_grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_out_);
  auto& grad_grad_x = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_grad_x.initialized()) {
    VLOG(10) << grad_grad_x.name() << "(grad_x_grad) use_count: " << grad_grad_x.impl().use_count();
    if (grad_grad_x.impl().use_count() == 1 || (grad_grad_x.impl().use_count() == 2 && grad_grad_x.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_grad_x, api_output_1);
    }
  }

  VLOG(5) << "Running C++ API: " << "sigmoid_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sigmoid_double_grad(out, fwd_grad_out, grad_grad_x, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sigmoid_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out = returns[0][0];
  egr::AutogradMeta* grad_out_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out) : nullptr;
  if (grad_out_autograd_meta) grad_out_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_out = returns[1][0];
  egr::AutogradMeta* grad_grad_out_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_out) : nullptr;
  if (grad_grad_out_autograd_meta) grad_grad_out_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("sigmoid_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SigmoidTripleGradNode>(new SigmoidTripleGradNode(2, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrapperfwd_grad_out(fwd_grad_out);
    grad_node->SetTensorWrappergrad_grad_x(grad_grad_x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(out, 0);
    grad_node->SetGradOutMeta(fwd_grad_out, 1);
    grad_node->SetGradOutMeta(grad_grad_x, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_out_autograd_meta, 0);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_grad_out_autograd_meta, 1);
    }
    if (grad_out_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_out_autograd_meta, grad_node);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_grad_out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_out, 0);
    grad_node->SetGradInMeta(grad_grad_out, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_out);
    egr::EagerUtils::CheckAndRetainGrad(grad_grad_out);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: sigmoid_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n ( grad_out , [%s]), ";
    std::string output_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    output_str += output_grad_out_str; 
    const char* TENSOR_GRAD_GRAD_OUT_TEMPLATE = " \n ( grad_grad_out , [%s]), ";
    std::string output_grad_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out));
    output_str += output_grad_grad_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SigmoidTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sigmoid_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto fwd_grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_out_);
  auto grad_grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->grad_grad_x_);
  auto& grad_out_grad = hooked_grads[0][0];
  auto& grad_grad_out_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_grad_out_grad_optional;
  if(grad_grad_out_grad.initialized()) grad_grad_out_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_grad_out_grad);

  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_grad_x.initialized()) {
    VLOG(10) << grad_grad_x.name() << "(grad_grad_x) use_count: " << grad_grad_x.impl().use_count();
    if (grad_grad_x.impl().use_count() == 1 || (grad_grad_x.impl().use_count() == 2 && grad_grad_x.impl().get() == (&this->grad_grad_x_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_grad_x, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "sigmoid_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_out_grad , [%s]), ";
    std::string input_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    input_str += input_grad_out_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sigmoid_triple_grad(out, fwd_grad_out, grad_grad_x, grad_out_grad, grad_grad_out_grad_optional, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sigmoid_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& out_grad = returns[0][0];
  egr::AutogradMeta* out_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&out_grad) : nullptr;
  if (out_grad_autograd_meta) out_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_out_grad = returns[1][0];
  egr::AutogradMeta* fwd_grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_out_grad) : nullptr;
  if (fwd_grad_out_grad_autograd_meta) fwd_grad_out_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_x_grad = returns[2][0];
  egr::AutogradMeta* grad_grad_x_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_x_grad) : nullptr;
  if (grad_grad_x_grad_autograd_meta) grad_grad_x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sigmoid_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sigmoid_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_out_grad , [%s]), ";
    std::string input_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    input_str += input_grad_out_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n ( out_grad , [%s]), ";
    std::string output_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    output_str += output_out_grad_str; 
    const char* TENSOR_FWD_GRAD_OUT_GRAD_TEMPLATE = " \n ( fwd_grad_out_grad , [%s]), ";
    std::string output_fwd_grad_out_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out_grad));
    output_str += output_fwd_grad_out_grad_str; 
    const char* TENSOR_GRAD_GRAD_X_GRAD_TEMPLATE = " \n ( grad_grad_x_grad , [%s]), ";
    std::string output_grad_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x_grad));
    output_str += output_grad_grad_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SiluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "silu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "silu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::silu_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("silu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op silu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: silu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SinGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sin_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "sin_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sin_grad(x, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sin_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("sin_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SinDoubleGradNode>(new SinDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: sin_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SinDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sin_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);

  paddle::optional<paddle::experimental::Tensor> grad_out_forward_optional;
  if( grad_out_forward.impl() ) grad_out_forward_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_forward);

  auto& grad_x_grad_forward = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad_forward.initialized()) {
    VLOG(10) << grad_x_grad_forward.name() << "(grad_x_grad) use_count: " << grad_x_grad_forward.impl().use_count();
    if (grad_x_grad_forward.impl().use_count() == 1 || (grad_x_grad_forward.impl().use_count() == 2 && grad_x_grad_forward.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad_forward, api_output_1);
    }
  }

  VLOG(5) << "Running C++ API: " << "sin_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sin_double_grad(x, grad_out_forward_optional, grad_x_grad_forward, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sin_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("sin_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SinTripleGradNode>(new SinTripleGradNode(2, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out_forward(grad_out_forward);
    grad_node->SetTensorWrappergrad_x_grad_forward(grad_x_grad_forward);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out_forward, 1);
    grad_node->SetGradOutMeta(grad_x_grad_forward, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_out_grad_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_out_grad_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_out_grad_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_out_grad_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_out_grad, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_out_grad);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: sin_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SinTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sin_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_forward_);

  paddle::optional<paddle::experimental::Tensor> grad_out_forward_optional;
  if( grad_out_forward.impl() ) grad_out_forward_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_forward);

  auto grad_x_grad_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_x_grad_forward_);

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_forward_optional;
  if( grad_x_grad_forward.impl() ) grad_x_grad_forward_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad_forward);

  auto& grad_x_grad = hooked_grads[0][0];
  auto& grad_out_grad_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_out_grad_grad_optional;
  if(grad_out_grad_grad.initialized()) grad_out_grad_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_grad_grad);

  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad_forward.initialized()) {
    VLOG(10) << grad_x_grad_forward.name() << "(grad_x_grad_forward) use_count: " << grad_x_grad_forward.impl().use_count();
    if (grad_x_grad_forward.impl().use_count() == 1 || (grad_x_grad_forward.impl().use_count() == 2 && grad_x_grad_forward.impl().get() == (&this->grad_x_grad_forward_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad_forward, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "sin_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE = " \n( grad_out_grad_grad , [%s]), ";
    std::string input_grad_out_grad_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad_grad));
    input_str += input_grad_out_grad_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sin_triple_grad(x, grad_out_forward_optional, grad_x_grad_forward_optional, grad_x_grad, grad_out_grad_grad_optional, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sin_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_forward_grad = returns[1][0];
  egr::AutogradMeta* grad_out_forward_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_forward_grad) : nullptr;
  if (grad_out_forward_grad_autograd_meta) grad_out_forward_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_x_grad_forward_grad = returns[2][0];
  egr::AutogradMeta* grad_x_grad_forward_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x_grad_forward_grad) : nullptr;
  if (grad_x_grad_forward_grad_autograd_meta) grad_x_grad_forward_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sin_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sin_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE = " \n( grad_out_grad_grad , [%s]), ";
    std::string input_grad_out_grad_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad_grad));
    input_str += input_grad_out_grad_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_GRAD_TEMPLATE = " \n ( grad_out_forward_grad , [%s]), ";
    std::string output_grad_out_forward_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward_grad));
    output_str += output_grad_out_forward_grad_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_GRAD_TEMPLATE = " \n ( grad_x_grad_forward_grad , [%s]), ";
    std::string output_grad_x_grad_forward_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward_grad));
    output_str += output_grad_x_grad_forward_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SinhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sinh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "sinh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sinh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sinh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sinh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sinh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SoftplusGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "softplus_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& beta = this->beta_;
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "softplus_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::softplus_grad(x, out_grad, beta, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softplus_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op softplus_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: softplus_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SoftshrinkGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "softshrink_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "softshrink_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::softshrink_grad(x, out_grad, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softshrink_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op softshrink_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: softshrink_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SoftsignGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "softsign_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "softsign_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::softsign_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softsign_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op softsign_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: softsign_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SolveGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "solve_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "solve_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::solve_grad(x, y, out, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("solve_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op solve_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: solve_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SqrtGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sqrt_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "sqrt_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sqrt_grad(out, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sqrt_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("sqrt_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SqrtDoubleGradNode>(new SqrtDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(out, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappergrad_x(grad_x);
  }

  VLOG(4) << "Finish AD API GRAD: sqrt_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SqrtDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sqrt_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->grad_x_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "sqrt_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n( grad_x , [%s]), ";
    std::string input_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    input_str += input_grad_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sqrt_double_grad(out, grad_x, grad_x_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sqrt_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& out_grad = returns[0][0];
  egr::AutogradMeta* out_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&out_grad) : nullptr;
  if (out_grad_autograd_meta) out_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sqrt_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sqrt_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n( grad_x , [%s]), ";
    std::string input_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    input_str += input_grad_x_str; 
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n ( out_grad , [%s]), ";
    std::string output_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    output_str += output_out_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SquareGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "square_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "square_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::square_grad(x, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("square_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("square_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SquareDoubleGradNode>(new SquareDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: square_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SquareDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "square_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "square_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::square_double_grad(x, grad_out, grad_x_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("square_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op square_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: square_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SvdGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "svd_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto u = egr::EagerUtils::RecoverTensorWrapper(&this->u_);
  auto vh = egr::EagerUtils::RecoverTensorWrapper(&this->vh_);
  auto s = egr::EagerUtils::RecoverTensorWrapper(&this->s_);
  auto& u_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> u_grad_optional;
  if(u_grad.initialized()) u_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(u_grad);

  auto& vh_grad = hooked_grads[2][0];

  paddle::optional<paddle::experimental::Tensor> vh_grad_optional;
  if(vh_grad.initialized()) vh_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(vh_grad);

  auto& s_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> s_grad_optional;
  if(s_grad.initialized()) s_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(s_grad);

  auto& full_matrices = this->full_matrices_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "svd_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_U_GRAD_TEMPLATE = " \n( u_grad , [%s]), ";
    std::string input_u_grad_str = paddle::string::Sprintf(TENSOR_U_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(u_grad));
    input_str += input_u_grad_str; 
    const char* TENSOR_VH_GRAD_TEMPLATE = " \n( vh_grad , [%s]), ";
    std::string input_vh_grad_str = paddle::string::Sprintf(TENSOR_VH_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(vh_grad));
    input_str += input_vh_grad_str; 
    const char* TENSOR_S_GRAD_TEMPLATE = " \n( s_grad , [%s]), ";
    std::string input_s_grad_str = paddle::string::Sprintf(TENSOR_S_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(s_grad));
    input_str += input_s_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_VH_TEMPLATE = " \n( vh , [%s]), ";
    std::string input_vh_str = paddle::string::Sprintf(TENSOR_VH_TEMPLATE, egr::EagerUtils::TensorStr(vh));
    input_str += input_vh_str; 
    const char* TENSOR_S_TEMPLATE = " \n( s , [%s]), ";
    std::string input_s_str = paddle::string::Sprintf(TENSOR_S_TEMPLATE, egr::EagerUtils::TensorStr(s));
    input_str += input_s_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::svd_grad(x, u, vh, s, u_grad_optional, vh_grad_optional, s_grad_optional, full_matrices, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("svd_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op svd_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: svd_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_U_GRAD_TEMPLATE = " \n( u_grad , [%s]), ";
    std::string input_u_grad_str = paddle::string::Sprintf(TENSOR_U_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(u_grad));
    input_str += input_u_grad_str; 
    const char* TENSOR_VH_GRAD_TEMPLATE = " \n( vh_grad , [%s]), ";
    std::string input_vh_grad_str = paddle::string::Sprintf(TENSOR_VH_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(vh_grad));
    input_str += input_vh_grad_str; 
    const char* TENSOR_S_GRAD_TEMPLATE = " \n( s_grad , [%s]), ";
    std::string input_s_grad_str = paddle::string::Sprintf(TENSOR_S_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(s_grad));
    input_str += input_s_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_VH_TEMPLATE = " \n( vh , [%s]), ";
    std::string input_vh_str = paddle::string::Sprintf(TENSOR_VH_TEMPLATE, egr::EagerUtils::TensorStr(vh));
    input_str += input_vh_str; 
    const char* TENSOR_S_TEMPLATE = " \n( s , [%s]), ";
    std::string input_s_str = paddle::string::Sprintf(TENSOR_S_TEMPLATE, egr::EagerUtils::TensorStr(s));
    input_str += input_s_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TakeAlongAxisGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "take_along_axis_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto arr = egr::EagerUtils::RecoverTensorWrapper(&this->arr_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "take_along_axis_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::take_along_axis_grad(arr, indices, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("take_along_axis_grad", returns); }

  // Get GradOut autograd_meta

  auto& arr_grad = returns[0][0];
  egr::AutogradMeta* arr_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&arr_grad) : nullptr;
  if (arr_grad_autograd_meta) arr_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op take_along_axis_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: take_along_axis_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_ARR_TEMPLATE = " \n( arr , [%s]), ";
    std::string input_arr_str = paddle::string::Sprintf(TENSOR_ARR_TEMPLATE, egr::EagerUtils::TensorStr(arr));
    input_str += input_arr_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_ARR_GRAD_TEMPLATE = " \n ( arr_grad , [%s]), ";
    std::string output_arr_grad_str = paddle::string::Sprintf(TENSOR_ARR_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(arr_grad));
    output_str += output_arr_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tan_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "tan_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::tan_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tan_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op tan_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: tan_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tanh_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "tanh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function
  if(FLAGS_prim_enabled){
    paddle::prim::tanh_grad<paddle::experimental::Tensor>(out, grad_out, api_output_0);
  }else{
    paddle::experimental::tanh_grad(out, grad_out, api_output_0);
  }
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("tanh_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<TanhDoubleGradNode>(new TanhDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(out, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: tanh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanhDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tanh_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto grad_out_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad_forward = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad_forward.initialized()) {
    VLOG(10) << grad_x_grad_forward.name() << "(grad_x_grad) use_count: " << grad_x_grad_forward.impl().use_count();
    if (grad_x_grad_forward.impl().use_count() == 1 || (grad_x_grad_forward.impl().use_count() == 2 && grad_x_grad_forward.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad_forward, api_output_1);
    }
  }

  VLOG(5) << "Running C++ API: " << "tanh_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::tanh_double_grad(out, grad_out_forward, grad_x_grad_forward, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_new = returns[0][0];
  egr::AutogradMeta* grad_out_new_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_new) : nullptr;
  if (grad_out_new_autograd_meta) grad_out_new_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("tanh_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<TanhTripleGradNode>(new TanhTripleGradNode(2, 3));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperout(out);
    grad_node->SetTensorWrappergrad_out_forward(grad_out_forward);
    grad_node->SetTensorWrappergrad_x_grad_forward(grad_x_grad_forward);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(out, 0);
    grad_node->SetGradOutMeta(grad_out_forward, 1);
    grad_node->SetGradOutMeta(grad_x_grad_forward, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_out_new_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_out_new_autograd_meta, 0);
    }
    if (grad_out_grad_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_out_grad_autograd_meta, 1);
    }
    if (grad_out_new_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_out_new_autograd_meta, grad_node);
    }
    if (grad_out_grad_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_out_grad_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_out_new, 0);
    grad_node->SetGradInMeta(grad_out_grad, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_out_new);
    egr::EagerUtils::CheckAndRetainGrad(grad_out_grad);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: tanh_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_OUT_NEW_TEMPLATE = " \n ( grad_out_new , [%s]), ";
    std::string output_grad_out_new_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_NEW_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_new));
    output_str += output_grad_out_new_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanhTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tanh_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto grad_out_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_forward_);
  auto grad_x_grad_forward = egr::EagerUtils::RecoverTensorWrapper(&this->grad_x_grad_forward_);
  auto& grad_out_new_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_out_new_grad_optional;
  if(grad_out_new_grad.initialized()) grad_out_new_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_new_grad);

  auto& grad_out_grad_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_out_grad_grad_optional;
  if(grad_out_grad_grad.initialized()) grad_out_grad_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_out_grad_grad);

  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad_forward.initialized()) {
    VLOG(10) << grad_x_grad_forward.name() << "(grad_x_grad_forward) use_count: " << grad_x_grad_forward.impl().use_count();
    if (grad_x_grad_forward.impl().use_count() == 1 || (grad_x_grad_forward.impl().use_count() == 2 && grad_x_grad_forward.impl().get() == (&this->grad_x_grad_forward_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_1 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad_forward, api_output_1);
    }

  VLOG(5) << "Running C++ API: " << "tanh_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_NEW_GRAD_TEMPLATE = " \n( grad_out_new_grad , [%s]), ";
    std::string input_grad_out_new_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_NEW_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_new_grad));
    input_str += input_grad_out_new_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE = " \n( grad_out_grad_grad , [%s]), ";
    std::string input_grad_out_grad_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad_grad));
    input_str += input_grad_out_grad_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::tanh_triple_grad(out, grad_out_forward, grad_x_grad_forward, grad_out_new_grad_optional, grad_out_grad_grad_optional, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& out_grad = returns[0][0];
  egr::AutogradMeta* out_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&out_grad) : nullptr;
  if (out_grad_autograd_meta) out_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_forward_grad = returns[1][0];
  egr::AutogradMeta* grad_out_forward_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_forward_grad) : nullptr;
  if (grad_out_forward_grad_autograd_meta) grad_out_forward_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_x_grad_forward_grad = returns[2][0];
  egr::AutogradMeta* grad_x_grad_forward_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x_grad_forward_grad) : nullptr;
  if (grad_x_grad_forward_grad_autograd_meta) grad_x_grad_forward_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op tanh_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: tanh_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_NEW_GRAD_TEMPLATE = " \n( grad_out_new_grad , [%s]), ";
    std::string input_grad_out_new_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_NEW_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_new_grad));
    input_str += input_grad_out_new_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE = " \n( grad_out_grad_grad , [%s]), ";
    std::string input_grad_out_grad_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad_grad));
    input_str += input_grad_out_grad_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_TEMPLATE = " \n( grad_out_forward , [%s]), ";
    std::string input_grad_out_forward_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward));
    input_str += input_grad_out_forward_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE = " \n( grad_x_grad_forward , [%s]), ";
    std::string input_grad_x_grad_forward_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward));
    input_str += input_grad_x_grad_forward_str; 
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n ( out_grad , [%s]), ";
    std::string output_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    output_str += output_out_grad_str; 
    const char* TENSOR_GRAD_OUT_FORWARD_GRAD_TEMPLATE = " \n ( grad_out_forward_grad , [%s]), ";
    std::string output_grad_out_forward_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_FORWARD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_forward_grad));
    output_str += output_grad_out_forward_grad_str; 
    const char* TENSOR_GRAD_X_GRAD_FORWARD_GRAD_TEMPLATE = " \n ( grad_x_grad_forward_grad , [%s]), ";
    std::string output_grad_x_grad_forward_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_FORWARD_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad_forward_grad));
    output_str += output_grad_x_grad_forward_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanhShrinkGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tanh_shrink_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "tanh_shrink_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::tanh_shrink_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_shrink_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op tanh_shrink_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: tanh_shrink_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ThresholdedReluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "thresholded_relu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "thresholded_relu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::thresholded_relu_grad(x, out_grad, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("thresholded_relu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op thresholded_relu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: thresholded_relu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TopkGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "topk_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto& out_grad = hooked_grads[0][0];
  auto& k = this->k_;
  auto& axis = this->axis_;
  auto& largest = this->largest_;
  auto& sorted = this->sorted_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "topk_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::topk_grad(x, indices, out_grad, k, axis, largest, sorted, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("topk_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op topk_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: topk_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TraceGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "trace_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& offset = this->offset_;
  auto& axis1 = this->axis1_;
  auto& axis2 = this->axis2_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "trace_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::trace_grad(x, out_grad, offset, axis1, axis2, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("trace_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op trace_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: trace_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TruncGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "trunc_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "trunc_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::trunc_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("trunc_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op trunc_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: trunc_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UnfoldGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unfold_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& kernel_sizes = this->kernel_sizes_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& dilations = this->dilations_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "unfold_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::unfold_grad(x, out_grad, kernel_sizes, strides, paddings, dilations, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unfold_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op unfold_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: unfold_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UnstackGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unstack_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "unstack_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::unstack_grad(out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unstack_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op unstack_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: unstack_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> WarprnntGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "warprnnt_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto input_lengths = egr::EagerUtils::RecoverTensorWrapper(&this->input_lengths_);
  auto warprnntgrad = egr::EagerUtils::RecoverTensorWrapper(&this->warprnntgrad_);
  auto& loss_grad = hooked_grads[0][0];
  auto& blank = this->blank_;
  auto& fastemit_lambda = this->fastemit_lambda_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "warprnnt_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_INPUT_LENGTHS_TEMPLATE = " \n( input_lengths , [%s]), ";
    std::string input_input_lengths_str = paddle::string::Sprintf(TENSOR_INPUT_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(input_lengths));
    input_str += input_input_lengths_str; 
    const char* TENSOR_WARPRNNTGRAD_TEMPLATE = " \n( warprnntgrad , [%s]), ";
    std::string input_warprnntgrad_str = paddle::string::Sprintf(TENSOR_WARPRNNTGRAD_TEMPLATE, egr::EagerUtils::TensorStr(warprnntgrad));
    input_str += input_warprnntgrad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::warprnnt_grad(input, input_lengths, warprnntgrad, loss_grad, blank, fastemit_lambda, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("warprnnt_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op warprnnt_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: warprnnt_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_INPUT_LENGTHS_TEMPLATE = " \n( input_lengths , [%s]), ";
    std::string input_input_lengths_str = paddle::string::Sprintf(TENSOR_INPUT_LENGTHS_TEMPLATE, egr::EagerUtils::TensorStr(input_lengths));
    input_str += input_input_lengths_str; 
    const char* TENSOR_WARPRNNTGRAD_TEMPLATE = " \n( warprnntgrad , [%s]), ";
    std::string input_warprnntgrad_str = paddle::string::Sprintf(TENSOR_WARPRNNTGRAD_TEMPLATE, egr::EagerUtils::TensorStr(warprnntgrad));
    input_str += input_warprnntgrad_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> WhereGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "where_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto condition = egr::EagerUtils::RecoverTensorWrapper(&this->condition_);
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "where_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_CONDITION_TEMPLATE = " \n( condition , [%s]), ";
    std::string input_condition_str = paddle::string::Sprintf(TENSOR_CONDITION_TEMPLATE, egr::EagerUtils::TensorStr(condition));
    input_str += input_condition_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::where_grad(condition, x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("where_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[1][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[2][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op where_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: where_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_CONDITION_TEMPLATE = " \n( condition , [%s]), ";
    std::string input_condition_str = paddle::string::Sprintf(TENSOR_CONDITION_TEMPLATE, egr::EagerUtils::TensorStr(condition));
    input_str += input_condition_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}



paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AbsGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "abs_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "abs_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::abs_grad(x, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("abs_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("abs_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<AbsDoubleGradNode>(new AbsDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: abs_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AbsDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "abs_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "abs_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::abs_double_grad(x, grad_x_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("abs_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op abs_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: abs_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AddGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "add_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "add_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::add_grad(x, y, grad_out, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("add_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<AddDoubleGradNode>(new AddDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: add_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AddDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "add_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_grad_x = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_grad_x_optional;
  if(grad_grad_x.initialized()) grad_grad_x_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_grad_x);

  auto& grad_grad_y = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_grad_y_optional;
  if(grad_grad_y.initialized()) grad_grad_y_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_grad_y);

  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_grad_x.initialized()) {
    VLOG(10) << grad_grad_x.name() << "(grad_x_grad) use_count: " << grad_grad_x.impl().use_count();
    if (grad_grad_x.impl().use_count() == 1 || (grad_grad_x.impl().use_count() == 2 && grad_grad_x.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_grad_x, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "add_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_GRAD_GRAD_Y_TEMPLATE = " \n( grad_grad_y , [%s]), ";
    std::string input_grad_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_y));
    input_str += input_grad_grad_y_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::add_double_grad(y, grad_out, grad_grad_x_optional, grad_grad_y_optional, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_grad_out = returns[2][0];
  egr::AutogradMeta* grad_grad_out_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_out) : nullptr;
  if (grad_grad_out_autograd_meta) grad_grad_out_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("add_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<AddTripleGradNode>(new AddTripleGradNode(1, 4));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrappergrad_grad_x(grad_grad_x);
    grad_node->SetTensorWrappergrad_grad_y(grad_grad_y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_grad_x, 2);
    grad_node->SetGradOutMeta(grad_grad_y, 3);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_grad_out_autograd_meta, 0);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_grad_out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_grad_out, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_grad_out);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: add_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_GRAD_GRAD_Y_TEMPLATE = " \n( grad_grad_y , [%s]), ";
    std::string input_grad_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_y));
    input_str += input_grad_grad_y_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_GRAD_OUT_TEMPLATE = " \n ( grad_grad_out , [%s]), ";
    std::string output_grad_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out));
    output_str += output_grad_grad_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AddTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "add_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto grad_grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->grad_grad_x_);
  auto grad_grad_y = egr::EagerUtils::RecoverTensorWrapper(&this->grad_grad_y_);
  auto& grad_grad_out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_1 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_grad_out_grad.initialized()) {
    VLOG(10) << grad_grad_out_grad.name() << "(grad_grad_out_grad) use_count: " << grad_grad_out_grad.impl().use_count();
    if (grad_grad_out_grad.impl().use_count() == 1 || (grad_grad_out_grad.impl().use_count() == 2 && grad_grad_out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_grad_out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "add_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_GRAD_GRAD_Y_TEMPLATE = " \n( grad_grad_y , [%s]), ";
    std::string input_grad_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_y));
    input_str += input_grad_grad_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::add_triple_grad(grad_grad_x, grad_grad_y, grad_grad_out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_grad_x_grad = returns[2][0];
  egr::AutogradMeta* grad_grad_x_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_x_grad) : nullptr;
  if (grad_grad_x_grad_autograd_meta) grad_grad_x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_y_grad = returns[3][0];
  egr::AutogradMeta* grad_grad_y_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_y_grad) : nullptr;
  if (grad_grad_y_grad_autograd_meta) grad_grad_y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op add_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: add_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_GRAD_GRAD_Y_TEMPLATE = " \n( grad_grad_y , [%s]), ";
    std::string input_grad_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_y));
    input_str += input_grad_grad_y_str; 
    const char* TENSOR_GRAD_GRAD_X_GRAD_TEMPLATE = " \n ( grad_grad_x_grad , [%s]), ";
    std::string output_grad_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x_grad));
    output_str += output_grad_grad_x_grad_str; 
    const char* TENSOR_GRAD_GRAD_Y_GRAD_TEMPLATE = " \n ( grad_grad_y_grad , [%s]), ";
    std::string output_grad_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_y_grad));
    output_str += output_grad_grad_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AffineGridGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "affine_grid_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto& output_grad = hooked_grads[0][0];
  auto& outputShape = this->outputShape_;
  auto& align_corners = this->align_corners_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "affine_grid_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::affine_grid_grad(input, output_grad, outputShape, align_corners, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("affine_grid_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op affine_grid_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: affine_grid_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "amax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "amax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::amax_grad(x, out, out_grad, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("amax_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op amax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: amax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AminGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "amin_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "amin_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::amin_grad(x, out, out_grad, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("amin_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op amin_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: amin_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AssignGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "assign_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "assign_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = assign_ad_func(out_grad);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::assign(out_grad);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("assign_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: assign_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AssignOutGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "assign_out__grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "assign_out__grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::assign_out__grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("assign_out__grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op assign_out__grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: assign_out__grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BatchNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "batch_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);
  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);
  auto out_mean = egr::EagerUtils::RecoverTensorWrapper(&this->mean_out_);

  paddle::optional<paddle::experimental::Tensor> out_mean_optional;
  if( out_mean.impl() ) out_mean_optional = paddle::make_optional<paddle::experimental::Tensor>(out_mean);

  auto out_variance = egr::EagerUtils::RecoverTensorWrapper(&this->variance_out_);

  paddle::optional<paddle::experimental::Tensor> out_variance_optional;
  if( out_variance.impl() ) out_variance_optional = paddle::make_optional<paddle::experimental::Tensor>(out_variance);

  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto reserve_space = egr::EagerUtils::RecoverTensorWrapper(&this->reserve_space_);

  paddle::optional<paddle::experimental::Tensor> reserve_space_optional;
  if( reserve_space.impl() ) reserve_space_optional = paddle::make_optional<paddle::experimental::Tensor>(reserve_space);

  auto& grad_out = hooked_grads[0][0];
  auto& momentum = this->momentum_;
  auto& epsilon = this->epsilon_;
  auto& data_layout = this->data_layout_;
  auto& is_test = this->is_test_;
  auto& use_global_stats = this->use_global_stats_;
  auto& trainable_statistics = this->trainable_statistics_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  auto* api_output_2 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "batch_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_MEAN_TEMPLATE = " \n( out_mean , [%s]), ";
    std::string input_out_mean_str = paddle::string::Sprintf(TENSOR_OUT_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(out_mean));
    input_str += input_out_mean_str; 
    const char* TENSOR_OUT_VARIANCE_TEMPLATE = " \n( out_variance , [%s]), ";
    std::string input_out_variance_str = paddle::string::Sprintf(TENSOR_OUT_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(out_variance));
    input_str += input_out_variance_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::batch_norm_grad(x, scale, bias, out_mean_optional, out_variance_optional, saved_mean, saved_variance, reserve_space_optional, grad_out, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("batch_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_scale = returns[3][0];
  egr::AutogradMeta* grad_scale_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_scale) : nullptr;
  if (grad_scale_autograd_meta) grad_scale_autograd_meta->SetStopGradient(false);
  

  auto& grad_bias = returns[4][0];
  egr::AutogradMeta* grad_bias_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_bias) : nullptr;
  if (grad_bias_autograd_meta) grad_bias_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("batch_norm_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<BatchNormDoubleGradNode>(new BatchNormDoubleGradNode(3, 9));
    // SetAttributes if needed
    grad_node->SetAttributemomentum(momentum);
    grad_node->SetAttributeepsilon(epsilon);
    grad_node->SetAttributedata_layout(data_layout);
    grad_node->SetAttributeis_test(is_test);
    grad_node->SetAttributeuse_global_stats(use_global_stats);
    grad_node->SetAttributetrainable_statistics(trainable_statistics);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperscale(scale);
    grad_node->SetTensorWrapperout_mean(out_mean);
    grad_node->SetTensorWrapperout_variance(out_variance);
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(scale, 1);
    grad_node->SetGradOutMeta(grad_out, 8);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_scale_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_scale_autograd_meta, 1);
    }
    if (grad_bias_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_bias_autograd_meta, 2);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_scale_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_scale_autograd_meta, grad_node);
    }
    if (grad_bias_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_bias_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_scale, 1);
    grad_node->SetGradInMeta(grad_bias, 2);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_scale);
    egr::EagerUtils::CheckAndRetainGrad(grad_bias);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: batch_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_OUT_MEAN_TEMPLATE = " \n( out_mean , [%s]), ";
    std::string input_out_mean_str = paddle::string::Sprintf(TENSOR_OUT_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(out_mean));
    input_str += input_out_mean_str; 
    const char* TENSOR_OUT_VARIANCE_TEMPLATE = " \n( out_variance , [%s]), ";
    std::string input_out_variance_str = paddle::string::Sprintf(TENSOR_OUT_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(out_variance));
    input_str += input_out_variance_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_SCALE_TEMPLATE = " \n ( grad_scale , [%s]), ";
    std::string output_grad_scale_str = paddle::string::Sprintf(TENSOR_GRAD_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(grad_scale));
    output_str += output_grad_scale_str; 
    const char* TENSOR_GRAD_BIAS_TEMPLATE = " \n ( grad_bias , [%s]), ";
    std::string output_grad_bias_str = paddle::string::Sprintf(TENSOR_GRAD_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(grad_bias));
    output_str += output_grad_bias_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BatchNormDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "batch_norm_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[2][0], input_metas[2][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);
  auto out_mean = egr::EagerUtils::RecoverTensorWrapper(&this->out_mean_);

  paddle::optional<paddle::experimental::Tensor> out_mean_optional;
  if( out_mean.impl() ) out_mean_optional = paddle::make_optional<paddle::experimental::Tensor>(out_mean);

  auto out_variance = egr::EagerUtils::RecoverTensorWrapper(&this->out_variance_);

  paddle::optional<paddle::experimental::Tensor> out_variance_optional;
  if( out_variance.impl() ) out_variance_optional = paddle::make_optional<paddle::experimental::Tensor>(out_variance);

  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_optional;
  if(grad_x_grad.initialized()) grad_x_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad);

  auto& grad_scale_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_scale_grad_optional;
  if(grad_scale_grad.initialized()) grad_scale_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_scale_grad);

  auto& grad_bias_grad = hooked_grads[2][0];

  paddle::optional<paddle::experimental::Tensor> grad_bias_grad_optional;
  if(grad_bias_grad.initialized()) grad_bias_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_bias_grad);

  auto& momentum = this->momentum_;
  auto& epsilon = this->epsilon_;
  auto& data_layout = this->data_layout_;
  auto& is_test = this->is_test_;
  auto& use_global_stats = this->use_global_stats_;
  auto& trainable_statistics = this->trainable_statistics_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(9);
  for (int i = 0; i < 9; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[8].empty() || out_metas[8][0].IsStopGradient()) ? nullptr : &returns[8][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(grad_out) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == (&this->grad_out_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_2 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_2);
    }

  VLOG(5) << "Running C++ API: " << "batch_norm_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_SCALE_GRAD_TEMPLATE = " \n( grad_scale_grad , [%s]), ";
    std::string input_grad_scale_grad_str = paddle::string::Sprintf(TENSOR_GRAD_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_scale_grad));
    input_str += input_grad_scale_grad_str; 
    const char* TENSOR_GRAD_BIAS_GRAD_TEMPLATE = " \n( grad_bias_grad , [%s]), ";
    std::string input_grad_bias_grad_str = paddle::string::Sprintf(TENSOR_GRAD_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_bias_grad));
    input_str += input_grad_bias_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_OUT_MEAN_TEMPLATE = " \n( out_mean , [%s]), ";
    std::string input_out_mean_str = paddle::string::Sprintf(TENSOR_OUT_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(out_mean));
    input_str += input_out_mean_str; 
    const char* TENSOR_OUT_VARIANCE_TEMPLATE = " \n( out_variance , [%s]), ";
    std::string input_out_variance_str = paddle::string::Sprintf(TENSOR_OUT_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(out_variance));
    input_str += input_out_variance_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::batch_norm_double_grad(x, scale, out_mean_optional, out_variance_optional, saved_mean, saved_variance, grad_out, grad_x_grad_optional, grad_scale_grad_optional, grad_bias_grad_optional, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("batch_norm_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& scale_grad = returns[1][0];
  egr::AutogradMeta* scale_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&scale_grad) : nullptr;
  if (scale_grad_autograd_meta) scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[8][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[8][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op batch_norm_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: batch_norm_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_SCALE_GRAD_TEMPLATE = " \n( grad_scale_grad , [%s]), ";
    std::string input_grad_scale_grad_str = paddle::string::Sprintf(TENSOR_GRAD_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_scale_grad));
    input_str += input_grad_scale_grad_str; 
    const char* TENSOR_GRAD_BIAS_GRAD_TEMPLATE = " \n( grad_bias_grad , [%s]), ";
    std::string input_grad_bias_grad_str = paddle::string::Sprintf(TENSOR_GRAD_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_bias_grad));
    input_str += input_grad_bias_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_OUT_MEAN_TEMPLATE = " \n( out_mean , [%s]), ";
    std::string input_out_mean_str = paddle::string::Sprintf(TENSOR_OUT_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(out_mean));
    input_str += input_out_mean_str; 
    const char* TENSOR_OUT_VARIANCE_TEMPLATE = " \n( out_variance , [%s]), ";
    std::string input_out_variance_str = paddle::string::Sprintf(TENSOR_OUT_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(out_variance));
    input_str += input_out_variance_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_SCALE_GRAD_TEMPLATE = " \n ( scale_grad , [%s]), ";
    std::string output_scale_grad_str = paddle::string::Sprintf(TENSOR_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(scale_grad));
    output_str += output_scale_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BceLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "bce_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "bce_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::bce_loss_grad(input, label, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bce_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op bce_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: bce_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BicubicInterpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "bicubic_interp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out_size = egr::EagerUtils::RecoverTensorWrapper(&this->out_size_);

  paddle::optional<paddle::experimental::Tensor> out_size_optional;
  if( out_size.impl() ) out_size_optional = paddle::make_optional<paddle::experimental::Tensor>(out_size);

  auto size_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->size_tensor_);

  paddle::optional<std::vector<paddle::experimental::Tensor>> size_tensor_optional;
  if( !size_tensor.empty() ) size_tensor_optional = paddle::make_optional<std::vector<paddle::experimental::Tensor>>(size_tensor);

  auto scale_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->scale_tensor_);

  paddle::optional<paddle::experimental::Tensor> scale_tensor_optional;
  if( scale_tensor.impl() ) scale_tensor_optional = paddle::make_optional<paddle::experimental::Tensor>(scale_tensor);

  auto& output_grad = hooked_grads[0][0];
  auto& data_layout = this->data_layout_;
  auto& out_d = this->out_d_;
  auto& out_h = this->out_h_;
  auto& out_w = this->out_w_;
  auto& scale = this->scale_;
  auto& interp_method = this->interp_method_;
  auto& align_corners = this->align_corners_;
  auto& align_mode = this->align_mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "bicubic_interp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::bicubic_interp_grad(x, out_size_optional, size_tensor_optional, scale_tensor_optional, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bicubic_interp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op bicubic_interp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: bicubic_interp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BilinearInterpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "bilinear_interp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out_size = egr::EagerUtils::RecoverTensorWrapper(&this->out_size_);

  paddle::optional<paddle::experimental::Tensor> out_size_optional;
  if( out_size.impl() ) out_size_optional = paddle::make_optional<paddle::experimental::Tensor>(out_size);

  auto size_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->size_tensor_);

  paddle::optional<std::vector<paddle::experimental::Tensor>> size_tensor_optional;
  if( !size_tensor.empty() ) size_tensor_optional = paddle::make_optional<std::vector<paddle::experimental::Tensor>>(size_tensor);

  auto scale_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->scale_tensor_);

  paddle::optional<paddle::experimental::Tensor> scale_tensor_optional;
  if( scale_tensor.impl() ) scale_tensor_optional = paddle::make_optional<paddle::experimental::Tensor>(scale_tensor);

  auto& output_grad = hooked_grads[0][0];
  auto& data_layout = this->data_layout_;
  auto& out_d = this->out_d_;
  auto& out_h = this->out_h_;
  auto& out_w = this->out_w_;
  auto& scale = this->scale_;
  auto& interp_method = this->interp_method_;
  auto& align_corners = this->align_corners_;
  auto& align_mode = this->align_mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "bilinear_interp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::bilinear_interp_grad(x, out_size_optional, size_tensor_optional, scale_tensor_optional, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bilinear_interp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op bilinear_interp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: bilinear_interp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BilinearTensorProductGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "bilinear_tensor_product_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto weight = egr::EagerUtils::RecoverTensorWrapper(&this->weight_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_3 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "bilinear_tensor_product_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::bilinear_tensor_product_grad(x, y, weight, out_grad, api_output_0, api_output_1, api_output_2, api_output_3);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("bilinear_tensor_product_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  

  auto& weight_grad = returns[2][0];
  egr::AutogradMeta* weight_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&weight_grad) : nullptr;
  if (weight_grad_autograd_meta) weight_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[3][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op bilinear_tensor_product_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: bilinear_tensor_product_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
    const char* TENSOR_WEIGHT_GRAD_TEMPLATE = " \n ( weight_grad , [%s]), ";
    std::string output_weight_grad_str = paddle::string::Sprintf(TENSOR_WEIGHT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(weight_grad));
    output_str += output_weight_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BroadcastTensorsGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "broadcast_tensors_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto& out_grad = hooked_grads[0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "broadcast_tensors_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::broadcast_tensors_grad(input, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("broadcast_tensors_grad", returns); }

  // Get GradOut autograd_meta

    auto& input_grad = returns[0];
    std::vector<egr::AutogradMeta*> input_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&input_grad);
    for(auto* meta : input_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op broadcast_tensors_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: broadcast_tensors_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CastGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cast_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cast_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = cast_ad_func (out_grad, x.dtype());
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::cast (out_grad, x.dtype());
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cast_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: cast_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ClipGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "clip_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& min = this->min_;
  auto& max = this->max_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "clip_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::clip_grad(x, grad_out, min, max, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("clip_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("clip_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<ClipDoubleGradNode>(new ClipDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributemin(min);
    grad_node->SetAttributemax(max);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: clip_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ClipDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "clip_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& min = this->min_;
  auto& max = this->max_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "clip_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::clip_double_grad(x, grad_x_grad, min, max, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("clip_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op clip_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: clip_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ConcatGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "concat_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "concat_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::concat_grad(x, grad_out, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("concat_grad", returns); }

  // Get GradOut autograd_meta

    auto& grad_x = returns[0];
    std::vector<egr::AutogradMeta*> grad_x_autograd_meta_vec = egr::EagerUtils::autograd_meta(&grad_x);
    std::vector<egr::AutogradMeta*>* grad_x_autograd_meta = &grad_x_autograd_meta_vec;
    for(auto* meta : grad_x_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("concat_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<ConcatDoubleGradNode>(new ConcatDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: concat_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ConcatDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "concat_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0], input_metas[0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "concat_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = concat_ad_func(grad_x_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::concat(grad_x_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("concat_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: concat_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Conv2dTransposeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conv2d_transpose_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto& grad_out = hooked_grads[0][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& output_padding = this->output_padding_;
  auto& output_size = this->output_size_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conv2d_transpose_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::conv2d_transpose_grad(x, filter, grad_out, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv2d_transpose_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_filter = returns[1][0];
  egr::AutogradMeta* grad_filter_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_filter) : nullptr;
  if (grad_filter_autograd_meta) grad_filter_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("conv2d_transpose_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<Conv2dTransposeDoubleGradNode>(new Conv2dTransposeDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeoutput_padding(output_padding);
    grad_node->SetAttributeoutput_size(output_size);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperfilter(filter);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(filter, 1);
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_filter_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_filter_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_filter_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_filter_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_filter, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_filter);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: conv2d_transpose_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_FILTER_TEMPLATE = " \n ( grad_filter , [%s]), ";
    std::string output_grad_filter_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter));
    output_str += output_grad_filter_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Conv2dTransposeDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conv2d_transpose_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& grad_filter_grad = hooked_grads[1][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& output_padding = this->output_padding_;
  auto& output_size = this->output_size_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conv2d_transpose_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_FILTER_GRAD_TEMPLATE = " \n( grad_filter_grad , [%s]), ";
    std::string input_grad_filter_grad_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter_grad));
    input_str += input_grad_filter_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::conv2d_transpose_double_grad(x, filter, grad_out, grad_x_grad, grad_filter_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv2d_transpose_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& filter_grad = returns[1][0];
  egr::AutogradMeta* filter_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&filter_grad) : nullptr;
  if (filter_grad_autograd_meta) filter_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[2][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op conv2d_transpose_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: conv2d_transpose_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_FILTER_GRAD_TEMPLATE = " \n( grad_filter_grad , [%s]), ";
    std::string input_grad_filter_grad_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter_grad));
    input_str += input_grad_filter_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_FILTER_GRAD_TEMPLATE = " \n ( filter_grad , [%s]), ";
    std::string output_filter_grad_str = paddle::string::Sprintf(TENSOR_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(filter_grad));
    output_str += output_filter_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Conv3dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conv3d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto& grad_out = hooked_grads[0][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conv3d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::conv3d_grad(input, filter, grad_out, strides, paddings, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_input = returns[0][0];
  egr::AutogradMeta* grad_input_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_input) : nullptr;
  if (grad_input_autograd_meta) grad_input_autograd_meta->SetStopGradient(false);
  

  auto& grad_filter = returns[1][0];
  egr::AutogradMeta* grad_filter_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_filter) : nullptr;
  if (grad_filter_autograd_meta) grad_filter_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("conv3d_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<Conv3dDoubleGradNode>(new Conv3dDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperfilter(filter);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(filter, 1);
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_input_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_input_autograd_meta, 0);
    }
    if (grad_filter_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_filter_autograd_meta, 1);
    }
    if (grad_input_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_input_autograd_meta, grad_node);
    }
    if (grad_filter_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_filter_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_input, 0);
    grad_node->SetGradInMeta(grad_filter, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_input);
    egr::EagerUtils::CheckAndRetainGrad(grad_filter);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: conv3d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_INPUT_TEMPLATE = " \n ( grad_input , [%s]), ";
    std::string output_grad_input_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_input));
    output_str += output_grad_input_str; 
    const char* TENSOR_GRAD_FILTER_TEMPLATE = " \n ( grad_filter , [%s]), ";
    std::string output_grad_filter_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter));
    output_str += output_grad_filter_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Conv3dDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conv3d_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_input_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_input_grad_optional;
  if(grad_input_grad.initialized()) grad_input_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_input_grad);

  auto& grad_filter_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_filter_grad_optional;
  if(grad_filter_grad.initialized()) grad_filter_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_filter_grad);

  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conv3d_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_INPUT_GRAD_TEMPLATE = " \n( grad_input_grad , [%s]), ";
    std::string input_grad_input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_input_grad));
    input_str += input_grad_input_grad_str; 
    const char* TENSOR_GRAD_FILTER_GRAD_TEMPLATE = " \n( grad_filter_grad , [%s]), ";
    std::string input_grad_filter_grad_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter_grad));
    input_str += input_grad_filter_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::conv3d_double_grad(input, filter, grad_out, grad_input_grad_optional, grad_filter_grad_optional, strides, paddings, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  

  auto& filter_grad = returns[1][0];
  egr::AutogradMeta* filter_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&filter_grad) : nullptr;
  if (filter_grad_autograd_meta) filter_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[2][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op conv3d_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: conv3d_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_INPUT_GRAD_TEMPLATE = " \n( grad_input_grad , [%s]), ";
    std::string input_grad_input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_input_grad));
    input_str += input_grad_input_grad_str; 
    const char* TENSOR_GRAD_FILTER_GRAD_TEMPLATE = " \n( grad_filter_grad , [%s]), ";
    std::string input_grad_filter_grad_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter_grad));
    input_str += input_grad_filter_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
    const char* TENSOR_FILTER_GRAD_TEMPLATE = " \n ( filter_grad , [%s]), ";
    std::string output_filter_grad_str = paddle::string::Sprintf(TENSOR_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(filter_grad));
    output_str += output_filter_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Conv3dTransposeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conv3d_transpose_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto& out_grad = hooked_grads[0][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& output_padding = this->output_padding_;
  auto& output_size = this->output_size_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conv3d_transpose_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::conv3d_transpose_grad(x, filter, out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d_transpose_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& filter_grad = returns[1][0];
  egr::AutogradMeta* filter_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&filter_grad) : nullptr;
  if (filter_grad_autograd_meta) filter_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op conv3d_transpose_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: conv3d_transpose_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_FILTER_GRAD_TEMPLATE = " \n ( filter_grad , [%s]), ";
    std::string output_filter_grad_str = paddle::string::Sprintf(TENSOR_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(filter_grad));
    output_str += output_filter_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CrossEntropyWithSoftmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cross_entropy_with_softmax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto softmax = egr::EagerUtils::RecoverTensorWrapper(&this->softmax_);
  auto& loss_grad = hooked_grads[1][0];
  auto& soft_label = this->soft_label_;
  auto& use_softmax = this->use_softmax_;
  auto& numeric_stable_mode = this->numeric_stable_mode_;
  auto& ignore_index = this->ignore_index_;
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (softmax.initialized()) {
    VLOG(10) << softmax.name() << "(softmax) use_count: " << softmax.impl().use_count();
    if (softmax.impl().use_count() == 1 || (softmax.impl().use_count() == 2 && softmax.impl().get() == (&this->softmax_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(softmax, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "cross_entropy_with_softmax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string input_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    input_str += input_softmax_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cross_entropy_with_softmax_grad(label, softmax, loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cross_entropy_with_softmax_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cross_entropy_with_softmax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cross_entropy_with_softmax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string input_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    input_str += input_softmax_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CumprodGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cumprod_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& dim = this->dim_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cumprod_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::cumprod_grad(x, out, out_grad, dim, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cumprod_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cumprod_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cumprod_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CumsumGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cumsum_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& flatten = this->flatten_;
  auto& exclusive = this->exclusive_;
  auto& reverse = this->reverse_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cumsum_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = cumsum_ad_func(out_grad, axis, flatten, exclusive, !reverse);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::cumsum(out_grad, axis, flatten, exclusive, !reverse);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cumsum_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: cumsum_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DeformableConvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "deformable_conv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto offset = egr::EagerUtils::RecoverTensorWrapper(&this->offset_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto mask = egr::EagerUtils::RecoverTensorWrapper(&this->mask_);

  paddle::optional<paddle::experimental::Tensor> mask_optional;
  if( mask.impl() ) mask_optional = paddle::make_optional<paddle::experimental::Tensor>(mask);

  auto& out_grad = hooked_grads[0][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& dilations = this->dilations_;
  auto& deformable_groups = this->deformable_groups_;
  auto& groups = this->groups_;
  auto& im2col_step = this->im2col_step_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_3 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "deformable_conv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OFFSET_TEMPLATE = " \n( offset , [%s]), ";
    std::string input_offset_str = paddle::string::Sprintf(TENSOR_OFFSET_TEMPLATE, egr::EagerUtils::TensorStr(offset));
    input_str += input_offset_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::deformable_conv_grad(x, offset, filter, mask_optional, out_grad, strides, paddings, dilations, deformable_groups, groups, im2col_step, api_output_0, api_output_1, api_output_2, api_output_3);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("deformable_conv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& offset_grad = returns[1][0];
  egr::AutogradMeta* offset_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&offset_grad) : nullptr;
  if (offset_grad_autograd_meta) offset_grad_autograd_meta->SetStopGradient(false);
  

  auto& filter_grad = returns[2][0];
  egr::AutogradMeta* filter_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&filter_grad) : nullptr;
  if (filter_grad_autograd_meta) filter_grad_autograd_meta->SetStopGradient(false);
  

  auto& mask_grad = returns[3][0];
  egr::AutogradMeta* mask_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&mask_grad) : nullptr;
  if (mask_grad_autograd_meta) mask_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op deformable_conv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: deformable_conv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OFFSET_TEMPLATE = " \n( offset , [%s]), ";
    std::string input_offset_str = paddle::string::Sprintf(TENSOR_OFFSET_TEMPLATE, egr::EagerUtils::TensorStr(offset));
    input_str += input_offset_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_OFFSET_GRAD_TEMPLATE = " \n ( offset_grad , [%s]), ";
    std::string output_offset_grad_str = paddle::string::Sprintf(TENSOR_OFFSET_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(offset_grad));
    output_str += output_offset_grad_str; 
    const char* TENSOR_FILTER_GRAD_TEMPLATE = " \n ( filter_grad , [%s]), ";
    std::string output_filter_grad_str = paddle::string::Sprintf(TENSOR_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(filter_grad));
    output_str += output_filter_grad_str; 
    const char* TENSOR_MASK_GRAD_TEMPLATE = " \n ( mask_grad , [%s]), ";
    std::string output_mask_grad_str = paddle::string::Sprintf(TENSOR_MASK_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(mask_grad));
    output_str += output_mask_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DepthwiseConv2dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "depthwise_conv2d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto& grad_out = hooked_grads[0][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "depthwise_conv2d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::depthwise_conv2d_grad(input, filter, grad_out, strides, paddings, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("depthwise_conv2d_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_input = returns[0][0];
  egr::AutogradMeta* grad_input_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_input) : nullptr;
  if (grad_input_autograd_meta) grad_input_autograd_meta->SetStopGradient(false);
  

  auto& grad_filter = returns[1][0];
  egr::AutogradMeta* grad_filter_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_filter) : nullptr;
  if (grad_filter_autograd_meta) grad_filter_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("depthwise_conv2d_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<DepthwiseConv2dDoubleGradNode>(new DepthwiseConv2dDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    grad_node->SetAttributegroups(groups);
    grad_node->SetAttributedilations(dilations);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperinput(input);
    grad_node->SetTensorWrapperfilter(filter);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(input, 0);
    grad_node->SetGradOutMeta(filter, 1);
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_input_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_input_autograd_meta, 0);
    }
    if (grad_filter_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_filter_autograd_meta, 1);
    }
    if (grad_input_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_input_autograd_meta, grad_node);
    }
    if (grad_filter_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_filter_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_input, 0);
    grad_node->SetGradInMeta(grad_filter, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_input);
    egr::EagerUtils::CheckAndRetainGrad(grad_filter);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: depthwise_conv2d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_INPUT_TEMPLATE = " \n ( grad_input , [%s]), ";
    std::string output_grad_input_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_input));
    output_str += output_grad_input_str; 
    const char* TENSOR_GRAD_FILTER_TEMPLATE = " \n ( grad_filter , [%s]), ";
    std::string output_grad_filter_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter));
    output_str += output_grad_filter_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DepthwiseConv2dDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "depthwise_conv2d_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_input_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_input_grad_optional;
  if(grad_input_grad.initialized()) grad_input_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_input_grad);

  auto& grad_filter_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_filter_grad_optional;
  if(grad_filter_grad.initialized()) grad_filter_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_filter_grad);

  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "depthwise_conv2d_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_INPUT_GRAD_TEMPLATE = " \n( grad_input_grad , [%s]), ";
    std::string input_grad_input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_input_grad));
    input_str += input_grad_input_grad_str; 
    const char* TENSOR_GRAD_FILTER_GRAD_TEMPLATE = " \n( grad_filter_grad , [%s]), ";
    std::string input_grad_filter_grad_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter_grad));
    input_str += input_grad_filter_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::depthwise_conv2d_double_grad(input, filter, grad_out, grad_input_grad_optional, grad_filter_grad_optional, strides, paddings, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("depthwise_conv2d_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  

  auto& filter_grad = returns[1][0];
  egr::AutogradMeta* filter_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&filter_grad) : nullptr;
  if (filter_grad_autograd_meta) filter_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[2][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op depthwise_conv2d_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: depthwise_conv2d_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_INPUT_GRAD_TEMPLATE = " \n( grad_input_grad , [%s]), ";
    std::string input_grad_input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_input_grad));
    input_str += input_grad_input_grad_str; 
    const char* TENSOR_GRAD_FILTER_GRAD_TEMPLATE = " \n( grad_filter_grad , [%s]), ";
    std::string input_grad_filter_grad_str = paddle::string::Sprintf(TENSOR_GRAD_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_filter_grad));
    input_str += input_grad_filter_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
    const char* TENSOR_FILTER_GRAD_TEMPLATE = " \n ( filter_grad , [%s]), ";
    std::string output_filter_grad_str = paddle::string::Sprintf(TENSOR_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(filter_grad));
    output_str += output_filter_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DepthwiseConv2dTransposeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "depthwise_conv2d_transpose_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto filter = egr::EagerUtils::RecoverTensorWrapper(&this->filter_);
  auto& out_grad = hooked_grads[0][0];
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& output_padding = this->output_padding_;
  auto& output_size = this->output_size_;
  auto& padding_algorithm = this->padding_algorithm_;
  auto& groups = this->groups_;
  auto& dilations = this->dilations_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "depthwise_conv2d_transpose_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::depthwise_conv2d_transpose_grad(x, filter, out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("depthwise_conv2d_transpose_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& filter_grad = returns[1][0];
  egr::AutogradMeta* filter_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&filter_grad) : nullptr;
  if (filter_grad_autograd_meta) filter_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op depthwise_conv2d_transpose_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: depthwise_conv2d_transpose_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FILTER_TEMPLATE = " \n( filter , [%s]), ";
    std::string input_filter_str = paddle::string::Sprintf(TENSOR_FILTER_TEMPLATE, egr::EagerUtils::TensorStr(filter));
    input_str += input_filter_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_FILTER_GRAD_TEMPLATE = " \n ( filter_grad , [%s]), ";
    std::string output_filter_grad_str = paddle::string::Sprintf(TENSOR_FILTER_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(filter_grad));
    output_str += output_filter_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "divide_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "divide_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::divide_grad(x, y, out, grad_out, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("divide_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<DivideDoubleGradNode>(new DivideDoubleGradNode(2, 4));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrapperout(out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(y, 1);
    grad_node->SetGradOutMeta(out, 2);
    grad_node->SetGradOutMeta(grad_out, 3);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    // Set TensorWrappers for Forward Outputs if needed
    grad_node->SetTensorWrappergrad_x(grad_x);
  }

  VLOG(4) << "Finish AD API GRAD: divide_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DivideDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "divide_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->grad_x_);
  auto& grad_x_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_optional;
  if(grad_x_grad.initialized()) grad_x_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad);

  auto& grad_y_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_y_grad_optional;
  if(grad_y_grad.initialized()) grad_y_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_y_grad);

  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_2 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_2 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_2);
    }

  VLOG(5) << "Running C++ API: " << "divide_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n( grad_x , [%s]), ";
    std::string input_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    input_str += input_grad_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::divide_double_grad(y, out, grad_x, grad_x_grad_optional, grad_y_grad_optional, axis, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  

  auto& out_grad = returns[2][0];
  egr::AutogradMeta* out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&out_grad) : nullptr;
  if (out_grad_autograd_meta) out_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[3][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op divide_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: divide_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n( grad_x , [%s]), ";
    std::string input_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    input_str += input_grad_x_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n ( out_grad , [%s]), ";
    std::string output_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    output_str += output_out_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DropoutGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "dropout_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto mask = egr::EagerUtils::RecoverTensorWrapper(&this->mask_);
  auto& out_grad = hooked_grads[0][0];
  auto& p = this->p_;
  auto& is_test = this->is_test_;
  auto& mode = this->mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "dropout_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::dropout_grad(mask, out_grad, p, is_test, mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("dropout_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op dropout_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: dropout_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EigvalshGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "eigvalsh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto eigenvectors = egr::EagerUtils::RecoverTensorWrapper(&this->eigenvectors_);
  auto& eigenvalues_grad = hooked_grads[0][0];
  auto& uplo = this->uplo_;
  auto& is_test = this->is_test_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "eigvalsh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_EIGENVALUES_GRAD_TEMPLATE = " \n( eigenvalues_grad , [%s]), ";
    std::string input_eigenvalues_grad_str = paddle::string::Sprintf(TENSOR_EIGENVALUES_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(eigenvalues_grad));
    input_str += input_eigenvalues_grad_str; 
    const char* TENSOR_EIGENVECTORS_TEMPLATE = " \n( eigenvectors , [%s]), ";
    std::string input_eigenvectors_str = paddle::string::Sprintf(TENSOR_EIGENVECTORS_TEMPLATE, egr::EagerUtils::TensorStr(eigenvectors));
    input_str += input_eigenvectors_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::eigvalsh_grad(eigenvectors, eigenvalues_grad, uplo, is_test, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("eigvalsh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op eigvalsh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: eigvalsh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_EIGENVALUES_GRAD_TEMPLATE = " \n( eigenvalues_grad , [%s]), ";
    std::string input_eigenvalues_grad_str = paddle::string::Sprintf(TENSOR_EIGENVALUES_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(eigenvalues_grad));
    input_str += input_eigenvalues_grad_str; 
    const char* TENSOR_EIGENVECTORS_TEMPLATE = " \n( eigenvectors , [%s]), ";
    std::string input_eigenvectors_str = paddle::string::Sprintf(TENSOR_EIGENVECTORS_TEMPLATE, egr::EagerUtils::TensorStr(eigenvectors));
    input_str += input_eigenvectors_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EinsumGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "einsum_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x_shape = egr::EagerUtils::RecoverTensorWrapper(&this->x_shape_);
  auto inner_cache = egr::EagerUtils::RecoverTensorWrapper(&this->inner_cache_);
  auto& out_grad = hooked_grads[0][0];
  auto& equation = this->equation_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "einsum_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_SHAPE_TEMPLATE = " \n( x_shape , [%s]), ";
    std::string input_x_shape_str = paddle::string::Sprintf(TENSOR_X_SHAPE_TEMPLATE, egr::EagerUtils::TensorStr(x_shape));
    input_str += input_x_shape_str; 
    const char* TENSOR_INNER_CACHE_TEMPLATE = " \n( inner_cache , [%s]), ";
    std::string input_inner_cache_str = paddle::string::Sprintf(TENSOR_INNER_CACHE_TEMPLATE, egr::EagerUtils::TensorStr(inner_cache));
    input_str += input_inner_cache_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::einsum_grad(x_shape, inner_cache, out_grad, equation, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("einsum_grad", returns); }

  // Get GradOut autograd_meta

    auto& x_grad = returns[0];
    std::vector<egr::AutogradMeta*> x_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&x_grad);
    for(auto* meta : x_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op einsum_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: einsum_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_SHAPE_TEMPLATE = " \n( x_shape , [%s]), ";
    std::string input_x_shape_str = paddle::string::Sprintf(TENSOR_X_SHAPE_TEMPLATE, egr::EagerUtils::TensorStr(x_shape));
    input_str += input_x_shape_str; 
    const char* TENSOR_INNER_CACHE_TEMPLATE = " \n( inner_cache , [%s]), ";
    std::string input_inner_cache_str = paddle::string::Sprintf(TENSOR_INNER_CACHE_TEMPLATE, egr::EagerUtils::TensorStr(inner_cache));
    input_str += input_inner_cache_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ElementwisePowGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "elementwise_pow_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "elementwise_pow_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::elementwise_pow_grad(x, y, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("elementwise_pow_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op elementwise_pow_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: elementwise_pow_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> EmbeddingGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "embedding_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto weight = egr::EagerUtils::RecoverTensorWrapper(&this->weight_);
  auto& out_grad = hooked_grads[0][0];
  auto& padding_idx = this->padding_idx_;
  auto& sparse = this->sparse_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "embedding_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::embedding_grad(x, weight, out_grad, padding_idx, sparse, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("embedding_grad", returns); }

  // Get GradOut autograd_meta

  auto& weight_grad = returns[1][0];
  egr::AutogradMeta* weight_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&weight_grad) : nullptr;
  if (weight_grad_autograd_meta) weight_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op embedding_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: embedding_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_WEIGHT_GRAD_TEMPLATE = " \n ( weight_grad , [%s]), ";
    std::string output_weight_grad_str = paddle::string::Sprintf(TENSOR_WEIGHT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(weight_grad));
    output_str += output_weight_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ExpandGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "expand_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& shape = this->shape_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "expand_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::expand_grad(x, grad_out, shape, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expand_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("expand_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<ExpandDoubleGradNode>(new ExpandDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeshape(shape);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: expand_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ExpandDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "expand_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& shape = this->shape_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "expand_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = expand_ad_func(grad_x_grad, shape);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::expand(grad_x_grad, shape);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expand_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: expand_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ExpandAsGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "expand_as_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& target_shape = this->target_shape_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "expand_as_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::expand_as_grad(x, out_grad, target_shape, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expand_as_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op expand_as_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: expand_as_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ExponentialGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "exponential__grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "exponential__grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = zeros_like_ad_func(out_grad);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::zeros_like(out_grad);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("exponential__grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: exponential__grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FillGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fill_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& value = this->value_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "fill_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fill_grad(out_grad, value, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fill_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fill_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FillDiagonalGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fill_diagonal_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& value = this->value_;
  auto& offset = this->offset_;
  auto& wrap = this->wrap_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fill_diagonal_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fill_diagonal_grad(out_grad, value, offset, wrap, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fill_diagonal_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fill_diagonal_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fill_diagonal_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FlattenGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "flatten_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto xshape = egr::EagerUtils::RecoverTensorWrapper(&this->xshape_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "flatten_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::flatten_grad(xshape, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("flatten_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op flatten_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: flatten_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fmax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fmax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fmax_grad(x, y, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fmax_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fmax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fmax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FminGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fmin_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fmin_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::fmin_grad(x, y, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fmin_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fmin_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fmin_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FrobeniusNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "frobenius_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keep_dim = this->keep_dim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "frobenius_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::frobenius_norm_grad(x, out, out_grad, axis, keep_dim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("frobenius_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op frobenius_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: frobenius_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> GatherGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "gather_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& overwrite = this->overwrite_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "gather_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::gather_grad(x, index, out_grad, axis, overwrite, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("gather_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op gather_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: gather_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> GroupNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "group_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);

  paddle::optional<paddle::experimental::Tensor> scale_optional;
  if( scale.impl() ) scale_optional = paddle::make_optional<paddle::experimental::Tensor>(scale);

  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);

  paddle::optional<paddle::experimental::Tensor> bias_optional;
  if( bias.impl() ) bias_optional = paddle::make_optional<paddle::experimental::Tensor>(bias);

  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto mean = egr::EagerUtils::RecoverTensorWrapper(&this->mean_);
  auto variance = egr::EagerUtils::RecoverTensorWrapper(&this->variance_);
  auto& y_grad = hooked_grads[0][0];
  auto& epsilon = this->epsilon_;
  auto& groups = this->groups_;
  auto& data_layout = this->data_layout_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (y_grad.initialized()) {
    VLOG(10) << y_grad.name() << "(y_grad) use_count: " << y_grad.impl().use_count();
    if (y_grad.impl().use_count() == 1 || (y_grad.impl().use_count() == 2 && y_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(y_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "group_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n( y_grad , [%s]), ";
    std::string input_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    input_str += input_y_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::group_norm_grad(x, scale_optional, bias_optional, y, mean, variance, y_grad, epsilon, groups, data_layout, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("group_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& scale_grad = returns[1][0];
  egr::AutogradMeta* scale_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&scale_grad) : nullptr;
  if (scale_grad_autograd_meta) scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[2][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op group_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: group_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n( y_grad , [%s]), ";
    std::string input_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    input_str += input_y_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_SCALE_GRAD_TEMPLATE = " \n ( scale_grad , [%s]), ";
    std::string output_scale_grad_str = paddle::string::Sprintf(TENSOR_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(scale_grad));
    output_str += output_scale_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> HardswishGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "hardswish_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  auto& scale = this->scale_;
  auto& offset = this->offset_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "hardswish_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::hardswish_grad(x, out_grad, threshold, scale, offset, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardswish_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op hardswish_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: hardswish_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> HardtanhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "hardtanh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& t_min = this->t_min_;
  auto& t_max = this->t_max_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "hardtanh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::hardtanh_grad(x, out_grad, t_min, t_max, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hardtanh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op hardtanh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: hardtanh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> HsigmoidLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "hsigmoid_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto w = egr::EagerUtils::RecoverTensorWrapper(&this->w_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto path = egr::EagerUtils::RecoverTensorWrapper(&this->path_);

  paddle::optional<paddle::experimental::Tensor> path_optional;
  if( path.impl() ) path_optional = paddle::make_optional<paddle::experimental::Tensor>(path);

  auto code = egr::EagerUtils::RecoverTensorWrapper(&this->code_);

  paddle::optional<paddle::experimental::Tensor> code_optional;
  if( code.impl() ) code_optional = paddle::make_optional<paddle::experimental::Tensor>(code);

  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);

  paddle::optional<paddle::experimental::Tensor> bias_optional;
  if( bias.impl() ) bias_optional = paddle::make_optional<paddle::experimental::Tensor>(bias);

  auto pre_out = egr::EagerUtils::RecoverTensorWrapper(&this->pre_out_);
  auto& out_grad = hooked_grads[0][0];
  auto& num_classes = this->num_classes_;
  auto& remote_prefetch = this->remote_prefetch_;
  auto& is_sparse = this->is_sparse_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(6);
  for (int i = 0; i < 6; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_2 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "hsigmoid_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_W_TEMPLATE = " \n( w , [%s]), ";
    std::string input_w_str = paddle::string::Sprintf(TENSOR_W_TEMPLATE, egr::EagerUtils::TensorStr(w));
    input_str += input_w_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_PATH_TEMPLATE = " \n( path , [%s]), ";
    std::string input_path_str = paddle::string::Sprintf(TENSOR_PATH_TEMPLATE, egr::EagerUtils::TensorStr(path));
    input_str += input_path_str; 
    const char* TENSOR_CODE_TEMPLATE = " \n( code , [%s]), ";
    std::string input_code_str = paddle::string::Sprintf(TENSOR_CODE_TEMPLATE, egr::EagerUtils::TensorStr(code));
    input_str += input_code_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_PRE_OUT_TEMPLATE = " \n( pre_out , [%s]), ";
    std::string input_pre_out_str = paddle::string::Sprintf(TENSOR_PRE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(pre_out));
    input_str += input_pre_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::hsigmoid_loss_grad(x, w, label, path_optional, code_optional, bias_optional, pre_out, out_grad, num_classes, remote_prefetch, is_sparse, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("hsigmoid_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& w_grad = returns[2][0];
  egr::AutogradMeta* w_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&w_grad) : nullptr;
  if (w_grad_autograd_meta) w_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[3][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op hsigmoid_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: hsigmoid_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_W_TEMPLATE = " \n( w , [%s]), ";
    std::string input_w_str = paddle::string::Sprintf(TENSOR_W_TEMPLATE, egr::EagerUtils::TensorStr(w));
    input_str += input_w_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_PATH_TEMPLATE = " \n( path , [%s]), ";
    std::string input_path_str = paddle::string::Sprintf(TENSOR_PATH_TEMPLATE, egr::EagerUtils::TensorStr(path));
    input_str += input_path_str; 
    const char* TENSOR_CODE_TEMPLATE = " \n( code , [%s]), ";
    std::string input_code_str = paddle::string::Sprintf(TENSOR_CODE_TEMPLATE, egr::EagerUtils::TensorStr(code));
    input_str += input_code_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_PRE_OUT_TEMPLATE = " \n( pre_out , [%s]), ";
    std::string input_pre_out_str = paddle::string::Sprintf(TENSOR_PRE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(pre_out));
    input_str += input_pre_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_W_GRAD_TEMPLATE = " \n ( w_grad , [%s]), ";
    std::string output_w_grad_str = paddle::string::Sprintf(TENSOR_W_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(w_grad));
    output_str += output_w_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> HuberLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "huber_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto residual = egr::EagerUtils::RecoverTensorWrapper(&this->residual_);
  auto& out_grad = hooked_grads[0][0];
  auto& delta = this->delta_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "huber_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_RESIDUAL_TEMPLATE = " \n( residual , [%s]), ";
    std::string input_residual_str = paddle::string::Sprintf(TENSOR_RESIDUAL_TEMPLATE, egr::EagerUtils::TensorStr(residual));
    input_str += input_residual_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::huber_loss_grad(residual, out_grad, delta, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("huber_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  

  auto& label_grad = returns[1][0];
  egr::AutogradMeta* label_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&label_grad) : nullptr;
  if (label_grad_autograd_meta) label_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op huber_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: huber_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_RESIDUAL_TEMPLATE = " \n( residual , [%s]), ";
    std::string input_residual_str = paddle::string::Sprintf(TENSOR_RESIDUAL_TEMPLATE, egr::EagerUtils::TensorStr(residual));
    input_str += input_residual_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
    const char* TENSOR_LABEL_GRAD_TEMPLATE = " \n ( label_grad , [%s]), ";
    std::string output_label_grad_str = paddle::string::Sprintf(TENSOR_LABEL_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(label_grad));
    output_str += output_label_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ImagGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "imag_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "imag_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::imag_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("imag_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op imag_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: imag_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> IndexAddGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "index_add_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto add_value = egr::EagerUtils::RecoverTensorWrapper(&this->add_value_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "index_add_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_ADD_VALUE_TEMPLATE = " \n( add_value , [%s]), ";
    std::string input_add_value_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(add_value));
    input_str += input_add_value_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::index_add_grad(index, add_value, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("index_add_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& add_value_grad = returns[2][0];
  egr::AutogradMeta* add_value_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&add_value_grad) : nullptr;
  if (add_value_grad_autograd_meta) add_value_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op index_add_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: index_add_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_ADD_VALUE_TEMPLATE = " \n( add_value , [%s]), ";
    std::string input_add_value_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(add_value));
    input_str += input_add_value_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_ADD_VALUE_GRAD_TEMPLATE = " \n ( add_value_grad , [%s]), ";
    std::string output_add_value_grad_str = paddle::string::Sprintf(TENSOR_ADD_VALUE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(add_value_grad));
    output_str += output_add_value_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> InstanceNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "instance_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto fwd_scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);

  paddle::optional<paddle::experimental::Tensor> fwd_scale_optional;
  if( fwd_scale.impl() ) fwd_scale_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_scale);

  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto& grad_y = hooked_grads[0][0];
  auto& epsilon = this->epsilon_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "instance_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n( grad_y , [%s]), ";
    std::string input_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    input_str += input_grad_y_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FWD_SCALE_TEMPLATE = " \n( fwd_scale , [%s]), ";
    std::string input_fwd_scale_str = paddle::string::Sprintf(TENSOR_FWD_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(fwd_scale));
    input_str += input_fwd_scale_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::instance_norm_grad(x, fwd_scale_optional, saved_mean, saved_variance, grad_y, epsilon, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("instance_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_scale = returns[1][0];
  egr::AutogradMeta* grad_scale_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_scale) : nullptr;
  if (grad_scale_autograd_meta) grad_scale_autograd_meta->SetStopGradient(false);
  

  auto& grad_bias = returns[2][0];
  egr::AutogradMeta* grad_bias_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_bias) : nullptr;
  if (grad_bias_autograd_meta) grad_bias_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("instance_norm_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<InstanceNormDoubleGradNode>(new InstanceNormDoubleGradNode(3, 5));
    // SetAttributes if needed
    grad_node->SetAttributeepsilon(epsilon);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrapperfwd_scale(fwd_scale);
    grad_node->SetTensorWrappersaved_mean(saved_mean);
    grad_node->SetTensorWrappersaved_variance(saved_variance);
    grad_node->SetTensorWrappergrad_y(grad_y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(fwd_scale, 1);
    grad_node->SetGradOutMeta(grad_y, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_scale_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_scale_autograd_meta, 1);
    }
    if (grad_bias_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_bias_autograd_meta, 2);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_scale_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_scale_autograd_meta, grad_node);
    }
    if (grad_bias_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_bias_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_scale, 1);
    grad_node->SetGradInMeta(grad_bias, 2);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_scale);
    egr::EagerUtils::CheckAndRetainGrad(grad_bias);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: instance_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n( grad_y , [%s]), ";
    std::string input_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    input_str += input_grad_y_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FWD_SCALE_TEMPLATE = " \n( fwd_scale , [%s]), ";
    std::string input_fwd_scale_str = paddle::string::Sprintf(TENSOR_FWD_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(fwd_scale));
    input_str += input_fwd_scale_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_SCALE_TEMPLATE = " \n ( grad_scale , [%s]), ";
    std::string output_grad_scale_str = paddle::string::Sprintf(TENSOR_GRAD_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(grad_scale));
    output_str += output_grad_scale_str; 
    const char* TENSOR_GRAD_BIAS_TEMPLATE = " \n ( grad_bias , [%s]), ";
    std::string output_grad_bias_str = paddle::string::Sprintf(TENSOR_GRAD_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(grad_bias));
    output_str += output_grad_bias_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> InstanceNormDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "instance_norm_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[2][0], input_metas[2][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto fwd_scale = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_scale_);

  paddle::optional<paddle::experimental::Tensor> fwd_scale_optional;
  if( fwd_scale.impl() ) fwd_scale_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_scale);

  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto grad_y = egr::EagerUtils::RecoverTensorWrapper(&this->grad_y_);
  auto& grad_x_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_optional;
  if(grad_x_grad.initialized()) grad_x_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad);

  auto& grad_scale_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_scale_grad_optional;
  if(grad_scale_grad.initialized()) grad_scale_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_scale_grad);

  auto& grad_bias_grad = hooked_grads[2][0];

  paddle::optional<paddle::experimental::Tensor> grad_bias_grad_optional;
  if(grad_bias_grad.initialized()) grad_bias_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_bias_grad);

  auto& epsilon = this->epsilon_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "instance_norm_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_SCALE_GRAD_TEMPLATE = " \n( grad_scale_grad , [%s]), ";
    std::string input_grad_scale_grad_str = paddle::string::Sprintf(TENSOR_GRAD_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_scale_grad));
    input_str += input_grad_scale_grad_str; 
    const char* TENSOR_GRAD_BIAS_GRAD_TEMPLATE = " \n( grad_bias_grad , [%s]), ";
    std::string input_grad_bias_grad_str = paddle::string::Sprintf(TENSOR_GRAD_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_bias_grad));
    input_str += input_grad_bias_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FWD_SCALE_TEMPLATE = " \n( fwd_scale , [%s]), ";
    std::string input_fwd_scale_str = paddle::string::Sprintf(TENSOR_FWD_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(fwd_scale));
    input_str += input_fwd_scale_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n( grad_y , [%s]), ";
    std::string input_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    input_str += input_grad_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::instance_norm_double_grad(x, fwd_scale_optional, saved_mean, saved_variance, grad_y, grad_x_grad_optional, grad_scale_grad_optional, grad_bias_grad_optional, epsilon, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("instance_norm_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_scale_grad = returns[1][0];
  egr::AutogradMeta* fwd_scale_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_scale_grad) : nullptr;
  if (fwd_scale_grad_autograd_meta) fwd_scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_y_grad = returns[4][0];
  egr::AutogradMeta* grad_y_grad_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y_grad) : nullptr;
  if (grad_y_grad_autograd_meta) grad_y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op instance_norm_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: instance_norm_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_SCALE_GRAD_TEMPLATE = " \n( grad_scale_grad , [%s]), ";
    std::string input_grad_scale_grad_str = paddle::string::Sprintf(TENSOR_GRAD_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_scale_grad));
    input_str += input_grad_scale_grad_str; 
    const char* TENSOR_GRAD_BIAS_GRAD_TEMPLATE = " \n( grad_bias_grad , [%s]), ";
    std::string input_grad_bias_grad_str = paddle::string::Sprintf(TENSOR_GRAD_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_bias_grad));
    input_str += input_grad_bias_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_FWD_SCALE_TEMPLATE = " \n( fwd_scale , [%s]), ";
    std::string input_fwd_scale_str = paddle::string::Sprintf(TENSOR_FWD_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(fwd_scale));
    input_str += input_fwd_scale_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n( grad_y , [%s]), ";
    std::string input_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    input_str += input_grad_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_FWD_SCALE_GRAD_TEMPLATE = " \n ( fwd_scale_grad , [%s]), ";
    std::string output_fwd_scale_grad_str = paddle::string::Sprintf(TENSOR_FWD_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_scale_grad));
    output_str += output_fwd_scale_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n ( grad_y_grad , [%s]), ";
    std::string output_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    output_str += output_grad_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> KldivLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "kldiv_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto& out_grad = hooked_grads[0][0];
  auto& reduction = this->reduction_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "kldiv_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::kldiv_loss_grad(x, label, out_grad, reduction, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("kldiv_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op kldiv_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: kldiv_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> KronGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "kron_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "kron_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::kron_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("kron_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op kron_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: kron_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LayerNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "layer_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);

  paddle::optional<paddle::experimental::Tensor> scale_optional;
  if( scale.impl() ) scale_optional = paddle::make_optional<paddle::experimental::Tensor>(scale);

  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);

  paddle::optional<paddle::experimental::Tensor> bias_optional;
  if( bias.impl() ) bias_optional = paddle::make_optional<paddle::experimental::Tensor>(bias);

  auto mean = egr::EagerUtils::RecoverTensorWrapper(&this->mean_);
  auto variance = egr::EagerUtils::RecoverTensorWrapper(&this->variance_);
  auto& out_grad = hooked_grads[0][0];
  auto& epsilon = this->epsilon_;
  auto& begin_norm_axis = this->begin_norm_axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "layer_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::layer_norm_grad(x, scale_optional, bias_optional, mean, variance, out_grad, epsilon, begin_norm_axis, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("layer_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& scale_grad = returns[1][0];
  egr::AutogradMeta* scale_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&scale_grad) : nullptr;
  if (scale_grad_autograd_meta) scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[2][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op layer_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: layer_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_MEAN_TEMPLATE = " \n( mean , [%s]), ";
    std::string input_mean_str = paddle::string::Sprintf(TENSOR_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(mean));
    input_str += input_mean_str; 
    const char* TENSOR_VARIANCE_TEMPLATE = " \n( variance , [%s]), ";
    std::string input_variance_str = paddle::string::Sprintf(TENSOR_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(variance));
    input_str += input_variance_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_SCALE_GRAD_TEMPLATE = " \n ( scale_grad , [%s]), ";
    std::string output_scale_grad_str = paddle::string::Sprintf(TENSOR_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(scale_grad));
    output_str += output_scale_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LinearInterpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "linear_interp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out_size = egr::EagerUtils::RecoverTensorWrapper(&this->out_size_);

  paddle::optional<paddle::experimental::Tensor> out_size_optional;
  if( out_size.impl() ) out_size_optional = paddle::make_optional<paddle::experimental::Tensor>(out_size);

  auto size_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->size_tensor_);

  paddle::optional<std::vector<paddle::experimental::Tensor>> size_tensor_optional;
  if( !size_tensor.empty() ) size_tensor_optional = paddle::make_optional<std::vector<paddle::experimental::Tensor>>(size_tensor);

  auto scale_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->scale_tensor_);

  paddle::optional<paddle::experimental::Tensor> scale_tensor_optional;
  if( scale_tensor.impl() ) scale_tensor_optional = paddle::make_optional<paddle::experimental::Tensor>(scale_tensor);

  auto& output_grad = hooked_grads[0][0];
  auto& data_layout = this->data_layout_;
  auto& out_d = this->out_d_;
  auto& out_h = this->out_h_;
  auto& out_w = this->out_w_;
  auto& scale = this->scale_;
  auto& interp_method = this->interp_method_;
  auto& align_corners = this->align_corners_;
  auto& align_mode = this->align_mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "linear_interp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::linear_interp_grad(x, out_size_optional, size_tensor_optional, scale_tensor_optional, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("linear_interp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op linear_interp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: linear_interp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogSoftmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log_softmax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "log_softmax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::log_softmax_grad(out, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log_softmax_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log_softmax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log_softmax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogcumsumexpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "logcumsumexp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& flatten = this->flatten_;
  auto& exclusive = this->exclusive_;
  auto& reverse = this->reverse_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "logcumsumexp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::logcumsumexp_grad(x, out, out_grad, axis, flatten, exclusive, reverse, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logcumsumexp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op logcumsumexp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: logcumsumexp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LogsumexpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "logsumexp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "logsumexp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::logsumexp_grad(x, out, out_grad, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("logsumexp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op logsumexp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: logsumexp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LuGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "lu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto pivots = egr::EagerUtils::RecoverTensorWrapper(&this->pivots_);
  auto& out_grad = hooked_grads[0][0];
  auto& pivot = this->pivot_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "lu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_PIVOTS_TEMPLATE = " \n( pivots , [%s]), ";
    std::string input_pivots_str = paddle::string::Sprintf(TENSOR_PIVOTS_TEMPLATE, egr::EagerUtils::TensorStr(pivots));
    input_str += input_pivots_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::lu_grad(x, out, pivots, out_grad, pivot, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("lu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op lu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: lu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_PIVOTS_TEMPLATE = " \n( pivots , [%s]), ";
    std::string input_pivots_str = paddle::string::Sprintf(TENSOR_PIVOTS_TEMPLATE, egr::EagerUtils::TensorStr(pivots));
    input_str += input_pivots_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MarginCrossEntropyGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "margin_cross_entropy_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto logits = egr::EagerUtils::RecoverTensorWrapper(&this->logits_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto softmax = egr::EagerUtils::RecoverTensorWrapper(&this->softmax_);
  auto& loss_grad = hooked_grads[1][0];
  auto& return_softmax = this->return_softmax_;
  auto& ring_id = this->ring_id_;
  auto& rank = this->rank_;
  auto& nranks = this->nranks_;
  auto& margin1 = this->margin1_;
  auto& margin2 = this->margin2_;
  auto& margin3 = this->margin3_;
  auto& scale = this->scale_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (softmax.initialized()) {
    VLOG(10) << softmax.name() << "(softmax) use_count: " << softmax.impl().use_count();
    if (softmax.impl().use_count() == 1 || (softmax.impl().use_count() == 2 && softmax.impl().get() == (&this->softmax_)->get_intermidiate_tensor().impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(softmax, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "margin_cross_entropy_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string input_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    input_str += input_softmax_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::margin_cross_entropy_grad(logits, label, softmax, loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("margin_cross_entropy_grad", returns); }

  // Get GradOut autograd_meta

  auto& logits_grad = returns[0][0];
  egr::AutogradMeta* logits_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&logits_grad) : nullptr;
  if (logits_grad_autograd_meta) logits_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op margin_cross_entropy_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: margin_cross_entropy_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string input_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    input_str += input_softmax_str; 
    const char* TENSOR_LOGITS_GRAD_TEMPLATE = " \n ( logits_grad , [%s]), ";
    std::string output_logits_grad_str = paddle::string::Sprintf(TENSOR_LOGITS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(logits_grad));
    output_str += output_logits_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MatmulGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "matmul_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& grad_out = hooked_grads[0][0];
  auto& transpose_x = this->transpose_x_;
  auto& transpose_y = this->transpose_y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "matmul_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::matmul_grad(x, y, grad_out, transpose_x, transpose_y, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matmul_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("matmul_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<MatmulDoubleGradNode>(new MatmulDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributetranspose_x(transpose_x);
    grad_node->SetAttributetranspose_y(transpose_y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: matmul_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MatmulDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "matmul_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto fwd_grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& fwd_grad_grad_x = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_x_optional;
  if(fwd_grad_grad_x.initialized()) fwd_grad_grad_x_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_x);

  auto& fwd_grad_grad_y = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_y_optional;
  if(fwd_grad_grad_y.initialized()) fwd_grad_grad_y_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_y);

  auto& transpose_x = this->transpose_x_;
  auto& transpose_y = this->transpose_y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "matmul_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::matmul_double_grad(x, y, fwd_grad_out, fwd_grad_grad_x_optional, fwd_grad_grad_y_optional, transpose_x, transpose_y, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matmul_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_out = returns[2][0];
  egr::AutogradMeta* grad_grad_out_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_out) : nullptr;
  if (grad_grad_out_autograd_meta) grad_grad_out_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("matmul_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<MatmulTripleGradNode>(new MatmulTripleGradNode(3, 5));
    // SetAttributes if needed
    grad_node->SetAttributetranspose_x(transpose_x);
    grad_node->SetAttributetranspose_y(transpose_y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrapperfwd_grad_out(fwd_grad_out);
    grad_node->SetTensorWrapperfwd_grad_grad_x(fwd_grad_grad_x);
    grad_node->SetTensorWrapperfwd_grad_grad_y(fwd_grad_grad_y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    grad_node->SetGradOutMeta(fwd_grad_out, 2);
    grad_node->SetGradOutMeta(fwd_grad_grad_x, 3);
    grad_node->SetGradOutMeta(fwd_grad_grad_y, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_grad_out_autograd_meta, 2);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_grad_out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    grad_node->SetGradInMeta(grad_grad_out, 2);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    egr::EagerUtils::CheckAndRetainGrad(grad_grad_out);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: matmul_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
    const char* TENSOR_GRAD_GRAD_OUT_TEMPLATE = " \n ( grad_grad_out , [%s]), ";
    std::string output_grad_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out));
    output_str += output_grad_grad_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MatmulTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "matmul_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[2][0], input_metas[2][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto fwd_grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_out_);
  auto fwd_grad_grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_grad_x_);

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_x_optional;
  if( fwd_grad_grad_x.impl() ) fwd_grad_grad_x_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_x);

  auto fwd_grad_grad_y = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_grad_y_);

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_y_optional;
  if( fwd_grad_grad_y.impl() ) fwd_grad_grad_y_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_y);

  auto& grad_x_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_optional;
  if(grad_x_grad.initialized()) grad_x_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad);

  auto& grad_y_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_y_grad_optional;
  if(grad_y_grad.initialized()) grad_y_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_y_grad);

  auto& grad_grad_out_grad = hooked_grads[2][0];

  paddle::optional<paddle::experimental::Tensor> grad_grad_out_grad_optional;
  if(grad_grad_out_grad.initialized()) grad_grad_out_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_grad_out_grad);

  auto& transpose_x = this->transpose_x_;
  auto& transpose_y = this->transpose_y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_3 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  auto* api_output_4 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "matmul_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::matmul_triple_grad(x, y, fwd_grad_out, fwd_grad_grad_x_optional, fwd_grad_grad_y_optional, grad_x_grad_optional, grad_y_grad_optional, grad_grad_out_grad_optional, transpose_x, transpose_y, api_output_0, api_output_1, api_output_2, api_output_3, api_output_4);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matmul_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_out_grad = returns[2][0];
  egr::AutogradMeta* fwd_grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_out_grad) : nullptr;
  if (fwd_grad_out_grad_autograd_meta) fwd_grad_out_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_grad_x_grad = returns[3][0];
  egr::AutogradMeta* fwd_grad_grad_x_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_grad_x_grad) : nullptr;
  if (fwd_grad_grad_x_grad_autograd_meta) fwd_grad_grad_x_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_grad_y_grad = returns[4][0];
  egr::AutogradMeta* fwd_grad_grad_y_grad_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_grad_y_grad) : nullptr;
  if (fwd_grad_grad_y_grad_autograd_meta) fwd_grad_grad_y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op matmul_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: matmul_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
    const char* TENSOR_FWD_GRAD_OUT_GRAD_TEMPLATE = " \n ( fwd_grad_out_grad , [%s]), ";
    std::string output_fwd_grad_out_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out_grad));
    output_str += output_fwd_grad_out_grad_str; 
    const char* TENSOR_FWD_GRAD_GRAD_X_GRAD_TEMPLATE = " \n ( fwd_grad_grad_x_grad , [%s]), ";
    std::string output_fwd_grad_grad_x_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x_grad));
    output_str += output_fwd_grad_grad_x_grad_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_GRAD_TEMPLATE = " \n ( fwd_grad_grad_y_grad , [%s]), ";
    std::string output_fwd_grad_grad_y_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y_grad));
    output_str += output_fwd_grad_grad_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "max_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "max_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::max_grad(x, out, out_grad, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("max_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op max_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: max_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaxPool2dWithIndexGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "max_pool2d_with_index_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto mask = egr::EagerUtils::RecoverTensorWrapper(&this->mask_);
  auto& out_grad = hooked_grads[0][0];
  auto& kernel_size = this->kernel_size_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& global_pooling = this->global_pooling_;
  auto& adaptive = this->adaptive_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "max_pool2d_with_index_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::max_pool2d_with_index_grad(x, mask, out_grad, kernel_size, strides, paddings, global_pooling, adaptive, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("max_pool2d_with_index_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op max_pool2d_with_index_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: max_pool2d_with_index_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaxPool3dWithIndexGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "max_pool3d_with_index_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto mask = egr::EagerUtils::RecoverTensorWrapper(&this->mask_);
  auto& out_grad = hooked_grads[0][0];
  auto& kernel_size = this->kernel_size_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& global_pooling = this->global_pooling_;
  auto& adaptive = this->adaptive_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "max_pool3d_with_index_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::max_pool3d_with_index_grad(x, mask, out_grad, kernel_size, strides, paddings, global_pooling, adaptive, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("max_pool3d_with_index_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op max_pool3d_with_index_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: max_pool3d_with_index_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_MASK_TEMPLATE = " \n( mask , [%s]), ";
    std::string input_mask_str = paddle::string::Sprintf(TENSOR_MASK_TEMPLATE, egr::EagerUtils::TensorStr(mask));
    input_str += input_mask_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaximumGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "maximum_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "maximum_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::maximum_grad(x, y, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("maximum_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op maximum_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: maximum_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MeanGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mean_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "mean_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::mean_grad(x, grad_out, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mean_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("mean_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<MeanDoubleGradNode>(new MeanDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: mean_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MeanDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mean_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "mean_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = mean_ad_func(grad_x_grad, axis, keepdim);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::mean(grad_x_grad, axis, keepdim);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mean_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: mean_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MeanAllGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mean_all_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "mean_all_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::mean_all_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mean_all_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op mean_all_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: mean_all_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MeshgridGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "meshgrid_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto inputs = egr::EagerUtils::RecoverTensorWrapper(&this->inputs_);
  auto& outputs_grad = hooked_grads[0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "meshgrid_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUTS_GRAD_TEMPLATE = " \n( outputs_grad , [%s]), ";
    std::string input_outputs_grad_str = paddle::string::Sprintf(TENSOR_OUTPUTS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(outputs_grad));
    input_str += input_outputs_grad_str; 
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::meshgrid_grad(inputs, outputs_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("meshgrid_grad", returns); }

  // Get GradOut autograd_meta

    auto& inputs_grad = returns[0];
    std::vector<egr::AutogradMeta*> inputs_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&inputs_grad);
    for(auto* meta : inputs_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op meshgrid_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: meshgrid_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUTS_GRAD_TEMPLATE = " \n( outputs_grad , [%s]), ";
    std::string input_outputs_grad_str = paddle::string::Sprintf(TENSOR_OUTPUTS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(outputs_grad));
    input_str += input_outputs_grad_str; 
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
    const char* TENSOR_INPUTS_GRAD_TEMPLATE = " \n ( inputs_grad , [%s]), ";
    std::string output_inputs_grad_str = paddle::string::Sprintf(TENSOR_INPUTS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(inputs_grad));
    output_str += output_inputs_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MinGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "min_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "min_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::min_grad(x, out, out_grad, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("min_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op min_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: min_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MinimumGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "minimum_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "minimum_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::minimum_grad(x, y, out_grad, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("minimum_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op minimum_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: minimum_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MishGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mish_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "mish_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::mish_grad(x, out_grad, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mish_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op mish_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: mish_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MultiDotGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "multi_dot_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "multi_dot_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::multi_dot_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multi_dot_grad", returns); }

  // Get GradOut autograd_meta

    auto& x_grad = returns[0];
    std::vector<egr::AutogradMeta*> x_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&x_grad);
    for(auto* meta : x_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op multi_dot_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: multi_dot_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MultiplexGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "multiplex_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto inputs = egr::EagerUtils::RecoverTensorWrapper(&this->inputs_);
  auto index = egr::EagerUtils::RecoverTensorWrapper(&this->index_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "multiplex_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::multiplex_grad(inputs, index, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiplex_grad", returns); }

  // Get GradOut autograd_meta

    auto& inputs_grad = returns[0];
    std::vector<egr::AutogradMeta*> inputs_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&inputs_grad);
    for(auto* meta : inputs_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op multiplex_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: multiplex_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUTS_TEMPLATE = " \n( inputs , [%s]), ";
    std::string input_inputs_str = paddle::string::Sprintf(TENSOR_INPUTS_TEMPLATE, egr::EagerUtils::TensorStr(inputs));
    input_str += input_inputs_str; 
    const char* TENSOR_INDEX_TEMPLATE = " \n( index , [%s]), ";
    std::string input_index_str = paddle::string::Sprintf(TENSOR_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(index));
    input_str += input_index_str; 
    const char* TENSOR_INPUTS_GRAD_TEMPLATE = " \n ( inputs_grad , [%s]), ";
    std::string output_inputs_grad_str = paddle::string::Sprintf(TENSOR_INPUTS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(inputs_grad));
    output_str += output_inputs_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MultiplyGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "multiply_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "multiply_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::multiply_grad(x, y, grad_out, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiply_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("multiply_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<MultiplyDoubleGradNode>(new MultiplyDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: multiply_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MultiplyDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "multiply_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto fwd_grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& fwd_grad_grad_x = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_x_optional;
  if(fwd_grad_grad_x.initialized()) fwd_grad_grad_x_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_x);

  auto& fwd_grad_grad_y = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_y_optional;
  if(fwd_grad_grad_y.initialized()) fwd_grad_grad_y_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_y);

  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (fwd_grad_grad_x.initialized()) {
    VLOG(10) << fwd_grad_grad_x.name() << "(grad_x_grad) use_count: " << fwd_grad_grad_x.impl().use_count();
    if (fwd_grad_grad_x.impl().use_count() == 1 || (fwd_grad_grad_x.impl().use_count() == 2 && fwd_grad_grad_x.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_2 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(fwd_grad_grad_x, api_output_2);
    }
  }

  VLOG(5) << "Running C++ API: " << "multiply_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::multiply_double_grad(x, y, fwd_grad_out, fwd_grad_grad_x_optional, fwd_grad_grad_y_optional, axis, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiply_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_out = returns[2][0];
  egr::AutogradMeta* grad_grad_out_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_out) : nullptr;
  if (grad_grad_out_autograd_meta) grad_grad_out_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("multiply_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<MultiplyTripleGradNode>(new MultiplyTripleGradNode(3, 5));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(-1);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrapperfwd_grad_out(fwd_grad_out);
    grad_node->SetTensorWrapperfwd_grad_grad_x(fwd_grad_grad_x);
    grad_node->SetTensorWrapperfwd_grad_grad_y(fwd_grad_grad_y);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(y, 1);
    grad_node->SetGradOutMeta(fwd_grad_out, 2);
    grad_node->SetGradOutMeta(fwd_grad_grad_x, 3);
    grad_node->SetGradOutMeta(fwd_grad_grad_y, 4);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_grad_out_autograd_meta, 2);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_grad_out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    grad_node->SetGradInMeta(grad_grad_out, 2);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    egr::EagerUtils::CheckAndRetainGrad(grad_grad_out);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: multiply_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
    const char* TENSOR_GRAD_GRAD_OUT_TEMPLATE = " \n ( grad_grad_out , [%s]), ";
    std::string output_grad_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out));
    output_str += output_grad_grad_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MultiplyTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "multiply_triple_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[2][0], input_metas[2][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto fwd_grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_out_);
  auto fwd_grad_grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_grad_x_);

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_x_optional;
  if( fwd_grad_grad_x.impl() ) fwd_grad_grad_x_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_x);

  auto fwd_grad_grad_y = egr::EagerUtils::RecoverTensorWrapper(&this->fwd_grad_grad_y_);

  paddle::optional<paddle::experimental::Tensor> fwd_grad_grad_y_optional;
  if( fwd_grad_grad_y.impl() ) fwd_grad_grad_y_optional = paddle::make_optional<paddle::experimental::Tensor>(fwd_grad_grad_y);

  auto& grad_x_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_optional;
  if(grad_x_grad.initialized()) grad_x_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad);

  auto& grad_y_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_y_grad_optional;
  if(grad_y_grad.initialized()) grad_y_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_y_grad);

  auto& grad_grad_out_grad = hooked_grads[2][0];

  paddle::optional<paddle::experimental::Tensor> grad_grad_out_grad_optional;
  if(grad_grad_out_grad.initialized()) grad_grad_out_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_grad_out_grad);

  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_3 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  auto* api_output_4 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "multiply_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::multiply_triple_grad(x, y, fwd_grad_out, fwd_grad_grad_x_optional, fwd_grad_grad_y_optional, grad_x_grad_optional, grad_y_grad_optional, grad_grad_out_grad_optional, axis, api_output_0, api_output_1, api_output_2, api_output_3, api_output_4);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiply_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_out_grad = returns[2][0];
  egr::AutogradMeta* fwd_grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_out_grad) : nullptr;
  if (fwd_grad_out_grad_autograd_meta) fwd_grad_out_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_grad_x_grad = returns[3][0];
  egr::AutogradMeta* fwd_grad_grad_x_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_grad_x_grad) : nullptr;
  if (fwd_grad_grad_x_grad_autograd_meta) fwd_grad_grad_x_grad_autograd_meta->SetStopGradient(false);
  

  auto& fwd_grad_grad_y_grad = returns[4][0];
  egr::AutogradMeta* fwd_grad_grad_y_grad_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&fwd_grad_grad_y_grad) : nullptr;
  if (fwd_grad_grad_y_grad_autograd_meta) fwd_grad_grad_y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op multiply_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: multiply_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_FWD_GRAD_OUT_TEMPLATE = " \n( fwd_grad_out , [%s]), ";
    std::string input_fwd_grad_out_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out));
    input_str += input_fwd_grad_out_str; 
    const char* TENSOR_FWD_GRAD_GRAD_X_TEMPLATE = " \n( fwd_grad_grad_x , [%s]), ";
    std::string input_fwd_grad_grad_x_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x));
    input_str += input_fwd_grad_grad_x_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE = " \n( fwd_grad_grad_y , [%s]), ";
    std::string input_fwd_grad_grad_y_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y));
    input_str += input_fwd_grad_grad_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
    const char* TENSOR_FWD_GRAD_OUT_GRAD_TEMPLATE = " \n ( fwd_grad_out_grad , [%s]), ";
    std::string output_fwd_grad_out_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_out_grad));
    output_str += output_fwd_grad_out_grad_str; 
    const char* TENSOR_FWD_GRAD_GRAD_X_GRAD_TEMPLATE = " \n ( fwd_grad_grad_x_grad , [%s]), ";
    std::string output_fwd_grad_grad_x_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_x_grad));
    output_str += output_fwd_grad_grad_x_grad_str; 
    const char* TENSOR_FWD_GRAD_GRAD_Y_GRAD_TEMPLATE = " \n ( fwd_grad_grad_y_grad , [%s]), ";
    std::string output_fwd_grad_grad_y_grad_str = paddle::string::Sprintf(TENSOR_FWD_GRAD_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(fwd_grad_grad_y_grad));
    output_str += output_fwd_grad_grad_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> NearestInterpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "nearest_interp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out_size = egr::EagerUtils::RecoverTensorWrapper(&this->out_size_);

  paddle::optional<paddle::experimental::Tensor> out_size_optional;
  if( out_size.impl() ) out_size_optional = paddle::make_optional<paddle::experimental::Tensor>(out_size);

  auto size_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->size_tensor_);

  paddle::optional<std::vector<paddle::experimental::Tensor>> size_tensor_optional;
  if( !size_tensor.empty() ) size_tensor_optional = paddle::make_optional<std::vector<paddle::experimental::Tensor>>(size_tensor);

  auto scale_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->scale_tensor_);

  paddle::optional<paddle::experimental::Tensor> scale_tensor_optional;
  if( scale_tensor.impl() ) scale_tensor_optional = paddle::make_optional<paddle::experimental::Tensor>(scale_tensor);

  auto& output_grad = hooked_grads[0][0];
  auto& data_layout = this->data_layout_;
  auto& out_d = this->out_d_;
  auto& out_h = this->out_h_;
  auto& out_w = this->out_w_;
  auto& scale = this->scale_;
  auto& interp_method = this->interp_method_;
  auto& align_corners = this->align_corners_;
  auto& align_mode = this->align_mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "nearest_interp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::nearest_interp_grad(x, out_size_optional, size_tensor_optional, scale_tensor_optional, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("nearest_interp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op nearest_interp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: nearest_interp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> NormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto norm = egr::EagerUtils::RecoverTensorWrapper(&this->norm_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& epsilon = this->epsilon_;
  auto& is_test = this->is_test_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_NORM_TEMPLATE = " \n( norm , [%s]), ";
    std::string input_norm_str = paddle::string::Sprintf(TENSOR_NORM_TEMPLATE, egr::EagerUtils::TensorStr(norm));
    input_str += input_norm_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::norm_grad(x, norm, out_grad, axis, epsilon, is_test, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_NORM_TEMPLATE = " \n( norm , [%s]), ";
    std::string input_norm_str = paddle::string::Sprintf(TENSOR_NORM_TEMPLATE, egr::EagerUtils::TensorStr(norm));
    input_str += input_norm_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "p_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& porder = this->porder_;
  auto& axis = this->axis_;
  auto& epsilon = this->epsilon_;
  auto& keepdim = this->keepdim_;
  auto& asvector = this->asvector_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "p_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::p_norm_grad(x, out, out_grad, porder, axis, epsilon, keepdim, asvector, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("p_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op p_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: p_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PadGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pad_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& paddings = this->paddings_;
  auto& pad_value = this->pad_value_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pad_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pad_grad(x, grad_out, paddings, pad_value, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pad_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("pad_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<PadDoubleGradNode>(new PadDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributepad_value(pad_value);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: pad_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PadDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pad_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& paddings = this->paddings_;
  auto& pad_value = this->pad_value_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pad_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pad_double_grad(grad_x_grad, paddings, pad_value, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pad_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pad_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pad_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Pad3dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pad3d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& paddings = this->paddings_;
  auto& mode = this->mode_;
  auto& pad_value = this->pad_value_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pad3d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pad3d_grad(x, grad_out, paddings, mode, pad_value, data_format, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pad3d_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("pad3d_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<Pad3dDoubleGradNode>(new Pad3dDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributemode(mode);
    grad_node->SetAttributepad_value(pad_value);
    grad_node->SetAttributedata_format(data_format);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: pad3d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Pad3dDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pad3d_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& paddings = this->paddings_;
  auto& mode = this->mode_;
  auto& pad_value = this->pad_value_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pad3d_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pad3d_double_grad(grad_x_grad, paddings, mode, pad_value, data_format, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pad3d_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pad3d_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pad3d_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Pool2dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pool2d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& grad_out = hooked_grads[0][0];
  auto& kernel_size = this->kernel_size_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& ceil_mode = this->ceil_mode_;
  auto& exclusive = this->exclusive_;
  auto& data_format = this->data_format_;
  auto& pooling_type = this->pooling_type_;
  auto& global_pooling = this->global_pooling_;
  auto& adaptive = this->adaptive_;
  auto& padding_algorithm = this->padding_algorithm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pool2d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pool2d_grad(x, out, grad_out, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pool2d_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("pool2d_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<Pool2dDoubleGradNode>(new Pool2dDoubleGradNode(1, 3));
    // SetAttributes if needed
    grad_node->SetAttributekernel_size(kernel_size);
    grad_node->SetAttributestrides(strides);
    grad_node->SetAttributepaddings(paddings);
    grad_node->SetAttributeceil_mode(ceil_mode);
    grad_node->SetAttributeexclusive(exclusive);
    grad_node->SetAttributedata_format(data_format);
    grad_node->SetAttributepooling_type(pooling_type);
    grad_node->SetAttributeglobal_pooling(global_pooling);
    grad_node->SetAttributeadaptive(adaptive);
    grad_node->SetAttributepadding_algorithm(padding_algorithm);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: pool2d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Pool2dDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pool2d_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& kernel_size = this->kernel_size_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& ceil_mode = this->ceil_mode_;
  auto& exclusive = this->exclusive_;
  auto& data_format = this->data_format_;
  auto& pooling_type = this->pooling_type_;
  auto& global_pooling = this->global_pooling_;
  auto& adaptive = this->adaptive_;
  auto& padding_algorithm = this->padding_algorithm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pool2d_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pool2d_double_grad(x, grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pool2d_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[2][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pool2d_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pool2d_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Pool3dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pool3d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& kernel_size = this->kernel_size_;
  auto& strides = this->strides_;
  auto& paddings = this->paddings_;
  auto& ceil_mode = this->ceil_mode_;
  auto& exclusive = this->exclusive_;
  auto& data_format = this->data_format_;
  auto& pooling_type = this->pooling_type_;
  auto& global_pooling = this->global_pooling_;
  auto& adaptive = this->adaptive_;
  auto& padding_algorithm = this->padding_algorithm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pool3d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pool3d_grad(x, out, out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pool3d_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pool3d_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pool3d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PowGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pow_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& y = this->y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "pow_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pow_grad(x, grad_out, y, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pow_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("pow_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<PowDoubleGradNode>(new PowDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributey(y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: pow_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PowDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pow_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_grad_x = hooked_grads[0][0];
  auto& y = this->y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_grad_x.initialized()) {
    VLOG(10) << grad_grad_x.name() << "(grad_x_grad) use_count: " << grad_grad_x.impl().use_count();
    if (grad_grad_x.impl().use_count() == 1 || (grad_grad_x.impl().use_count() == 2 && grad_grad_x.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_grad_x, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "pow_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pow_double_grad(x, grad_out, grad_grad_x, y, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pow_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_out = returns[1][0];
  egr::AutogradMeta* grad_grad_out_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_out) : nullptr;
  if (grad_grad_out_autograd_meta) grad_grad_out_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("pow_double_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<PowTripleGradNode>(new PowTripleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributey(y);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrapperx(x);
    grad_node->SetTensorWrappergrad_out(grad_out);
    grad_node->SetTensorWrappergrad_grad_x(grad_grad_x);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(x, 0);
    grad_node->SetGradOutMeta(grad_out, 1);
    grad_node->SetGradOutMeta(grad_grad_x, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_grad_out_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_grad_out_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_grad_out_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_grad_out, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_grad_out);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: pow_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_GRAD_OUT_TEMPLATE = " \n ( grad_grad_out , [%s]), ";
    std::string output_grad_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out));
    output_str += output_grad_grad_out_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PowTripleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pow_triple_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto grad_grad_x = egr::EagerUtils::RecoverTensorWrapper(&this->grad_grad_x_);
  auto& grad_x_grad = hooked_grads[0][0];
  auto& grad_grad_out_grad = hooked_grads[1][0];
  auto& y = this->y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pow_triple_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::pow_triple_grad(x, grad_out, grad_grad_x, grad_x_grad, grad_grad_out_grad, y, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pow_triple_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  

  auto& grad_grad_x_grad = returns[2][0];
  egr::AutogradMeta* grad_grad_x_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_grad_x_grad) : nullptr;
  if (grad_grad_x_grad_autograd_meta) grad_grad_x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pow_triple_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pow_triple_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE = " \n( grad_grad_out_grad , [%s]), ";
    std::string input_grad_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_out_grad));
    input_str += input_grad_grad_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_GRAD_X_TEMPLATE = " \n( grad_grad_x , [%s]), ";
    std::string input_grad_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x));
    input_str += input_grad_grad_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
    const char* TENSOR_GRAD_GRAD_X_GRAD_TEMPLATE = " \n ( grad_grad_x_grad , [%s]), ";
    std::string output_grad_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_grad_x_grad));
    output_str += output_grad_grad_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PreluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "prelu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto alpha = egr::EagerUtils::RecoverTensorWrapper(&this->alpha_);
  auto& out_grad = hooked_grads[0][0];
  auto& data_format = this->data_format_;
  auto& mode = this->mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "prelu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_ALPHA_TEMPLATE = " \n( alpha , [%s]), ";
    std::string input_alpha_str = paddle::string::Sprintf(TENSOR_ALPHA_TEMPLATE, egr::EagerUtils::TensorStr(alpha));
    input_str += input_alpha_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::prelu_grad(x, alpha, out_grad, data_format, mode, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("prelu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& alpha_grad = returns[1][0];
  egr::AutogradMeta* alpha_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&alpha_grad) : nullptr;
  if (alpha_grad_autograd_meta) alpha_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op prelu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: prelu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_ALPHA_TEMPLATE = " \n( alpha , [%s]), ";
    std::string input_alpha_str = paddle::string::Sprintf(TENSOR_ALPHA_TEMPLATE, egr::EagerUtils::TensorStr(alpha));
    input_str += input_alpha_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_ALPHA_GRAD_TEMPLATE = " \n ( alpha_grad , [%s]), ";
    std::string output_alpha_grad_str = paddle::string::Sprintf(TENSOR_ALPHA_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(alpha_grad));
    output_str += output_alpha_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ProdGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "prod_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& dims = this->dims_;
  auto& keep_dim = this->keep_dim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "prod_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::prod_grad(x, out, out_grad, dims, keep_dim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("prod_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op prod_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: prod_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PsroiPoolGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "psroi_pool_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto boxes = egr::EagerUtils::RecoverTensorWrapper(&this->boxes_);
  auto boxes_num = egr::EagerUtils::RecoverTensorWrapper(&this->boxes_num_);

  paddle::optional<paddle::experimental::Tensor> boxes_num_optional;
  if( boxes_num.impl() ) boxes_num_optional = paddle::make_optional<paddle::experimental::Tensor>(boxes_num);

  auto& out_grad = hooked_grads[0][0];
  auto& pooled_height = this->pooled_height_;
  auto& pooled_width = this->pooled_width_;
  auto& output_channels = this->output_channels_;
  auto& spatial_scale = this->spatial_scale_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "psroi_pool_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::psroi_pool_grad(x, boxes, boxes_num_optional, out_grad, pooled_height, pooled_width, output_channels, spatial_scale, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("psroi_pool_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op psroi_pool_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: psroi_pool_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RealGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "real_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "real_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::real_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("real_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op real_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: real_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Relu6GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "relu6_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "relu6_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::relu6_grad(out, out_grad, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu6_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op relu6_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: relu6_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RepeatInterleaveGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "repeat_interleave_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& repeats = this->repeats_;
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "repeat_interleave_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::repeat_interleave_grad(x, out_grad, repeats, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("repeat_interleave_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op repeat_interleave_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: repeat_interleave_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RepeatInterleaveWithTensorIndexGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "repeat_interleave_with_tensor_index_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto repeats = egr::EagerUtils::RecoverTensorWrapper(&this->repeats_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "repeat_interleave_with_tensor_index_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_REPEATS_TEMPLATE = " \n( repeats , [%s]), ";
    std::string input_repeats_str = paddle::string::Sprintf(TENSOR_REPEATS_TEMPLATE, egr::EagerUtils::TensorStr(repeats));
    input_str += input_repeats_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::repeat_interleave_with_tensor_index_grad(x, repeats, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("repeat_interleave_with_tensor_index_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op repeat_interleave_with_tensor_index_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: repeat_interleave_with_tensor_index_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_REPEATS_TEMPLATE = " \n( repeats , [%s]), ";
    std::string input_repeats_str = paddle::string::Sprintf(TENSOR_REPEATS_TEMPLATE, egr::EagerUtils::TensorStr(repeats));
    input_str += input_repeats_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReshapeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "reshape_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto xshape = egr::EagerUtils::RecoverTensorWrapper(&this->xshape_);
  auto& grad_out = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "reshape_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::reshape_grad(xshape, grad_out, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reshape_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("reshape_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<ReshapeDoubleGradNode>(new ReshapeDoubleGradNode(1, 2));
    // SetAttributes if needed

    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: reshape_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReshapeDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "reshape_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "reshape_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::reshape_double_grad(grad_out, grad_x_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reshape_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op reshape_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: reshape_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReverseGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "reverse_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "reverse_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = reverse_ad_func(out_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::reverse(out_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reverse_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: reverse_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RnnGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "rnn_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[2], input_metas[2]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto pre_state = egr::EagerUtils::RecoverTensorWrapper(&this->pre_state_);
  auto weight_list = egr::EagerUtils::RecoverTensorWrapper(&this->weight_list_);
  auto sequence_length = egr::EagerUtils::RecoverTensorWrapper(&this->sequence_length_);

  paddle::optional<paddle::experimental::Tensor> sequence_length_optional;
  if( sequence_length.impl() ) sequence_length_optional = paddle::make_optional<paddle::experimental::Tensor>(sequence_length);

  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto dropout_state_out = egr::EagerUtils::RecoverTensorWrapper(&this->dropout_state_out_);
  auto reserve = egr::EagerUtils::RecoverTensorWrapper(&this->reserve_);
  auto& out_grad = hooked_grads[0][0];
  auto& state_grad = hooked_grads[2];
  auto& dropout_prob = this->dropout_prob_;
  auto& is_bidirec = this->is_bidirec_;
  auto& input_size = this->input_size_;
  auto& hidden_size = this->hidden_size_;
  auto& num_layers = this->num_layers_;
  auto& mode = this->mode_;
  auto& seed = this->seed_;
  auto& is_test = this->is_test_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  std::vector<paddle::experimental::Tensor*> api_output_1;
  api_output_1.reserve(returns[1].size());
  for (size_t i = 0; i < returns[1].size(); ++i) {
    if (out_metas[1].empty() || out_metas[1][i].IsStopGradient()) {
      api_output_1.push_back(nullptr);
    } else {
      api_output_1.push_back(&returns[1][i]);
    }
  }
  std::vector<paddle::experimental::Tensor*> api_output_2;
  api_output_2.reserve(returns[2].size());
  for (size_t i = 0; i < returns[2].size(); ++i) {
    if (out_metas[2].empty() || out_metas[2][i].IsStopGradient()) {
      api_output_2.push_back(nullptr);
    } else {
      api_output_2.push_back(&returns[2][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "rnn_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_STATE_GRAD_TEMPLATE = " \n( state_grad , [%s]), ";
    std::string input_state_grad_str = paddle::string::Sprintf(TENSOR_STATE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(state_grad));
    input_str += input_state_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_PRE_STATE_TEMPLATE = " \n( pre_state , [%s]), ";
    std::string input_pre_state_str = paddle::string::Sprintf(TENSOR_PRE_STATE_TEMPLATE, egr::EagerUtils::TensorStr(pre_state));
    input_str += input_pre_state_str; 
    const char* TENSOR_WEIGHT_LIST_TEMPLATE = " \n( weight_list , [%s]), ";
    std::string input_weight_list_str = paddle::string::Sprintf(TENSOR_WEIGHT_LIST_TEMPLATE, egr::EagerUtils::TensorStr(weight_list));
    input_str += input_weight_list_str; 
    const char* TENSOR_SEQUENCE_LENGTH_TEMPLATE = " \n( sequence_length , [%s]), ";
    std::string input_sequence_length_str = paddle::string::Sprintf(TENSOR_SEQUENCE_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(sequence_length));
    input_str += input_sequence_length_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_DROPOUT_STATE_OUT_TEMPLATE = " \n( dropout_state_out , [%s]), ";
    std::string input_dropout_state_out_str = paddle::string::Sprintf(TENSOR_DROPOUT_STATE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(dropout_state_out));
    input_str += input_dropout_state_out_str; 
    const char* TENSOR_RESERVE_TEMPLATE = " \n( reserve , [%s]), ";
    std::string input_reserve_str = paddle::string::Sprintf(TENSOR_RESERVE_TEMPLATE, egr::EagerUtils::TensorStr(reserve));
    input_str += input_reserve_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::rnn_grad(x, pre_state, weight_list, sequence_length_optional, out, dropout_state_out, reserve, out_grad, state_grad, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("rnn_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

    auto& pre_state_grad = returns[1];
    std::vector<egr::AutogradMeta*> pre_state_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&pre_state_grad);
    for(auto* meta : pre_state_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }


    auto& weight_list_grad = returns[2];
    std::vector<egr::AutogradMeta*> weight_list_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&weight_list_grad);
    for(auto* meta : weight_list_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op rnn_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: rnn_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_STATE_GRAD_TEMPLATE = " \n( state_grad , [%s]), ";
    std::string input_state_grad_str = paddle::string::Sprintf(TENSOR_STATE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(state_grad));
    input_str += input_state_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_PRE_STATE_TEMPLATE = " \n( pre_state , [%s]), ";
    std::string input_pre_state_str = paddle::string::Sprintf(TENSOR_PRE_STATE_TEMPLATE, egr::EagerUtils::TensorStr(pre_state));
    input_str += input_pre_state_str; 
    const char* TENSOR_WEIGHT_LIST_TEMPLATE = " \n( weight_list , [%s]), ";
    std::string input_weight_list_str = paddle::string::Sprintf(TENSOR_WEIGHT_LIST_TEMPLATE, egr::EagerUtils::TensorStr(weight_list));
    input_str += input_weight_list_str; 
    const char* TENSOR_SEQUENCE_LENGTH_TEMPLATE = " \n( sequence_length , [%s]), ";
    std::string input_sequence_length_str = paddle::string::Sprintf(TENSOR_SEQUENCE_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(sequence_length));
    input_str += input_sequence_length_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_DROPOUT_STATE_OUT_TEMPLATE = " \n( dropout_state_out , [%s]), ";
    std::string input_dropout_state_out_str = paddle::string::Sprintf(TENSOR_DROPOUT_STATE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(dropout_state_out));
    input_str += input_dropout_state_out_str; 
    const char* TENSOR_RESERVE_TEMPLATE = " \n( reserve , [%s]), ";
    std::string input_reserve_str = paddle::string::Sprintf(TENSOR_RESERVE_TEMPLATE, egr::EagerUtils::TensorStr(reserve));
    input_str += input_reserve_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_PRE_STATE_GRAD_TEMPLATE = " \n ( pre_state_grad , [%s]), ";
    std::string output_pre_state_grad_str = paddle::string::Sprintf(TENSOR_PRE_STATE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(pre_state_grad));
    output_str += output_pre_state_grad_str; 
    const char* TENSOR_WEIGHT_LIST_GRAD_TEMPLATE = " \n ( weight_list_grad , [%s]), ";
    std::string output_weight_list_grad_str = paddle::string::Sprintf(TENSOR_WEIGHT_LIST_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(weight_list_grad));
    output_str += output_weight_list_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RoiAlignGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "roi_align_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto boxes = egr::EagerUtils::RecoverTensorWrapper(&this->boxes_);
  auto boxes_num = egr::EagerUtils::RecoverTensorWrapper(&this->boxes_num_);

  paddle::optional<paddle::experimental::Tensor> boxes_num_optional;
  if( boxes_num.impl() ) boxes_num_optional = paddle::make_optional<paddle::experimental::Tensor>(boxes_num);

  auto& out_grad = hooked_grads[0][0];
  auto& pooled_height = this->pooled_height_;
  auto& pooled_width = this->pooled_width_;
  auto& spatial_scale = this->spatial_scale_;
  auto& sampling_ratio = this->sampling_ratio_;
  auto& aligned = this->aligned_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "roi_align_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::roi_align_grad(x, boxes, boxes_num_optional, out_grad, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("roi_align_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op roi_align_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: roi_align_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> RoiPoolGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "roi_pool_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto boxes = egr::EagerUtils::RecoverTensorWrapper(&this->boxes_);
  auto boxes_num = egr::EagerUtils::RecoverTensorWrapper(&this->boxes_num_);

  paddle::optional<paddle::experimental::Tensor> boxes_num_optional;
  if( boxes_num.impl() ) boxes_num_optional = paddle::make_optional<paddle::experimental::Tensor>(boxes_num);

  auto arg_max = egr::EagerUtils::RecoverTensorWrapper(&this->arg_max_);
  auto& out_grad = hooked_grads[0][0];
  auto& pooled_height = this->pooled_height_;
  auto& pooled_width = this->pooled_width_;
  auto& spatial_scale = this->spatial_scale_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "roi_pool_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_ARG_MAX_TEMPLATE = " \n( arg_max , [%s]), ";
    std::string input_arg_max_str = paddle::string::Sprintf(TENSOR_ARG_MAX_TEMPLATE, egr::EagerUtils::TensorStr(arg_max));
    input_str += input_arg_max_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::roi_pool_grad(x, boxes, boxes_num_optional, arg_max, out_grad, pooled_height, pooled_width, spatial_scale, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("roi_pool_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op roi_pool_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: roi_pool_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_BOXES_TEMPLATE = " \n( boxes , [%s]), ";
    std::string input_boxes_str = paddle::string::Sprintf(TENSOR_BOXES_TEMPLATE, egr::EagerUtils::TensorStr(boxes));
    input_str += input_boxes_str; 
    const char* TENSOR_BOXES_NUM_TEMPLATE = " \n( boxes_num , [%s]), ";
    std::string input_boxes_num_str = paddle::string::Sprintf(TENSOR_BOXES_NUM_TEMPLATE, egr::EagerUtils::TensorStr(boxes_num));
    input_str += input_boxes_num_str; 
    const char* TENSOR_ARG_MAX_TEMPLATE = " \n( arg_max , [%s]), ";
    std::string input_arg_max_str = paddle::string::Sprintf(TENSOR_ARG_MAX_TEMPLATE, egr::EagerUtils::TensorStr(arg_max));
    input_str += input_arg_max_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ScaleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "scale_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& scale = this->scale_;
  auto& bias_after_scale = this->bias_after_scale_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "scale_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = scale_ad_func(out_grad, scale, 0.0, bias_after_scale);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::scale(out_grad, scale, 0.0, bias_after_scale);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scale_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: scale_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SegmentPoolGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "segment_pool_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto segment_ids = egr::EagerUtils::RecoverTensorWrapper(&this->segment_ids_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto summed_ids = egr::EagerUtils::RecoverTensorWrapper(&this->summed_ids_);

  paddle::optional<paddle::experimental::Tensor> summed_ids_optional;
  if( summed_ids.impl() ) summed_ids_optional = paddle::make_optional<paddle::experimental::Tensor>(summed_ids);

  auto& out_grad = hooked_grads[0][0];
  auto& pooltype = this->pooltype_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "segment_pool_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SEGMENT_IDS_TEMPLATE = " \n( segment_ids , [%s]), ";
    std::string input_segment_ids_str = paddle::string::Sprintf(TENSOR_SEGMENT_IDS_TEMPLATE, egr::EagerUtils::TensorStr(segment_ids));
    input_str += input_segment_ids_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_SUMMED_IDS_TEMPLATE = " \n( summed_ids , [%s]), ";
    std::string input_summed_ids_str = paddle::string::Sprintf(TENSOR_SUMMED_IDS_TEMPLATE, egr::EagerUtils::TensorStr(summed_ids));
    input_str += input_summed_ids_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::segment_pool_grad(x, segment_ids, out, summed_ids_optional, out_grad, pooltype, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("segment_pool_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op segment_pool_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: segment_pool_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SEGMENT_IDS_TEMPLATE = " \n( segment_ids , [%s]), ";
    std::string input_segment_ids_str = paddle::string::Sprintf(TENSOR_SEGMENT_IDS_TEMPLATE, egr::EagerUtils::TensorStr(segment_ids));
    input_str += input_segment_ids_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_SUMMED_IDS_TEMPLATE = " \n( summed_ids , [%s]), ";
    std::string input_summed_ids_str = paddle::string::Sprintf(TENSOR_SUMMED_IDS_TEMPLATE, egr::EagerUtils::TensorStr(summed_ids));
    input_str += input_summed_ids_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SendURecvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "send_u_recv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto src_index = egr::EagerUtils::RecoverTensorWrapper(&this->src_index_);
  auto dst_index = egr::EagerUtils::RecoverTensorWrapper(&this->dst_index_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);

  paddle::optional<paddle::experimental::Tensor> out_optional;
  if( out.impl() ) out_optional = paddle::make_optional<paddle::experimental::Tensor>(out);

  auto dst_count = egr::EagerUtils::RecoverTensorWrapper(&this->dst_count_);

  paddle::optional<paddle::experimental::Tensor> dst_count_optional;
  if( dst_count.impl() ) dst_count_optional = paddle::make_optional<paddle::experimental::Tensor>(dst_count);

  auto& out_grad = hooked_grads[0][0];
  auto& reduce_op = this->reduce_op_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "send_u_recv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_DST_COUNT_TEMPLATE = " \n( dst_count , [%s]), ";
    std::string input_dst_count_str = paddle::string::Sprintf(TENSOR_DST_COUNT_TEMPLATE, egr::EagerUtils::TensorStr(dst_count));
    input_str += input_dst_count_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::send_u_recv_grad(x, src_index, dst_index, out_optional, dst_count_optional, out_grad, reduce_op, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("send_u_recv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op send_u_recv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: send_u_recv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_DST_COUNT_TEMPLATE = " \n( dst_count , [%s]), ";
    std::string input_dst_count_str = paddle::string::Sprintf(TENSOR_DST_COUNT_TEMPLATE, egr::EagerUtils::TensorStr(dst_count));
    input_str += input_dst_count_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SendUeRecvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "send_ue_recv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto src_index = egr::EagerUtils::RecoverTensorWrapper(&this->src_index_);
  auto dst_index = egr::EagerUtils::RecoverTensorWrapper(&this->dst_index_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);

  paddle::optional<paddle::experimental::Tensor> out_optional;
  if( out.impl() ) out_optional = paddle::make_optional<paddle::experimental::Tensor>(out);

  auto dst_count = egr::EagerUtils::RecoverTensorWrapper(&this->dst_count_);

  paddle::optional<paddle::experimental::Tensor> dst_count_optional;
  if( dst_count.impl() ) dst_count_optional = paddle::make_optional<paddle::experimental::Tensor>(dst_count);

  auto& out_grad = hooked_grads[0][0];
  auto& message_op = this->message_op_;
  auto& reduce_op = this->reduce_op_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "send_ue_recv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_DST_COUNT_TEMPLATE = " \n( dst_count , [%s]), ";
    std::string input_dst_count_str = paddle::string::Sprintf(TENSOR_DST_COUNT_TEMPLATE, egr::EagerUtils::TensorStr(dst_count));
    input_str += input_dst_count_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::send_ue_recv_grad(x, y, src_index, dst_index, out_optional, dst_count_optional, out_grad, message_op, reduce_op, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("send_ue_recv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op send_ue_recv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: send_ue_recv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_SRC_INDEX_TEMPLATE = " \n( src_index , [%s]), ";
    std::string input_src_index_str = paddle::string::Sprintf(TENSOR_SRC_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(src_index));
    input_str += input_src_index_str; 
    const char* TENSOR_DST_INDEX_TEMPLATE = " \n( dst_index , [%s]), ";
    std::string input_dst_index_str = paddle::string::Sprintf(TENSOR_DST_INDEX_TEMPLATE, egr::EagerUtils::TensorStr(dst_index));
    input_str += input_dst_index_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_DST_COUNT_TEMPLATE = " \n( dst_count , [%s]), ";
    std::string input_dst_count_str = paddle::string::Sprintf(TENSOR_DST_COUNT_TEMPLATE, egr::EagerUtils::TensorStr(dst_count));
    input_str += input_dst_count_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SigmoidCrossEntropyWithLogitsGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sigmoid_cross_entropy_with_logits_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto label = egr::EagerUtils::RecoverTensorWrapper(&this->label_);
  auto& out_grad = hooked_grads[0][0];
  auto& normalize = this->normalize_;
  auto& ignore_index = this->ignore_index_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "sigmoid_cross_entropy_with_logits_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sigmoid_cross_entropy_with_logits_grad(x, label, out_grad, normalize, ignore_index, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sigmoid_cross_entropy_with_logits_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sigmoid_cross_entropy_with_logits_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sigmoid_cross_entropy_with_logits_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_LABEL_TEMPLATE = " \n( label , [%s]), ";
    std::string input_label_str = paddle::string::Sprintf(TENSOR_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(label));
    input_str += input_label_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SignGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sign_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sign_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = scale_ad_func(out_grad, 0.0, 0.0, true);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::scale(out_grad, 0.0, 0.0, true);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sign_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: sign_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SliceGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "slice_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto& grad_out = hooked_grads[0][0];
  auto& axes = this->axes_;
  auto& starts = this->starts_;
  auto& ends = this->ends_;
  auto& infer_flags = this->infer_flags_;
  auto& decrease_axis = this->decrease_axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "slice_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::slice_grad(input, grad_out, axes, starts, ends, infer_flags, decrease_axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("slice_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_input = returns[0][0];
  egr::AutogradMeta* grad_input_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_input) : nullptr;
  if (grad_input_autograd_meta) grad_input_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("slice_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SliceDoubleGradNode>(new SliceDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    grad_node->SetAttributestarts(starts);
    grad_node->SetAttributeends(ends);
    grad_node->SetAttributeinfer_flags(infer_flags);
    grad_node->SetAttributedecrease_axis(decrease_axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_input_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_input_autograd_meta, 0);
    }
    if (grad_input_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_input_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_input, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_input);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: slice_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_GRAD_INPUT_TEMPLATE = " \n ( grad_input , [%s]), ";
    std::string output_grad_input_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_input));
    output_str += output_grad_input_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SliceDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "slice_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_input_grad = hooked_grads[0][0];
  auto& axes = this->axes_;
  auto& starts = this->starts_;
  auto& ends = this->ends_;
  auto& infer_flags = this->infer_flags_;
  auto& decrease_axis = this->decrease_axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "slice_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_INPUT_GRAD_TEMPLATE = " \n( grad_input_grad , [%s]), ";
    std::string input_grad_input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_input_grad));
    input_str += input_grad_input_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = slice_ad_func(grad_input_grad, axes, starts, ends, infer_flags, decrease_axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::slice(grad_input_grad, axes, starts, ends, infer_flags, decrease_axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("slice_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: slice_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_INPUT_GRAD_TEMPLATE = " \n( grad_input_grad , [%s]), ";
    std::string input_grad_input_grad_str = paddle::string::Sprintf(TENSOR_GRAD_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_input_grad));
    input_str += input_grad_input_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SlogdetGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "slogdet_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "slogdet_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::slogdet_grad(x, out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("slogdet_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op slogdet_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: slogdet_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SoftmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "softmax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "softmax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::softmax_grad(out, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softmax_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op softmax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: softmax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SpectralNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "spectral_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto weight = egr::EagerUtils::RecoverTensorWrapper(&this->weight_);
  auto u = egr::EagerUtils::RecoverTensorWrapper(&this->u_);
  auto v = egr::EagerUtils::RecoverTensorWrapper(&this->v_);
  auto& out_grad = hooked_grads[0][0];
  auto& dim = this->dim_;
  auto& power_iters = this->power_iters_;
  auto& eps = this->eps_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "spectral_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_V_TEMPLATE = " \n( v , [%s]), ";
    std::string input_v_str = paddle::string::Sprintf(TENSOR_V_TEMPLATE, egr::EagerUtils::TensorStr(v));
    input_str += input_v_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::spectral_norm_grad(weight, u, v, out_grad, dim, power_iters, eps, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("spectral_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& weight_grad = returns[0][0];
  egr::AutogradMeta* weight_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&weight_grad) : nullptr;
  if (weight_grad_autograd_meta) weight_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op spectral_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: spectral_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_WEIGHT_TEMPLATE = " \n( weight , [%s]), ";
    std::string input_weight_str = paddle::string::Sprintf(TENSOR_WEIGHT_TEMPLATE, egr::EagerUtils::TensorStr(weight));
    input_str += input_weight_str; 
    const char* TENSOR_U_TEMPLATE = " \n( u , [%s]), ";
    std::string input_u_str = paddle::string::Sprintf(TENSOR_U_TEMPLATE, egr::EagerUtils::TensorStr(u));
    input_str += input_u_str; 
    const char* TENSOR_V_TEMPLATE = " \n( v , [%s]), ";
    std::string input_v_str = paddle::string::Sprintf(TENSOR_V_TEMPLATE, egr::EagerUtils::TensorStr(v));
    input_str += input_v_str; 
    const char* TENSOR_WEIGHT_GRAD_TEMPLATE = " \n ( weight_grad , [%s]), ";
    std::string output_weight_grad_str = paddle::string::Sprintf(TENSOR_WEIGHT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(weight_grad));
    output_str += output_weight_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SplitGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "split_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0], input_metas[0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "split_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = concat_ad_func( out_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::concat( out_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("split_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: split_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SplitWithNumGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "split_with_num_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0], input_metas[0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "split_with_num_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = concat_ad_func( out_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::concat( out_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("split_with_num_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: split_with_num_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SquaredL2NormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "squared_l2_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "squared_l2_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::squared_l2_norm_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("squared_l2_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op squared_l2_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: squared_l2_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "squeeze_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto xshape = egr::EagerUtils::RecoverTensorWrapper(&this->xshape_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "squeeze_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::squeeze_grad(xshape, grad_out, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("squeeze_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("squeeze_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SqueezeDoubleGradNode>(new SqueezeDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: squeeze_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SqueezeDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "squeeze_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "squeeze_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = squeeze_ad_func(grad_x_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::squeeze(grad_x_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("squeeze_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: squeeze_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> StackGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "stack_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  std::vector<paddle::experimental::Tensor*> api_output_0;
  api_output_0.reserve(returns[0].size());
  for (size_t i = 0; i < returns[0].size(); ++i) {
    if (out_metas[0].empty() || out_metas[0][i].IsStopGradient()) {
      api_output_0.push_back(nullptr);
    } else {
      api_output_0.push_back(&returns[0][i]);
    }
  }
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "stack_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::stack_grad(x, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("stack_grad", returns); }

  // Get GradOut autograd_meta

    auto& x_grad = returns[0];
    std::vector<egr::AutogradMeta*> x_grad_autograd_meta_vec = egr::EagerUtils::autograd_meta(&x_grad);
    for(auto* meta : x_grad_autograd_meta_vec){
        meta->SetStopGradient(false);
    }

  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op stack_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: stack_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> StridedSliceGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "strided_slice_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& axes = this->axes_;
  auto& starts = this->starts_;
  auto& ends = this->ends_;
  auto& strides = this->strides_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "strided_slice_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::strided_slice_grad(x, out_grad, axes, starts, ends, strides, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("strided_slice_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op strided_slice_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: strided_slice_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SubtractGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "subtract_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    VLOG(6) << "No Inplace should happend for wrappered input: {inplace_grad_input_str}";
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "subtract_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::subtract_grad(x, y, grad_out, axis, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("subtract_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  

  auto& grad_y = returns[1][0];
  egr::AutogradMeta* grad_y_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_y) : nullptr;
  if (grad_y_autograd_meta) grad_y_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("subtract_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SubtractDoubleGradNode>(new SubtractDoubleGradNode(2, 3));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    // Set TensorWrappers for Forward Inputs if needed
    grad_node->SetTensorWrappery(y);
    grad_node->SetTensorWrappergrad_out(grad_out);
    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 2);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_y_autograd_meta, 1);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    if (grad_y_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_y_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    grad_node->SetGradInMeta(grad_y, 1);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    egr::EagerUtils::CheckAndRetainGrad(grad_y);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: subtract_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
    const char* TENSOR_GRAD_Y_TEMPLATE = " \n ( grad_y , [%s]), ";
    std::string output_grad_y_str = paddle::string::Sprintf(TENSOR_GRAD_Y_TEMPLATE, egr::EagerUtils::TensorStr(grad_y));
    output_str += output_grad_y_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SubtractDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "subtract_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[0][0], input_metas[0][0]);
  egr::EagerUtils::FillZeroForEmptyOptionalGradInput(&grads[1][0], input_metas[1][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto grad_out = egr::EagerUtils::RecoverTensorWrapper(&this->grad_out_);
  auto& grad_x_grad = hooked_grads[0][0];

  paddle::optional<paddle::experimental::Tensor> grad_x_grad_optional;
  if(grad_x_grad.initialized()) grad_x_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_x_grad);

  auto& grad_y_grad = hooked_grads[1][0];

  paddle::optional<paddle::experimental::Tensor> grad_y_grad_optional;
  if(grad_y_grad.initialized()) grad_y_grad_optional = paddle::make_optional<paddle::experimental::Tensor>(grad_y_grad);

  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_x_grad.initialized()) {
    VLOG(10) << grad_x_grad.name() << "(grad_x_grad) use_count: " << grad_x_grad.impl().use_count();
    if (grad_x_grad.impl().use_count() == 1 || (grad_x_grad.impl().use_count() == 2 && grad_x_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_x_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "subtract_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::subtract_double_grad(y, grad_out, grad_x_grad_optional, grad_y_grad_optional, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("subtract_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[2][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op subtract_double_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: subtract_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_Y_GRAD_TEMPLATE = " \n( grad_y_grad , [%s]), ";
    std::string input_grad_y_grad_str = paddle::string::Sprintf(TENSOR_GRAD_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_y_grad));
    input_str += input_grad_y_grad_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SumGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sum_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  auto& reduce_all = this->reduce_all_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sum_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sum_grad(x, grad_out, axis, keepdim, reduce_all, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sum_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("sum_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<SumDoubleGradNode>(new SumDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxis(axis);
    grad_node->SetAttributekeepdim(keepdim);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: sum_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SumDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sum_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  auto& keepdim = this->keepdim_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sum_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = sum_ad_func(grad_x_grad, axis, grad_x_grad.dtype(), keepdim);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::sum(grad_x_grad, axis, grad_x_grad.dtype(), keepdim);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sum_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: sum_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SwishGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "swish_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& bete = this->bete_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "swish_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::swish_grad(x, out_grad, bete, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("swish_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op swish_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: swish_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SyncBatchNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sync_batch_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);
  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);
  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto reserve_space = egr::EagerUtils::RecoverTensorWrapper(&this->reserve_space_);

  paddle::optional<paddle::experimental::Tensor> reserve_space_optional;
  if( reserve_space.impl() ) reserve_space_optional = paddle::make_optional<paddle::experimental::Tensor>(reserve_space);

  auto& out_grad = hooked_grads[0][0];
  auto& momentum = this->momentum_;
  auto& epsilon = this->epsilon_;
  auto& data_layout = this->data_layout_;
  auto& is_test = this->is_test_;
  auto& use_global_stats = this->use_global_stats_;
  auto& trainable_statistics = this->trainable_statistics_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  auto* api_output_2 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sync_batch_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sync_batch_norm_grad(x, scale, bias, saved_mean, saved_variance, reserve_space_optional, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sync_batch_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& scale_grad = returns[3][0];
  egr::AutogradMeta* scale_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&scale_grad) : nullptr;
  if (scale_grad_autograd_meta) scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[4][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sync_batch_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sync_batch_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_SCALE_GRAD_TEMPLATE = " \n ( scale_grad , [%s]), ";
    std::string output_scale_grad_str = paddle::string::Sprintf(TENSOR_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(scale_grad));
    output_str += output_scale_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TemporalShiftGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "temporal_shift_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& seg_num = this->seg_num_;
  auto& shift_ratio = this->shift_ratio_;
  auto& data_format_str = this->data_format_str_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "temporal_shift_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::temporal_shift_grad(out_grad, seg_num, shift_ratio, data_format_str, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("temporal_shift_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op temporal_shift_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: temporal_shift_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TileGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tile_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& grad_out = hooked_grads[0][0];
  auto& repeat_times = this->repeat_times_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "tile_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::tile_grad(x, grad_out, repeat_times, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tile_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("tile_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<TileDoubleGradNode>(new TileDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributerepeat_times(repeat_times);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: tile_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TileDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tile_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& repeat_times = this->repeat_times_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "tile_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = tile_ad_func(grad_x_grad, repeat_times);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::tile(grad_x_grad, repeat_times);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tile_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: tile_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TransposeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "transpose_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_out = hooked_grads[0][0];
  auto& perm = this->perm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "transpose_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::transpose_grad(grad_out, perm, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("transpose_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("transpose_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<TransposeDoubleGradNode>(new TransposeDoubleGradNode(1, 1));
    // SetAttributes if needed
    grad_node->SetAttributeperm(perm);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 0);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: transpose_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TransposeDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "transpose_double_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& perm = this->perm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "transpose_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = transpose_ad_func(grad_x_grad, perm);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::transpose(grad_x_grad, perm);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("transpose_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[0][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: transpose_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TriangularSolveGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "triangular_solve_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& upper = this->upper_;
  auto& tranpose = this->tranpose_;
  auto& unitriangular = this->unitriangular_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "triangular_solve_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::triangular_solve_grad(x, y, out, out_grad, upper, tranpose, unitriangular, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("triangular_solve_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op triangular_solve_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: triangular_solve_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TrilGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tril_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& diagonal = this->diagonal_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "tril_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::tril_grad(out_grad, diagonal, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tril_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op tril_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: tril_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TrilinearInterpGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "trilinear_interp_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto out_size = egr::EagerUtils::RecoverTensorWrapper(&this->out_size_);

  paddle::optional<paddle::experimental::Tensor> out_size_optional;
  if( out_size.impl() ) out_size_optional = paddle::make_optional<paddle::experimental::Tensor>(out_size);

  auto size_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->size_tensor_);

  paddle::optional<std::vector<paddle::experimental::Tensor>> size_tensor_optional;
  if( !size_tensor.empty() ) size_tensor_optional = paddle::make_optional<std::vector<paddle::experimental::Tensor>>(size_tensor);

  auto scale_tensor = egr::EagerUtils::RecoverTensorWrapper(&this->scale_tensor_);

  paddle::optional<paddle::experimental::Tensor> scale_tensor_optional;
  if( scale_tensor.impl() ) scale_tensor_optional = paddle::make_optional<paddle::experimental::Tensor>(scale_tensor);

  auto& output_grad = hooked_grads[0][0];
  auto& data_layout = this->data_layout_;
  auto& out_d = this->out_d_;
  auto& out_h = this->out_h_;
  auto& out_w = this->out_w_;
  auto& scale = this->scale_;
  auto& interp_method = this->interp_method_;
  auto& align_corners = this->align_corners_;
  auto& align_mode = this->align_mode_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "trilinear_interp_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::trilinear_interp_grad(x, out_size_optional, size_tensor_optional, scale_tensor_optional, output_grad, data_layout, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("trilinear_interp_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op trilinear_interp_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: trilinear_interp_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUTPUT_GRAD_TEMPLATE = " \n( output_grad , [%s]), ";
    std::string input_output_grad_str = paddle::string::Sprintf(TENSOR_OUTPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(output_grad));
    input_str += input_output_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_OUT_SIZE_TEMPLATE = " \n( out_size , [%s]), ";
    std::string input_out_size_str = paddle::string::Sprintf(TENSOR_OUT_SIZE_TEMPLATE, egr::EagerUtils::TensorStr(out_size));
    input_str += input_out_size_str; 
    const char* TENSOR_SIZE_TENSOR_TEMPLATE = " \n( size_tensor , [%s]), ";
    std::string input_size_tensor_str = paddle::string::Sprintf(TENSOR_SIZE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(size_tensor));
    input_str += input_size_tensor_str; 
    const char* TENSOR_SCALE_TENSOR_TEMPLATE = " \n( scale_tensor , [%s]), ";
    std::string input_scale_tensor_str = paddle::string::Sprintf(TENSOR_SCALE_TENSOR_TEMPLATE, egr::EagerUtils::TensorStr(scale_tensor));
    input_str += input_scale_tensor_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TriuGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "triu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& diagonal = this->diagonal_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "triu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::triu_grad(out_grad, diagonal, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("triu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op triu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: triu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UnbindGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unbind_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "unbind_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = stack_ad_func(out_grad, axis);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::stack(out_grad, axis);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unbind_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: unbind_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UniformInplaceGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "uniform_inplace_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& min = this->min_;
  auto& max = this->max_;
  auto& seed = this->seed_;
  auto& diag_num = this->diag_num_;
  auto& diag_step = this->diag_step_;
  auto& diag_val = this->diag_val_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (out_grad.initialized()) {
    VLOG(10) << out_grad.name() << "(out_grad) use_count: " << out_grad.impl().use_count();
    if (out_grad.impl().use_count() == 1 || (out_grad.impl().use_count() == 2 && out_grad.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy
 if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(out_grad, api_output_0);
    }

  VLOG(5) << "Running C++ API: " << "uniform_inplace_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::uniform_inplace_grad(out_grad, min, max, seed, diag_num, diag_step, diag_val, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("uniform_inplace_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op uniform_inplace_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: uniform_inplace_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UnpoolGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unpool_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& ksize = this->ksize_;
  auto& strides = this->strides_;
  auto& padding = this->padding_;
  auto& output_size = this->output_size_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "unpool_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::unpool_grad(x, indices, out, out_grad, ksize, strides, padding, output_size, data_format, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unpool_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op unpool_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: unpool_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Unpool3dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unpool3d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& ksize = this->ksize_;
  auto& strides = this->strides_;
  auto& padding = this->padding_;
  auto& output_size = this->output_size_;
  auto& data_format = this->data_format_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "unpool3d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::unpool3d_grad(x, indices, out, out_grad, ksize, strides, padding, output_size, data_format, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unpool3d_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op unpool3d_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: unpool3d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UnsqueezeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unsqueeze_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto xshape = egr::EagerUtils::RecoverTensorWrapper(&this->xshape_);
  auto& grad_out = hooked_grads[0][0];
  auto& axes = this->axes_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  bool can_be_inplaced = false;
  if (grad_out.initialized()) {
    VLOG(10) << grad_out.name() << "(out_grad) use_count: " << grad_out.impl().use_count();
    if (grad_out.impl().use_count() == 1 || (grad_out.impl().use_count() == 2 && grad_out.impl().get() == grads[0][0].impl().get())) {
      can_be_inplaced = true;
    }
  }
  // Inplace Strategy

  if (trace_backward) {
    if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  } else {
     if (api_output_0 != nullptr && can_be_inplaced) {
      egr::EagerUtils::HandleViewBetweenInputAndOutput(grad_out, api_output_0);
    }
  }

  VLOG(5) << "Running C++ API: " << "unsqueeze_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::unsqueeze_grad(xshape, grad_out, axes, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unsqueeze_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_x = returns[0][0];
  egr::AutogradMeta* grad_x_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_x) : nullptr;
  if (grad_x_autograd_meta) grad_x_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    paddle::platform::RecordEvent node_creation_record_event("unsqueeze_grad node_creation", paddle::platform::TracerEventType::OperatorInner, 1);

    // Node Construction
    auto grad_node = std::shared_ptr<UnsqueezeDoubleGradNode>(new UnsqueezeDoubleGradNode(1, 2));
    // SetAttributes if needed
    grad_node->SetAttributeaxes(axes);
    // Set TensorWrappers for Forward Inputs if needed

    // SetGradOutMeta & SetEdges
    grad_node->SetGradOutMeta(grad_out, 1);
    // SetOutRank & SetHistory & SetGradInMeta & RetainGrad
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetOutRankWithSlot(grad_x_autograd_meta, 0);
    }
    if (grad_x_autograd_meta) {
      egr::EagerUtils::SetHistory(grad_x_autograd_meta, grad_node);
    }
    grad_node->SetGradInMeta(grad_x, 0);
    egr::EagerUtils::CheckAndRetainGrad(grad_x);
    // Set TensorWrappers for Forward Outputs if needed

  }

  VLOG(4) << "Finish AD API GRAD: unsqueeze_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_OUT_TEMPLATE = " \n( grad_out , [%s]), ";
    std::string input_grad_out_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_TEMPLATE, egr::EagerUtils::TensorStr(grad_out));
    input_str += input_grad_out_str; 
    const char* TENSOR_XSHAPE_TEMPLATE = " \n( xshape , [%s]), ";
    std::string input_xshape_str = paddle::string::Sprintf(TENSOR_XSHAPE_TEMPLATE, egr::EagerUtils::TensorStr(xshape));
    input_str += input_xshape_str; 
    const char* TENSOR_GRAD_X_TEMPLATE = " \n ( grad_x , [%s]), ";
    std::string output_grad_x_str = paddle::string::Sprintf(TENSOR_GRAD_X_TEMPLATE, egr::EagerUtils::TensorStr(grad_x));
    output_str += output_grad_x_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> UnsqueezeDoubleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "unsqueeze_double_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& grad_x_grad = hooked_grads[0][0];
  auto& axes = this->axes_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "unsqueeze_double_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = unsqueeze_ad_func(grad_x_grad, axes);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::unsqueeze(grad_x_grad, axes);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("unsqueeze_double_grad", returns); }

  // Get GradOut autograd_meta

  auto& grad_out_grad = returns[1][0];
  egr::AutogradMeta* grad_out_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&grad_out_grad) : nullptr;
  if (grad_out_grad_autograd_meta) grad_out_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: unsqueeze_double_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_GRAD_X_GRAD_TEMPLATE = " \n( grad_x_grad , [%s]), ";
    std::string input_grad_x_grad_str = paddle::string::Sprintf(TENSOR_GRAD_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_x_grad));
    input_str += input_grad_x_grad_str; 
    const char* TENSOR_GRAD_OUT_GRAD_TEMPLATE = " \n ( grad_out_grad , [%s]), ";
    std::string output_grad_out_grad_str = paddle::string::Sprintf(TENSOR_GRAD_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(grad_out_grad));
    output_str += output_grad_out_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> WarpctcGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "warpctc_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto logits = egr::EagerUtils::RecoverTensorWrapper(&this->logits_);
  auto logits_length = egr::EagerUtils::RecoverTensorWrapper(&this->logits_length_);

  paddle::optional<paddle::experimental::Tensor> logits_length_optional;
  if( logits_length.impl() ) logits_length_optional = paddle::make_optional<paddle::experimental::Tensor>(logits_length);

  auto warpctcgrad = egr::EagerUtils::RecoverTensorWrapper(&this->warpctcgrad_);
  auto& loss_grad = hooked_grads[0][0];
  auto& blank = this->blank_;
  auto& norm_by_times = this->norm_by_times_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "warpctc_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LOGITS_LENGTH_TEMPLATE = " \n( logits_length , [%s]), ";
    std::string input_logits_length_str = paddle::string::Sprintf(TENSOR_LOGITS_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(logits_length));
    input_str += input_logits_length_str; 
    const char* TENSOR_WARPCTCGRAD_TEMPLATE = " \n( warpctcgrad , [%s]), ";
    std::string input_warpctcgrad_str = paddle::string::Sprintf(TENSOR_WARPCTCGRAD_TEMPLATE, egr::EagerUtils::TensorStr(warpctcgrad));
    input_str += input_warpctcgrad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::warpctc_grad(logits, logits_length_optional, warpctcgrad, loss_grad, blank, norm_by_times, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("warpctc_grad", returns); }

  // Get GradOut autograd_meta

  auto& logits_grad = returns[0][0];
  egr::AutogradMeta* logits_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&logits_grad) : nullptr;
  if (logits_grad_autograd_meta) logits_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op warpctc_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: warpctc_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_LOGITS_TEMPLATE = " \n( logits , [%s]), ";
    std::string input_logits_str = paddle::string::Sprintf(TENSOR_LOGITS_TEMPLATE, egr::EagerUtils::TensorStr(logits));
    input_str += input_logits_str; 
    const char* TENSOR_LOGITS_LENGTH_TEMPLATE = " \n( logits_length , [%s]), ";
    std::string input_logits_length_str = paddle::string::Sprintf(TENSOR_LOGITS_LENGTH_TEMPLATE, egr::EagerUtils::TensorStr(logits_length));
    input_str += input_logits_length_str; 
    const char* TENSOR_WARPCTCGRAD_TEMPLATE = " \n( warpctcgrad , [%s]), ";
    std::string input_warpctcgrad_str = paddle::string::Sprintf(TENSOR_WARPCTCGRAD_TEMPLATE, egr::EagerUtils::TensorStr(warpctcgrad));
    input_str += input_warpctcgrad_str; 
    const char* TENSOR_LOGITS_GRAD_TEMPLATE = " \n ( logits_grad , [%s]), ";
    std::string output_logits_grad_str = paddle::string::Sprintf(TENSOR_LOGITS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(logits_grad));
    output_str += output_logits_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> YoloLossGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "yolo_loss_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto gt_box = egr::EagerUtils::RecoverTensorWrapper(&this->gt_box_);
  auto gt_label = egr::EagerUtils::RecoverTensorWrapper(&this->gt_label_);
  auto gt_score = egr::EagerUtils::RecoverTensorWrapper(&this->gt_score_);

  paddle::optional<paddle::experimental::Tensor> gt_score_optional;
  if( gt_score.impl() ) gt_score_optional = paddle::make_optional<paddle::experimental::Tensor>(gt_score);

  auto objectness_mask = egr::EagerUtils::RecoverTensorWrapper(&this->objectness_mask_);
  auto gt_match_mask = egr::EagerUtils::RecoverTensorWrapper(&this->gt_match_mask_);
  auto& loss_grad = hooked_grads[0][0];
  auto& anchors = this->anchors_;
  auto& anchor_mask = this->anchor_mask_;
  auto& class_num = this->class_num_;
  auto& ignore_thresh = this->ignore_thresh_;
  auto& downsample_ratio = this->downsample_ratio_;
  auto& use_label_smooth = this->use_label_smooth_;
  auto& scale_x_y = this->scale_x_y_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(4);
  for (int i = 0; i < 4; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  auto* api_output_3 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "yolo_loss_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GT_BOX_TEMPLATE = " \n( gt_box , [%s]), ";
    std::string input_gt_box_str = paddle::string::Sprintf(TENSOR_GT_BOX_TEMPLATE, egr::EagerUtils::TensorStr(gt_box));
    input_str += input_gt_box_str; 
    const char* TENSOR_GT_LABEL_TEMPLATE = " \n( gt_label , [%s]), ";
    std::string input_gt_label_str = paddle::string::Sprintf(TENSOR_GT_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(gt_label));
    input_str += input_gt_label_str; 
    const char* TENSOR_GT_SCORE_TEMPLATE = " \n( gt_score , [%s]), ";
    std::string input_gt_score_str = paddle::string::Sprintf(TENSOR_GT_SCORE_TEMPLATE, egr::EagerUtils::TensorStr(gt_score));
    input_str += input_gt_score_str; 
    const char* TENSOR_OBJECTNESS_MASK_TEMPLATE = " \n( objectness_mask , [%s]), ";
    std::string input_objectness_mask_str = paddle::string::Sprintf(TENSOR_OBJECTNESS_MASK_TEMPLATE, egr::EagerUtils::TensorStr(objectness_mask));
    input_str += input_objectness_mask_str; 
    const char* TENSOR_GT_MATCH_MASK_TEMPLATE = " \n( gt_match_mask , [%s]), ";
    std::string input_gt_match_mask_str = paddle::string::Sprintf(TENSOR_GT_MATCH_MASK_TEMPLATE, egr::EagerUtils::TensorStr(gt_match_mask));
    input_str += input_gt_match_mask_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::yolo_loss_grad(x, gt_box, gt_label, gt_score_optional, objectness_mask, gt_match_mask, loss_grad, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, api_output_0, api_output_1, api_output_2, api_output_3);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("yolo_loss_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& gt_box_grad = returns[1][0];
  egr::AutogradMeta* gt_box_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&gt_box_grad) : nullptr;
  if (gt_box_grad_autograd_meta) gt_box_grad_autograd_meta->SetStopGradient(false);
  

  auto& gt_label_grad = returns[2][0];
  egr::AutogradMeta* gt_label_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&gt_label_grad) : nullptr;
  if (gt_label_grad_autograd_meta) gt_label_grad_autograd_meta->SetStopGradient(false);
  

  auto& gt_score_grad = returns[3][0];
  egr::AutogradMeta* gt_score_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&gt_score_grad) : nullptr;
  if (gt_score_grad_autograd_meta) gt_score_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op yolo_loss_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: yolo_loss_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_LOSS_GRAD_TEMPLATE = " \n( loss_grad , [%s]), ";
    std::string input_loss_grad_str = paddle::string::Sprintf(TENSOR_LOSS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(loss_grad));
    input_str += input_loss_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_GT_BOX_TEMPLATE = " \n( gt_box , [%s]), ";
    std::string input_gt_box_str = paddle::string::Sprintf(TENSOR_GT_BOX_TEMPLATE, egr::EagerUtils::TensorStr(gt_box));
    input_str += input_gt_box_str; 
    const char* TENSOR_GT_LABEL_TEMPLATE = " \n( gt_label , [%s]), ";
    std::string input_gt_label_str = paddle::string::Sprintf(TENSOR_GT_LABEL_TEMPLATE, egr::EagerUtils::TensorStr(gt_label));
    input_str += input_gt_label_str; 
    const char* TENSOR_GT_SCORE_TEMPLATE = " \n( gt_score , [%s]), ";
    std::string input_gt_score_str = paddle::string::Sprintf(TENSOR_GT_SCORE_TEMPLATE, egr::EagerUtils::TensorStr(gt_score));
    input_str += input_gt_score_str; 
    const char* TENSOR_OBJECTNESS_MASK_TEMPLATE = " \n( objectness_mask , [%s]), ";
    std::string input_objectness_mask_str = paddle::string::Sprintf(TENSOR_OBJECTNESS_MASK_TEMPLATE, egr::EagerUtils::TensorStr(objectness_mask));
    input_str += input_objectness_mask_str; 
    const char* TENSOR_GT_MATCH_MASK_TEMPLATE = " \n( gt_match_mask , [%s]), ";
    std::string input_gt_match_mask_str = paddle::string::Sprintf(TENSOR_GT_MATCH_MASK_TEMPLATE, egr::EagerUtils::TensorStr(gt_match_mask));
    input_str += input_gt_match_mask_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_GT_BOX_GRAD_TEMPLATE = " \n ( gt_box_grad , [%s]), ";
    std::string output_gt_box_grad_str = paddle::string::Sprintf(TENSOR_GT_BOX_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(gt_box_grad));
    output_str += output_gt_box_grad_str; 
    const char* TENSOR_GT_LABEL_GRAD_TEMPLATE = " \n ( gt_label_grad , [%s]), ";
    std::string output_gt_label_grad_str = paddle::string::Sprintf(TENSOR_GT_LABEL_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(gt_label_grad));
    output_str += output_gt_label_grad_str; 
    const char* TENSOR_GT_SCORE_GRAD_TEMPLATE = " \n ( gt_score_grad , [%s]), ";
    std::string output_gt_score_grad_str = paddle::string::Sprintf(TENSOR_GT_SCORE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(gt_score_grad));
    output_str += output_gt_score_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}



namespace sparse {
    
paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AbsGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "abs_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "abs_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::abs_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("abs_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op abs_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: abs_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AcosGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "acos_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "acos_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::acos_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acos_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op acos_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: acos_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AcoshGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "acosh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "acosh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::acosh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("acosh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op acosh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: acosh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AddGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "add_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "add_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::add_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("add_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op add_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: add_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AsinGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "asin_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "asin_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::asin_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asin_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op asin_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: asin_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AsinhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "asinh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "asinh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::asinh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("asinh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op asinh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: asinh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AtanGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "atan_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "atan_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::atan_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atan_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op atan_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: atan_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AtanhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "atanh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "atanh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::atanh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("atanh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op atanh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: atanh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> BatchNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "batch_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);
  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);
  auto mean_out = egr::EagerUtils::RecoverTensorWrapper(&this->mean_out_);

  paddle::optional<paddle::experimental::Tensor> mean_out_optional;
  if( mean_out.impl() ) mean_out_optional = paddle::make_optional<paddle::experimental::Tensor>(mean_out);

  auto variance_out = egr::EagerUtils::RecoverTensorWrapper(&this->variance_out_);

  paddle::optional<paddle::experimental::Tensor> variance_out_optional;
  if( variance_out.impl() ) variance_out_optional = paddle::make_optional<paddle::experimental::Tensor>(variance_out);

  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto reserve_space = egr::EagerUtils::RecoverTensorWrapper(&this->reserve_space_);

  paddle::optional<paddle::experimental::Tensor> reserve_space_optional;
  if( reserve_space.impl() ) reserve_space_optional = paddle::make_optional<paddle::experimental::Tensor>(reserve_space);

  auto& out_grad = hooked_grads[0][0];
  auto& momentum = this->momentum_;
  auto& epsilon = this->epsilon_;
  auto& data_layout = this->data_layout_;
  auto& is_test = this->is_test_;
  auto& use_global_stats = this->use_global_stats_;
  auto& trainable_statistics = this->trainable_statistics_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  auto* api_output_2 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "batch_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_MEAN_OUT_TEMPLATE = " \n( mean_out , [%s]), ";
    std::string input_mean_out_str = paddle::string::Sprintf(TENSOR_MEAN_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_out));
    input_str += input_mean_out_str; 
    const char* TENSOR_VARIANCE_OUT_TEMPLATE = " \n( variance_out , [%s]), ";
    std::string input_variance_out_str = paddle::string::Sprintf(TENSOR_VARIANCE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(variance_out));
    input_str += input_variance_out_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::batch_norm_grad(x, scale, bias, mean_out_optional, variance_out_optional, saved_mean, saved_variance, reserve_space_optional, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("batch_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& scale_grad = returns[3][0];
  egr::AutogradMeta* scale_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&scale_grad) : nullptr;
  if (scale_grad_autograd_meta) scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[4][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op batch_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: batch_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_MEAN_OUT_TEMPLATE = " \n( mean_out , [%s]), ";
    std::string input_mean_out_str = paddle::string::Sprintf(TENSOR_MEAN_OUT_TEMPLATE, egr::EagerUtils::TensorStr(mean_out));
    input_str += input_mean_out_str; 
    const char* TENSOR_VARIANCE_OUT_TEMPLATE = " \n( variance_out , [%s]), ";
    std::string input_variance_out_str = paddle::string::Sprintf(TENSOR_VARIANCE_OUT_TEMPLATE, egr::EagerUtils::TensorStr(variance_out));
    input_str += input_variance_out_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_SCALE_GRAD_TEMPLATE = " \n ( scale_grad , [%s]), ";
    std::string output_scale_grad_str = paddle::string::Sprintf(TENSOR_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(scale_grad));
    output_str += output_scale_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> CastGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "cast_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& value_dtype = this->value_dtype_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "cast_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::cast_grad(x, out_grad, value_dtype, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("cast_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op cast_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: cast_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Conv3dGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "conv3d_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto kernel = egr::EagerUtils::RecoverTensorWrapper(&this->kernel_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto rulebook = egr::EagerUtils::RecoverTensorWrapper(&this->rulebook_);
  auto counter = egr::EagerUtils::RecoverTensorWrapper(&this->counter_);
  auto& out_grad = hooked_grads[0][0];
  auto& paddings = this->paddings_;
  auto& dilations = this->dilations_;
  auto& strides = this->strides_;
  auto& groups = this->groups_;
  auto& subm = this->subm_;
  auto& key = this->key_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "conv3d_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_KERNEL_TEMPLATE = " \n( kernel , [%s]), ";
    std::string input_kernel_str = paddle::string::Sprintf(TENSOR_KERNEL_TEMPLATE, egr::EagerUtils::TensorStr(kernel));
    input_str += input_kernel_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_RULEBOOK_TEMPLATE = " \n( rulebook , [%s]), ";
    std::string input_rulebook_str = paddle::string::Sprintf(TENSOR_RULEBOOK_TEMPLATE, egr::EagerUtils::TensorStr(rulebook));
    input_str += input_rulebook_str; 
    const char* TENSOR_COUNTER_TEMPLATE = " \n( counter , [%s]), ";
    std::string input_counter_str = paddle::string::Sprintf(TENSOR_COUNTER_TEMPLATE, egr::EagerUtils::TensorStr(counter));
    input_str += input_counter_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::conv3d_grad(x, kernel, out, rulebook, counter, out_grad, paddings, dilations, strides, groups, subm, key, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("conv3d_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& kernel_grad = returns[1][0];
  egr::AutogradMeta* kernel_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&kernel_grad) : nullptr;
  if (kernel_grad_autograd_meta) kernel_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op conv3d_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: conv3d_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_KERNEL_TEMPLATE = " \n( kernel , [%s]), ";
    std::string input_kernel_str = paddle::string::Sprintf(TENSOR_KERNEL_TEMPLATE, egr::EagerUtils::TensorStr(kernel));
    input_str += input_kernel_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_RULEBOOK_TEMPLATE = " \n( rulebook , [%s]), ";
    std::string input_rulebook_str = paddle::string::Sprintf(TENSOR_RULEBOOK_TEMPLATE, egr::EagerUtils::TensorStr(rulebook));
    input_str += input_rulebook_str; 
    const char* TENSOR_COUNTER_TEMPLATE = " \n( counter , [%s]), ";
    std::string input_counter_str = paddle::string::Sprintf(TENSOR_COUNTER_TEMPLATE, egr::EagerUtils::TensorStr(counter));
    input_str += input_counter_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_KERNEL_GRAD_TEMPLATE = " \n ( kernel_grad , [%s]), ";
    std::string output_kernel_grad_str = paddle::string::Sprintf(TENSOR_KERNEL_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(kernel_grad));
    output_str += output_kernel_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "divide_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "divide_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::divide_grad(x, y, out, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op divide_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: divide_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> DivideScalarGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "divide_scalar_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& scalar = this->scalar_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "divide_scalar_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = divide_scalar_ad_func(out_grad, scalar);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::sparse::divide_scalar(out_grad, scalar);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("divide_scalar_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: divide_scalar_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Expm1GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "expm1_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "expm1_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::expm1_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("expm1_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op expm1_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: expm1_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> LeakyReluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "leaky_relu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "leaky_relu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::leaky_relu_grad(x, out_grad, alpha, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("leaky_relu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op leaky_relu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: leaky_relu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Log1pGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "log1p_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "log1p_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::log1p_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("log1p_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op log1p_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: log1p_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MultiplyGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "multiply_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "multiply_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::multiply_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("multiply_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op multiply_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: multiply_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> PowGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "pow_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  auto& factor = this->factor_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "pow_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::pow_grad(x, out_grad, factor, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("pow_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op pow_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: pow_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReluGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "relu_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "relu_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::relu_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op relu_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: relu_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> Relu6GradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "relu6_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& threshold = this->threshold_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "relu6_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::relu6_grad(out, out_grad, threshold, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("relu6_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op relu6_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: relu6_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ReshapeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "reshape_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "reshape_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::reshape_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("reshape_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op reshape_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: reshape_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ScaleGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "scale_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& scale = this->scale_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "scale_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  if (trace_backward) {
    auto api_output = scale_ad_func(out_grad, scale, 0.0, true);
    *api_output_0 = api_output;
} else {
    auto api_output = paddle::experimental::sparse::scale(out_grad, scale, 0.0, true);
    *api_output_0 = api_output;
  }
  
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("scale_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node

  VLOG(4) << "Finish AD API GRAD: scale_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SinGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sin_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sin_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::sin_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sin_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sin_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sin_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SinhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sinh_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sinh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::sinh_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sinh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sinh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sinh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SoftmaxGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "softmax_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& axis = this->axis_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "softmax_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::softmax_grad(out, out_grad, axis, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("softmax_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op softmax_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: softmax_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SparseCooTensorGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sparse_coo_tensor_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto indices = egr::EagerUtils::RecoverTensorWrapper(&this->indices_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sparse_coo_tensor_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::sparse_coo_tensor_grad(indices, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sparse_coo_tensor_grad", returns); }

  // Get GradOut autograd_meta

  auto& values_grad = returns[0][0];
  egr::AutogradMeta* values_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&values_grad) : nullptr;
  if (values_grad_autograd_meta) values_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sparse_coo_tensor_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sparse_coo_tensor_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INDICES_TEMPLATE = " \n( indices , [%s]), ";
    std::string input_indices_str = paddle::string::Sprintf(TENSOR_INDICES_TEMPLATE, egr::EagerUtils::TensorStr(indices));
    input_str += input_indices_str; 
    const char* TENSOR_VALUES_GRAD_TEMPLATE = " \n ( values_grad , [%s]), ";
    std::string output_values_grad_str = paddle::string::Sprintf(TENSOR_VALUES_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(values_grad));
    output_str += output_values_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SqrtGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sqrt_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sqrt_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::sqrt_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sqrt_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sqrt_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sqrt_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SquareGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "square_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "square_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::square_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("square_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op square_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: square_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SubtractGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "subtract_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "subtract_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::subtract_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("subtract_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op subtract_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: subtract_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> SyncBatchNormGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "sync_batch_norm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto scale = egr::EagerUtils::RecoverTensorWrapper(&this->scale_);
  auto bias = egr::EagerUtils::RecoverTensorWrapper(&this->bias_);
  auto saved_mean = egr::EagerUtils::RecoverTensorWrapper(&this->saved_mean_);
  auto saved_variance = egr::EagerUtils::RecoverTensorWrapper(&this->saved_variance_);
  auto reserve_space = egr::EagerUtils::RecoverTensorWrapper(&this->reserve_space_);

  paddle::optional<paddle::experimental::Tensor> reserve_space_optional;
  if( reserve_space.impl() ) reserve_space_optional = paddle::make_optional<paddle::experimental::Tensor>(reserve_space);

  auto& out_grad = hooked_grads[0][0];
  auto& momentum = this->momentum_;
  auto& epsilon = this->epsilon_;
  auto& data_layout = this->data_layout_;
  auto& is_test = this->is_test_;
  auto& use_global_stats = this->use_global_stats_;
  auto& trainable_statistics = this->trainable_statistics_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(5);
  for (int i = 0; i < 5; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[3].empty() || out_metas[3][0].IsStopGradient()) ? nullptr : &returns[3][0];
  auto* api_output_2 = (out_metas[4].empty() || out_metas[4][0].IsStopGradient()) ? nullptr : &returns[4][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "sync_batch_norm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::sync_batch_norm_grad(x, scale, bias, saved_mean, saved_variance, reserve_space_optional, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("sync_batch_norm_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& scale_grad = returns[3][0];
  egr::AutogradMeta* scale_grad_autograd_meta = returns[3][0].initialized() ? egr::EagerUtils::autograd_meta(&scale_grad) : nullptr;
  if (scale_grad_autograd_meta) scale_grad_autograd_meta->SetStopGradient(false);
  

  auto& bias_grad = returns[4][0];
  egr::AutogradMeta* bias_grad_autograd_meta = returns[4][0].initialized() ? egr::EagerUtils::autograd_meta(&bias_grad) : nullptr;
  if (bias_grad_autograd_meta) bias_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op sync_batch_norm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: sync_batch_norm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_SCALE_TEMPLATE = " \n( scale , [%s]), ";
    std::string input_scale_str = paddle::string::Sprintf(TENSOR_SCALE_TEMPLATE, egr::EagerUtils::TensorStr(scale));
    input_str += input_scale_str; 
    const char* TENSOR_BIAS_TEMPLATE = " \n( bias , [%s]), ";
    std::string input_bias_str = paddle::string::Sprintf(TENSOR_BIAS_TEMPLATE, egr::EagerUtils::TensorStr(bias));
    input_str += input_bias_str; 
    const char* TENSOR_SAVED_MEAN_TEMPLATE = " \n( saved_mean , [%s]), ";
    std::string input_saved_mean_str = paddle::string::Sprintf(TENSOR_SAVED_MEAN_TEMPLATE, egr::EagerUtils::TensorStr(saved_mean));
    input_str += input_saved_mean_str; 
    const char* TENSOR_SAVED_VARIANCE_TEMPLATE = " \n( saved_variance , [%s]), ";
    std::string input_saved_variance_str = paddle::string::Sprintf(TENSOR_SAVED_VARIANCE_TEMPLATE, egr::EagerUtils::TensorStr(saved_variance));
    input_str += input_saved_variance_str; 
    const char* TENSOR_RESERVE_SPACE_TEMPLATE = " \n( reserve_space , [%s]), ";
    std::string input_reserve_space_str = paddle::string::Sprintf(TENSOR_RESERVE_SPACE_TEMPLATE, egr::EagerUtils::TensorStr(reserve_space));
    input_str += input_reserve_space_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_SCALE_GRAD_TEMPLATE = " \n ( scale_grad , [%s]), ";
    std::string output_scale_grad_str = paddle::string::Sprintf(TENSOR_SCALE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(scale_grad));
    output_str += output_scale_grad_str; 
    const char* TENSOR_BIAS_GRAD_TEMPLATE = " \n ( bias_grad , [%s]), ";
    std::string output_bias_grad_str = paddle::string::Sprintf(TENSOR_BIAS_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(bias_grad));
    output_str += output_bias_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tan_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "tan_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::tan_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tan_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op tan_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: tan_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TanhGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "tanh_grad";
  // Fill Zero For GradIn Tensors
  const auto& input_metas = this->InputMeta();
  egr::EagerUtils::FillZeroForEmptyGradInput(&grads[0][0], input_metas[0][0]);

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "tanh_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::tanh_grad(out, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("tanh_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op tanh_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: tanh_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ToDenseGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "to_dense_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "to_dense_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::to_dense_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("to_dense_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op to_dense_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: to_dense_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ToSparseCooGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "to_sparse_coo_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "to_sparse_coo_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::to_sparse_coo_grad(out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("to_sparse_coo_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op to_sparse_coo_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: to_sparse_coo_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> TransposeGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "transpose_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto& out_grad = hooked_grads[0][0];
  auto& perm = this->perm_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "transpose_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::transpose_grad(out_grad, perm, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("transpose_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op transpose_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: transpose_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> ValuesGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "values_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "values_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::values_grad(x, out_grad, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("values_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op values_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: values_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> AddmmGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "addmm_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto input = egr::EagerUtils::RecoverTensorWrapper(&this->input_);
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  auto& alpha = this->alpha_;
  auto& beta = this->beta_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "addmm_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::addmm_grad(input, x, y, out_grad, alpha, beta, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("addmm_grad", returns); }

  // Get GradOut autograd_meta

  auto& input_grad = returns[0][0];
  egr::AutogradMeta* input_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&input_grad) : nullptr;
  if (input_grad_autograd_meta) input_grad_autograd_meta->SetStopGradient(false);
  

  auto& x_grad = returns[1][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[2][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op addmm_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: addmm_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_INPUT_TEMPLATE = " \n( input , [%s]), ";
    std::string input_input_str = paddle::string::Sprintf(TENSOR_INPUT_TEMPLATE, egr::EagerUtils::TensorStr(input));
    input_str += input_input_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_INPUT_GRAD_TEMPLATE = " \n ( input_grad , [%s]), ";
    std::string output_input_grad_str = paddle::string::Sprintf(TENSOR_INPUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(input_grad));
    output_str += output_input_grad_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> FusedAttentionGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "fused_attention_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto query = egr::EagerUtils::RecoverTensorWrapper(&this->query_);
  auto key = egr::EagerUtils::RecoverTensorWrapper(&this->key_);
  auto value = egr::EagerUtils::RecoverTensorWrapper(&this->value_);
  auto softmax = egr::EagerUtils::RecoverTensorWrapper(&this->softmax_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(6);
  for (int i = 0; i < 6; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  auto* api_output_2 = (out_metas[2].empty() || out_metas[2][0].IsStopGradient()) ? nullptr : &returns[2][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "fused_attention_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_QUERY_TEMPLATE = " \n( query , [%s]), ";
    std::string input_query_str = paddle::string::Sprintf(TENSOR_QUERY_TEMPLATE, egr::EagerUtils::TensorStr(query));
    input_str += input_query_str; 
    const char* TENSOR_KEY_TEMPLATE = " \n( key , [%s]), ";
    std::string input_key_str = paddle::string::Sprintf(TENSOR_KEY_TEMPLATE, egr::EagerUtils::TensorStr(key));
    input_str += input_key_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string input_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    input_str += input_softmax_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::fused_attention_grad(query, key, value, softmax, out_grad, api_output_0, api_output_1, api_output_2);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("fused_attention_grad", returns); }

  // Get GradOut autograd_meta

  auto& query_grad = returns[0][0];
  egr::AutogradMeta* query_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&query_grad) : nullptr;
  if (query_grad_autograd_meta) query_grad_autograd_meta->SetStopGradient(false);
  

  auto& key_grad = returns[1][0];
  egr::AutogradMeta* key_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&key_grad) : nullptr;
  if (key_grad_autograd_meta) key_grad_autograd_meta->SetStopGradient(false);
  

  auto& value_grad = returns[2][0];
  egr::AutogradMeta* value_grad_autograd_meta = returns[2][0].initialized() ? egr::EagerUtils::autograd_meta(&value_grad) : nullptr;
  if (value_grad_autograd_meta) value_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op fused_attention_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: fused_attention_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_QUERY_TEMPLATE = " \n( query , [%s]), ";
    std::string input_query_str = paddle::string::Sprintf(TENSOR_QUERY_TEMPLATE, egr::EagerUtils::TensorStr(query));
    input_str += input_query_str; 
    const char* TENSOR_KEY_TEMPLATE = " \n( key , [%s]), ";
    std::string input_key_str = paddle::string::Sprintf(TENSOR_KEY_TEMPLATE, egr::EagerUtils::TensorStr(key));
    input_str += input_key_str; 
    const char* TENSOR_VALUE_TEMPLATE = " \n( value , [%s]), ";
    std::string input_value_str = paddle::string::Sprintf(TENSOR_VALUE_TEMPLATE, egr::EagerUtils::TensorStr(value));
    input_str += input_value_str; 
    const char* TENSOR_SOFTMAX_TEMPLATE = " \n( softmax , [%s]), ";
    std::string input_softmax_str = paddle::string::Sprintf(TENSOR_SOFTMAX_TEMPLATE, egr::EagerUtils::TensorStr(softmax));
    input_str += input_softmax_str; 
    const char* TENSOR_QUERY_GRAD_TEMPLATE = " \n ( query_grad , [%s]), ";
    std::string output_query_grad_str = paddle::string::Sprintf(TENSOR_QUERY_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(query_grad));
    output_str += output_query_grad_str; 
    const char* TENSOR_KEY_GRAD_TEMPLATE = " \n ( key_grad , [%s]), ";
    std::string output_key_grad_str = paddle::string::Sprintf(TENSOR_KEY_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(key_grad));
    output_str += output_key_grad_str; 
    const char* TENSOR_VALUE_GRAD_TEMPLATE = " \n ( value_grad , [%s]), ";
    std::string output_value_grad_str = paddle::string::Sprintf(TENSOR_VALUE_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(value_grad));
    output_str += output_value_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaskedMatmulGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "masked_matmul_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(3);
  for (int i = 0; i < 3; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "masked_matmul_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::masked_matmul_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("masked_matmul_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op masked_matmul_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: masked_matmul_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MatmulGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "matmul_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto y = egr::EagerUtils::RecoverTensorWrapper(&this->y_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "matmul_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::matmul_grad(x, y, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("matmul_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& y_grad = returns[1][0];
  egr::AutogradMeta* y_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&y_grad) : nullptr;
  if (y_grad_autograd_meta) y_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op matmul_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: matmul_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_Y_TEMPLATE = " \n( y , [%s]), ";
    std::string input_y_str = paddle::string::Sprintf(TENSOR_Y_TEMPLATE, egr::EagerUtils::TensorStr(y));
    input_str += input_y_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_Y_GRAD_TEMPLATE = " \n ( y_grad , [%s]), ";
    std::string output_y_grad_str = paddle::string::Sprintf(TENSOR_Y_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(y_grad));
    output_str += output_y_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MaxpoolGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "maxpool_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto rulebook = egr::EagerUtils::RecoverTensorWrapper(&this->rulebook_);
  auto counter = egr::EagerUtils::RecoverTensorWrapper(&this->counter_);
  auto out = egr::EagerUtils::RecoverTensorWrapper(&this->out_);
  auto& out_grad = hooked_grads[0][0];
  auto& kernel_sizes = this->kernel_sizes_;
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(1);
  for (int i = 0; i < 1; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "maxpool_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_RULEBOOK_TEMPLATE = " \n( rulebook , [%s]), ";
    std::string input_rulebook_str = paddle::string::Sprintf(TENSOR_RULEBOOK_TEMPLATE, egr::EagerUtils::TensorStr(rulebook));
    input_str += input_rulebook_str; 
    const char* TENSOR_COUNTER_TEMPLATE = " \n( counter , [%s]), ";
    std::string input_counter_str = paddle::string::Sprintf(TENSOR_COUNTER_TEMPLATE, egr::EagerUtils::TensorStr(counter));
    input_str += input_counter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::maxpool_grad(x, rulebook, counter, out, out_grad, kernel_sizes, api_output_0);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("maxpool_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op maxpool_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: maxpool_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_RULEBOOK_TEMPLATE = " \n( rulebook , [%s]), ";
    std::string input_rulebook_str = paddle::string::Sprintf(TENSOR_RULEBOOK_TEMPLATE, egr::EagerUtils::TensorStr(rulebook));
    input_str += input_rulebook_str; 
    const char* TENSOR_COUNTER_TEMPLATE = " \n( counter , [%s]), ";
    std::string input_counter_str = paddle::string::Sprintf(TENSOR_COUNTER_TEMPLATE, egr::EagerUtils::TensorStr(counter));
    input_str += input_counter_str; 
    const char* TENSOR_OUT_TEMPLATE = " \n( out , [%s]), ";
    std::string input_out_str = paddle::string::Sprintf(TENSOR_OUT_TEMPLATE, egr::EagerUtils::TensorStr(out));
    input_str += input_out_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> MvGradNode::operator()(paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize>& grads, bool create_graph, bool is_new_grad) {
  VLOG(3) << "Running AD API GRAD: " << "mv_grad";
  // Fill Zero For GradIn Tensors

  // Apply Gradient Hooks
  auto hooked_grads = ApplyGradientHooks(grads);

  // Collect GradIn Tensors, Attrs and Recovered TensorWrappers
  auto x = egr::EagerUtils::RecoverTensorWrapper(&this->x_);
  auto vec = egr::EagerUtils::RecoverTensorWrapper(&this->vec_);
  auto& out_grad = hooked_grads[0][0];
  // Prepare Grad function call

  const auto& out_metas = OutputMeta();
  paddle::small_vector<std::vector<paddle::experimental::Tensor>, egr::kSlotSmallVectorSize> returns(2);
  for (int i = 0; i < 2; ++i) {
    out_metas[i].size() == 0 ? returns[i].resize(1) : returns[i].resize(out_metas[i].size());
  }

  auto* api_output_0 = (out_metas[0].empty() || out_metas[0][0].IsStopGradient()) ? nullptr : &returns[0][0];
  auto* api_output_1 = (out_metas[1].empty() || out_metas[1][0].IsStopGradient()) ? nullptr : &returns[1][0];
  // Runtime check if we need next grad
  bool trace_backward = egr::Controller::Instance().HasGrad() && create_graph;

  // Inplace Check

  // Inplace Strategy


  VLOG(5) << "Running C++ API: " << "mv_grad";
  // Before log info

  if(VLOG_IS_ON(3)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s]} ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
      VLOG(3) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str);
  }

  // Call grad_api function

  paddle::experimental::sparse::mv_grad(x, vec, out_grad, api_output_0, api_output_1);
  // Check NaN and Inf id needed
  if (FLAGS_check_nan_inf) { egr::CheckTensorHasNanOrInf("mv_grad", returns); }

  // Get GradOut autograd_meta

  auto& x_grad = returns[0][0];
  egr::AutogradMeta* x_grad_autograd_meta = returns[0][0].initialized() ? egr::EagerUtils::autograd_meta(&x_grad) : nullptr;
  if (x_grad_autograd_meta) x_grad_autograd_meta->SetStopGradient(false);
  

  auto& vec_grad = returns[1][0];
  egr::AutogradMeta* vec_grad_autograd_meta = returns[1][0].initialized() ? egr::EagerUtils::autograd_meta(&vec_grad) : nullptr;
  if (vec_grad_autograd_meta) vec_grad_autograd_meta->SetStopGradient(false);
  
  // Create Grad Node
  if(trace_backward) {
    PADDLE_THROW(phi::errors::Unavailable(
    "The Op mv_grad doesn't have any grad"
    "op. If you don't intend calculating higher order"
    "derivatives, please set `create_graph`to False."));
  }
  VLOG(4) << "Finish AD API GRAD: mv_grad";
  // LOG IF DEBUG
  
  if(VLOG_IS_ON(4)){
      const char* INPUT_PRINT_TEMPLATE = "{ Input: [%s],  \n Output: [%s] } ";
      
    std::string input_str = "";
    std::string output_str = "";
    const char* TENSOR_OUT_GRAD_TEMPLATE = " \n( out_grad , [%s]), ";
    std::string input_out_grad_str = paddle::string::Sprintf(TENSOR_OUT_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(out_grad));
    input_str += input_out_grad_str; 
    const char* TENSOR_X_TEMPLATE = " \n( x , [%s]), ";
    std::string input_x_str = paddle::string::Sprintf(TENSOR_X_TEMPLATE, egr::EagerUtils::TensorStr(x));
    input_str += input_x_str; 
    const char* TENSOR_VEC_TEMPLATE = " \n( vec , [%s]), ";
    std::string input_vec_str = paddle::string::Sprintf(TENSOR_VEC_TEMPLATE, egr::EagerUtils::TensorStr(vec));
    input_str += input_vec_str; 
    const char* TENSOR_X_GRAD_TEMPLATE = " \n ( x_grad , [%s]), ";
    std::string output_x_grad_str = paddle::string::Sprintf(TENSOR_X_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(x_grad));
    output_str += output_x_grad_str; 
    const char* TENSOR_VEC_GRAD_TEMPLATE = " \n ( vec_grad , [%s]), ";
    std::string output_vec_grad_str = paddle::string::Sprintf(TENSOR_VEC_GRAD_TEMPLATE, egr::EagerUtils::TensorStr(vec_grad));
    output_str += output_vec_grad_str; 
      VLOG(4) << paddle::string::Sprintf(INPUT_PRINT_TEMPLATE, input_str, output_str);
  }

  // Return
  if(NeedComplexToRealConversion()) HandleComplexGradToRealGrad(&returns);
  return returns;

}


}


namespace strings {
    
}


